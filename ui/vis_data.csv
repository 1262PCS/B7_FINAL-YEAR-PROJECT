paper_id,title,author,abstract,affiliation,address,category
1,A Trip-based Multicasting Model in Wormhole-routed Networks with Virtual Channels,"Yu-Chee Tseng, Dhabaleswar K. Panda, Member, IEEE, and Ten-Hwang Lai, Member, IEEE","Abstract| This paper focuses on efficient multicasting in wormhole-routed networks. A trip-based model is proposed to support adaptive, distributed, and deadlock-free multiple multicast on any network with arbitrary topology using at most two virtual channels per physical channel. This model significantly generalizes the path-based model proposed earlier [21], [22], which works only for Hamiltonian networks and can not be applicable to networks with arbitrary topology resulted due to system faults. Fundamentals of the trip-based model, including the necessary and sufficient condition to be deadlock-free, and the use of appropriate number of virtual channels to avoid deadlock are investigated. The potential of this model is illustrated by applying it to hyper-cubes with faulty nodes. Simulation results indicate that the proposed model can implement multiple multicast on faulty hypercubes with negligible performance degradation.",,,3
2,Space Deformation using Ray Deectors,Yair Kurzion and Roni Yagel,"Abstract In this paper we introduce a new approach to the deformation of surface and raster models in two and three dimensions. Rather then deforming the objects in the model, we deform the rays used to render the scene. The mechanism to specify the deformation, which we call a deector, is a vector of gravity positioned in space. This gravity vector bends any ray that travels through its field of gravity. Images generated by these curved rays give the impression of a deformed space. Unlike previous methods that deform all the objects in the scene, our approach deforms only those parts of the model that contribute to the final image. In addition, using deectors, our approach can deform any object type that can be rendered by a ray casting algorithm, providing a unified solution to space deformation.",Department of Computer and Information Science   The Ohio State University,,4
3,Efficient Rasterization of Implicit Functions,Torsten Mller and Roni Yagel,"Abstract Implicit curves are widely used in computer graphics because of their powerful features for modeling and their ability for general function description. The most popular rasterization techniques for implicit curves are space subdivision and curve tracking. In this paper we are introducing an efficient curve tracking algorithm that is also more robust then existing methods. We employ the Predictor-Corrector Method on the implicit function to get a very accurate curve approximation in a short time. Speedup is achieved by adapting the step size to the curvature. In addition, we provide mechanisms to detect and properly handle bifurcation points, where the curve intersects itself. Finally, the algorithm allows the user to trade-off accuracy for speed and vice a versa. We conclude by providing examples that dem-onstrate the capabilities of our algorithm.",Department of Computer and Information Science   The Ohio State University,"Columbus, Ohio",4
4,Alleviating Consumption Channel Bottleneck in Wormhole-Routed k-ary n-cube Systems 1,Debashis Basak and Dhabaleswar K. Panda,"Abstract This paper identifies performance degradation in wormhole routed k-ary n-cube networks due to limited number of router-to-processor consumption channels at each node. Many recent research in wormhole routing have advocated the advantages of adaptive routing and virtual channel flow control schemes to deliver better network performance. This paper indicates that the advantages associated with these schemes can not be realized with limited consumption capacity. To alleviate such performance bottleneck, a new network interface design using multiple consumption channels is proposed. To match virtual multiplexing on network channels, we also propose each consumption channel to support multiple virtual consumption channels. The impact of message arrival rate at a node on the required number of consumption channel is studied analytically. It is shown that wormhole networks with higher routing adaptivity, dimensionality, degree of hot-spot traffic, and number of virtual lanes have to take advantage of multiple consumption channels to deliver better performance. The interplay between system topology, routing algorithm, number of virtual lanes, messaging overheads, and communication traffic is studied through simulation to derive the effective number of consumption channels required in a system. Using the on-going technological trend, it is shown that wormhole-routed systems can use up to 2-4 consumption channels per node to deliver better system performance.",Dept. of Computer and Information Science   The Ohio State University,"Columbus, OH 43210-1277",0
5,A Certificate Path Generation Algorithm for Authenticated Signaling in ATM Networks,Jun Xu Mukesh Singhal,"Abstract ATM Forum specifies public key cryptography to be the default ATM authentication mechanism and directory services like X.509 to be the infrastructure for public key distribution and certification. Authenticated signaling, widely acknowledged as a necessary security feature of ATM network, requires the signaling message to be authenticated with a digital signature signed by the private key of the calling party. To verify the digital signature, the called party needs to obtain the public key of the calling party and a proof of the calling party's ownership to that public key. In X.509, the standard form of such a proof is a chain of public key certificates, called the certificate path between two parties. Certificate exchange protocol (CEP), proposed by ATM Forum, requires that another bi-directional connection be established between two parties to exchange public keys and certificate paths before an authenticated connection can be set up, which is not an ideal approach. We propose an algorithm which is embedded into ATM routing protocols to generate a certificate path inside a signaling message on-the-fly as the signaling message travels through the ATM network. In this approach, all that a calling party needs to know for authentication purpose is its own public key certificate and the ATM network builds the rest of the certificate path for it. Related issues like distribution of public key certificates and optimization of CA hierarchy are also addressed in this paper.",Department of Computer and Information Science   The Ohio State University,"Columbus, OH 43210",3
6,A Neural Network for Attentional Spotlight,Wee Kheng Leow and Risto Miikkulainen,"Abstract According to space-based theory, visual attention is limited to a local region in space called the attentional field. Visual information within the attentional field is enhanced for further processing while information outside is suppressed. There is evidence that enhancement and suppression are achieved with dynamic weighting of network activity. This paper discusses a neural network that generates the appropriate weights, called the attentional spotlight, given the size and the position of the intended attentional field. The network has three layers. A shunting feedback network serves as the output layer and performs a critical task which cannot be accomplished by feedforward networks.","Department of Computer Sciences,   University of Texas at Austin,","Austin, Texas 78712",2
7,Integrating Topological and Metric Maps for Mobile Robot Navigation: A Statistical Approach,"Sebastian Thrun 1 , Steffen Gutmann 2 , Dieter Fox 3 , Wolfram Burgard 3 , and Benjamin J. Kuipers 4","Abstract The problem of concurrent mapping and localization has received considerable attention in the mobile robotics community. With few exceptions, existing approaches can largely be grouped into two distinct paradigms: topological and metric. This paper proposes a method that integrates both paradigms. It poses the mapping problem as a statistical maximum likelihood problem, and devises an efficient algorithm for search in likelihood space. Based on that, it presents an novel mapping algorithm that integrates two phases: a topological and a metric mapping phase. The topological mapping phase solves a global position alignment problem between potentially indistinguishable, significant places. The subsequent metric mapping phase produces a fine-grained metric map of the environment in floating-point resolution. Experimental results in cyclic environments of sizes up to 80 by 25 meters demonstrate the appropriateness of this approach.",1  Computer Science Department  2 Institut fur Informatik 3 Institut fur Informatik III 4 Computer Science Department   Carnegie Mellon University Universitat Freiburg University of Bonn University of Texas at Austin,"Pittsburgh, PA 15213 D-79110 Freiburg, Germany D-53117 Bonn, Germany Austin, TX 78712",0
8,Massively Parallel Computation for Three-Dimensional Monte Carlo Semiconductor Device Simulation,"Henry Sheng, Roberto Guerrieri  and Alberto Sangiovanni-Vincentelli","Abstract This work presents a study of the applicability of a massively parallel computing paradigm to Monte Carlo techniques for device simulation. A unique mapping of Monte Carlo to SIMD fine-grained parallelism has been developed, decoupling the problem into separate computational domains. For MOSFET simulation, this novel mapping allows estimated speeds of over 200,000 scatterings processed per second on a 65,536 processor Connection Machine, nearly a factor of six over the fastest known to date.","Department of Electrical Engineering and Computer Sciences   University of California,","Berkeley, CA 94720, U.S.A.",3
9,On Hamiltonian Triangulations in Simple Polygons,Giri NARASIMHAN 1,"Abstract A simple polygon P is said to have a Hamilitonian Triangulation if it has a triangulation whose dual graph contains a Hamiltonian path. Such triangulations are useful in fast rendering engines in Computer Graphics. Arkin et al. [AHMS] observed that a polygon has a Hamiltonian triangulation if and only if it is Discretely Straight Walkable, a concept that is a discrete version of Straight Walkability concept as introduced by Icking and Klein [IK]. Using this characterization, Arkin et al. also showed an algorithm to recognize such polygons in time that is linear in the size of the visibility polygon of the given polygon P . The size of the visbility polygon of P could be quadratic in the size of P and hence their algorithm could be very inefficient even for nearly convex polygons. We give a new characterization of polygons with Hamiltonian triangulations. We use this characterization to present the following algorithms: * An O(n log n)-time algorithm to recognize polygons with a Hamiltonian triangulation. * An O(n log n)-time algorithm to construct such a triangulation. * Given vertices p and q on a simple polygon, an O(n)-time algorithm to determine whether the polygon is discretely straight walkable with respect to the two vertices. * An O(n log n)-time algorithm to list out all pairs of points on a simple polygon with respect to which the polygon is discretely straight walkable.",Dept. of Mathematical Sciences   The University of Memphis,Memphis TN 38152,3
10,On the Robustness of the Damped V -cycle of the Wavelet Frequency Decomposition Multigrid Method,Andreas Rieder 1fl Xiaodong Zhou 1,"Abstract The V -cycle of the wavelet variation of the ""Frequency decomposition multigrid method"" of Hackbusch [Numer. Math., 56, pp. 229-245, 1989] is considered. It is shown that its convergence speed is not affected by the presence of anisotropy provided that the corresponding coarse grid correction is damped sufficiently strong. Our analysis is based on properties of wavelet packets which are supplied and proved. Numerical approximations to the speed of convergence illustrate the theoretical results.",,,2
11,Neuronal Goals: Efficient Coding and Coincidence Detection,Nathan Intrator,"Abstract| Barlow's seminal work on minimal entropy codes and unsupervised learning is reiterated. In particular, the need to transmit the probability of events is put in a practical neuronal framework for detecting suspicious events. A variant of the BCM learning rule [15] is presented together with some mathematical results suggesting optimal minimal entropy coding.",School of Mathematical Sciences   Tel Aviv University,,0
12,Evolving Visually Guided Robots,"D. Cliff, P. Husbands, I. Harvey",,The University of Sussex   School of Cognitive and Computing Sciences,"Falmer Brighton BN1 9QH England, U.K.",0
13,MPI: A Message Passing Interface The MPI Forum,,"This paper presents an overview of mpi, a proposed standard message passing interface for MIMD distributed memory concurrent computers. The design of mpi has been a collective effort involving researchers in the United States and Europe from many organizations and institutions. mpi includes point-to-point and collective communication routines, as well as support for process groups, communication contexts, and application topologies. While making use of new ideas where appropriate, the mpi standard is based largely on current practice.",,,4
14,Reactive Functional Programming,Richard B. Kieburtz,"Abstract Reactive systems respond to concurrent, possibly unsynchronized streams of input events. Programming reactive systems is challenging without language support for event-triggered actions. It is even more challenging to reason about reactive systems. This paper explores a new conceptual basis for applying functional programming techniques to the design and formal verification of reactive systems. The mathematical foundation for this approach is based upon signature coalgebras and derived proof rules for coinduction. The concepts are illustrated with an example that has been used with the language Esterel.",Oregon Graduate Institute of Science & Technology,"P.O. Box 91000, Portland, OR 97291-1000 USA",3
15,Comparison of Statistical and Neural Classifiers and Their Applications to Optical Character Recognition and Speech Classification,"Ethem Alpaydn, Fikret Gurgen","Abstract We give a review of basic statistical and neural techniques for classification. Statistical techniques are based on the idea of estimating class-conditional likelihoods and using Bayes rule to convert these to posterior class probabilities whereas neural techniques estimate directly the posteriors. Statistical techniques include (i) Parametric (Gaussian) Bayes classifiers, (ii) Non-parametric kernel-based density estimators like k-nearest neighbor and Parzen windows, and (iii) mixtures of (Gaussian) densities (a special case of which is the Learning Vector Quantization). As neural classifiers, we include simple perceptrons and multilayer perceptrons with sigmoid and Gaussian hidden units. The neural and statistical techniques are quite similar in many respects and many approaches have been discovered independently twice, once in 1960s by statisticians and once in 1980s by the neural network researchers. One of the aims of this article is to make this link more apparent. We also discuss two, most popular, pattern recognition applications: Optical character recognition and speech recognition. Though they seem different, in many respects, the two applications are similar and in the past, almost the same techniques have been applied for their implementation. We implement the well known statistical and neural classification techniques for two datasets of these applications and compare them in terms of generalization accuracy, memory requirement and learning time. We especially advise to take into account statistics of the sample even if a neural classifier is to be used. The similarity between statistical and neural techniques is greater than generally agreed and simple statistical methods like k-NN perform generally quite well and much of the functionality of neural networks like distributed parallel computation can be obtained by such methods without requiring complicated computation and precise error minimization procedures.",Department of Computer Engineering   Bogazi~ci University,_ Istanbul Turkey,3
16,Crowds: Anonymity for Web Transactions,by Michael K. Reiter 1 Aviel D. Rubin 2,,"AT&T Labs|Research,","Murray Hill, New Jersey, USA",3
17,Kit: A Study in Operating System Verification,William R. Bevier,,Computational Logic Inc.,"1717 W. 6th St. Suite 290 Austin, Texas 78703",2
18,Callee-save Registers in Continuation-passing Style,ANDREW W. APPEL,"Abstract. Continuation-passing style (CPS) is a good abstract representation to use for compilation and optimization: it has a clean semantics and is easily manipulated. We examine how CPS expresses the saving and restoring of registers in source-language procedure calls. In most CPS-based compilers, the context of the calling procedure is saved in a ""continuation closure""|a single variable that is passed as an argument to the function being called. This closure is a record containing bindings of all the free variables of the continuation; that is, registers that hold values needed by the caller ""after the call"" are written to memory in the closure, and fetched back after the call. Consider the procedure-call mechanisms used by conventional compilers. In particular, registers holding values needed after the call must be saved and later restored. The responsibility for saving registers can lie with the caller (a ""caller-saves"" convention) or with the called function (""callee-saves""). In practice, to optimize memory traffic, compilers find it useful to have some caller-saves registers and some callee-saves. ""Conventional"" CPS-based compilers that pass a pointer to a record containing all the variables needed after the call (i.e., the continuation closure), are using a caller-saves convention. We explain how to express callee-save registers in Continuation-Passing Style, and give measurements showing the resulting improvement in execution time.","Department of Computer Science,   Princeton University,","Princeton, NJ 08544-2087",3
19,Discovering Compressive Partial Determinations in Mixed Numerical and Symbolic Domains,Bernhard Pfahringer and Stefan Kramer,Abstract Partial determinations are an interesting form of dependency between attributes in a relation. They generalize functional dependencies by allowing exceptions. We modify a known MDL formula for evaluating such partial determinations to allow for its use in an admissible heuristic in exhaustive search. Furthermore we describe an efficient preprocessing-based approach for handling numerical attributes. An empirical investigation tries to evaluate the viability of the presented ideas.,Austrian Research Institute for Artificial Intelligence,"Schottengasse 3 A-1010 Vienna, Austria",1
20,A Knowledge-Sharing Strategy,David Goldstein and Albert Esterline,,North Carolina A&T State University,"Greensboro, North Carolina",3
21,Core Selection Methods for Multicast Routing,Kenneth L. Calvert Ellen W. Zegura Michael J. Donahoo,"Abstract Multicast routing is an important topic of both theoretical and practical interest. Some recently-proposed multicast routing algorithms involve the designation of one or more network nodes as the ""center"" of the routing tree for each multicast group address. The choice of this designated router (which we refer to as the ""core"") influences the shape of the multicast routing tree, and thus influences performance of the routing scheme. In this paper we investigate the relationship between the choice of core and three performance measures. Specifically, we compare various methods of selecting a core with respect to their effect on bandwidth, delay, and traffic concentration. We conclude that simple methods are adequate for widely distributed groups, but that the addition of group information can be leveraged to improve performance especially when the group is small or exhibits a high degree of locality. We also conclude that core choice has a significant impact on traffic concentration, in fact traffic concentration effects can be ameliorated by appropriate core choice policies.",College of Computing   Georgia Institute of Technology,"Atlanta, Georgia 30332-0280",5
22,Implementing Schema-theoretic Models of Animal Behavior in Robotic Systems,Khaled S. Ali and Ronald C. Arkin,"Abstract Formal models of animal sensorimotor behavior can provide effective methods for generating robotic intelligence. In this paper we describe how schema-theoretic models of the praying mantis are implemented on a hexapod robot equipped with a real-time color vision system. The model upon which the implementation is based was developed by ethologists studying man-tids. This implementation incorporates a wide range of behaviors, including obstacle avoidance, prey acquisition, predator avoidance, mating, and chantlitaxia behaviors.",Mobile Robot Laboratory   College of Computing   Georgia Institute of Technology,"Atlanta, GA, 30332-0281 USA",3
23,Hypertextual Concurrent Control of a Lisp Kernel,P. David Stotts Richard Furuta,"Abstract Using the Trellis human/computer interaction model as an implementation vehicle, we demonstrate how to use concurrency-supporting hypertext to provide visual displays of the execution flows through a parallel Lisp program. In addition to displays, the hypertext interface allows injection of control flow into an otherwise functional computation, and therefore provides reader control over the order of evaluation of expressions. The resulting system, termed Trellis, can be thought of as a concurrent control flow browser for composing functional computations, providing a visual implementation of kernel-control decomposition. The advantages of Trellis are ease of exploring program side effects; ease of debugging parallel code; aid in teaching functional languages; and the ability to construct hypertext documents that have parallel execution semantics and flexible browsing behaviors. Key words: functional programming, parallelism, kernel-control decomposition, Lisp, hypertext, exe cution visualization.",Department of Computer and Department of Computer Science and Information Sciences Institute for Advanced Computer Studies   University of Florida University of Maryland,"Gainesville, FL 32611 College Park, MD 20742",4
24,Selection Predicate Indexing for Active Databases Using Interval Skip Lists,Eric N. Hanson Theodore Johnson,"Abstract A new, efficient selection predicate indexing scheme for active database systems is introduced. The selection predicate index proposed uses an interval index on an attribute of a relation or object collection when one or more rule condition clauses are defined on that attribute. The selection predicate index uses a new type of interval index called the interval skip list (IS-list). The IS-list is designed to allow efficient retrieval of all intervals that overlap a point, while allowing dynamic insertion and deletion of intervals. IS-list algorithms are described in detail. The IS-list allows efficient on-line searches, insertions, and deletions, yet is much simpler to implement than other comparable interval index data structures such as the priority search tree and balanced interval binary search tree (IBS-tree). IS-lists require only one third as much code to implement as balanced IBS-trees. The combination of simplicity, performance, and dynamic updateability of the IS-list is unmatched by any other interval index data structure. This makes the IS-list a good interval index structure for implementation in an active database predicate index.",Computer and Information Sciences Department   University of Florida,"Gainesville, FL 32611",6
25,Quality Management of Information Systems Development,"Geoff Beckworth,","Abstract The role of the systems analyst in the implementation process has changed dramatically in recent times because of changes to organisational boundaries, the need to align IT with business objectives and the complexity of the systems now required. Some organisations are finding themselves in a continually changing environment and being involved in multi-organisational structures. Establishing strategies and requirements for these organisations requires a new understanding of the implementation process. Implementation is concerned with behavioural phenomena since people are involved from the inception of the idea, as well as being involved in the development process. They are also affected by the changes which the new system brings to the organisation. The research is attempting to understand the critical factors associated with the implementation process and consequently develop an appropriate model.","School of Computing and Mathematics   Deakin University,","Geelong, Victoria, 3217, Australia.",3
26,UNSUPERVISED LEARNING OF INVARIANT REPRESENTATIONS OF FACES THROUGH TEMPORAL ASSOCIATION,Marian Stewart Bartlett ; Terrence J. Sejnowski ;,"Abstract The appearance of an object or a face changes continuously as the observer moves through the environment or as a face changes expression or pose. Recognizing an object or a face despite these image changes is a challenging problem for computer vision systems, yet we perform the task quickly and easily. This simulation investigates the ability of an unsupervised learning mechanism to acquire representations that are tolerant to such changes in the image. The learning mechanism finds these representations by capturing temporal relationships between 2-D patterns. Previous models of temporal association learning have used idealized input representations. The input to this model consists of graylevel images of faces. A two-layer network learned face representations that incorporated changes of pose up to 30 ffi . A second network learned representations that were independent of facial expression.","Departments of Cognitive Science and Psychology, UCSD   Howard Hughes Medical Institute The Salk Institute,","La Jolla, CA, 92037",3
27,Displaying 3D Images: Algorithms for Single Image Random Dot Stereograms,"Harold W. Thimbleby, Stuart Inglis, and Ian H. Witten *","Abstract This paper describes how to generate a single image which, when viewed in the appropriate way, appears to the brain as a 3D scene. The image is a stereogram composed of seemingly random dots. A new, simple and symmetric algorithm for generating such images from a solid model is given, along with the design parameters and their influence on the display. The algorithm improves on previously-described ones in several ways: it is symmetric and hence free from directional (right-to-left or left-to-right) bias, it corrects a slight distortion in the rendering of depth, it removes hidden parts of surfaces, and it also eliminates a type of artifact that we call an echo. Random dot stereograms have one remaining problem: difficulty of initial viewing. If a computer screen rather than paper is used for output, the problem can be ameliorated by shimmering, or time-multiplexing of pixel values. We also describe a simple computational technique for determining what is present in a stereogram so that, if viewing is difficult, one can ascertain what to look for.","Department of Psychology,   University of Stirling,","Stirling, Scotland.",6
28,Development of an Intelligent Monitoring and Control System for a Heterogeneous Numerical Propulsion System Simulation,"Abdollah A. Afjeh * , Patrick T. Homer , Henry Lewandowski , John A. Reed * , and Richard D. Schlichting","Abstract The NASA Numerical Propulsion System Simulation (NPSS) project is exploring the use of computer simulation to facilitate the design of new jet engines. Several key issues raised in this research are being examined in an NPSS-related research project: zooming, monitoring and control, and support for heterogeneity. The design of a simulation executive that addresses each of these issues is described. In this work, the strategy of zooming, which allows codes that model at different levels of fidelity to be integrated within a single simulation, is applied to the fan component of a turbofan propulsion system. A prototype monitoring and control system has been designed for this simulation to support experimentation with expert system techniques for active control of the simulation. An interconnection system provides a transparent means of connecting the heterogeneous systems that comprise the prototype.","Cleveland State University , The University of Arizona , University of Toledo *",,3
29,Why Aren't Operating Systems Getting Faster As Fast as Hardware?,John K. Ousterhout,"ABSTRACT This paper evaluates several hardware platforms and operating systems using a set of benchmarks that stress kernel entry/exit, file systems, and other things related to operating systems. The overall conclusion is that operating system performance is not improving at the same rate as the base speed of the underlying hardware. The most obvious ways to remedy this situation are to improve memory bandwidth and reduce operating systems' tendency to wait for disk operations to complete.",University of California at Berkeley,,2
30,Genetic Programming Bloat with Dynamic Fitness,W. B. Langdon and R. Poli,"Abstract In artificial evolution individuals which perform as their parents are usually rewarded identically to their parents. We note that Nature is more dynamic and there may be a penalty to pay for doing the same thing as your parents. We report two sets of experiments where static fitness functions are firstly augmented by a penalty for unchanged offspring and secondly the static fitness case is replaced by randomly generated dynamic test cases. We conclude genetic programming, when evolving artificial ant control programs, is surprisingly little effected by large penalties and program growth is observed in all our experiments.","School of Computer Science,   University of Birmingham,","Birmingham B15 2TT, UK",4
31,GP-Music: An Interactive Genetic Programming System for Music Generation with Automated Fitness Raters,Brad Johanson,"Abstract In this paper we present the GP-Music System, an interactive system which allows users to evolve short musical sequences using interactive genetic programming, and its extensions aimed at making the system fully automated. The basic GP-system works by using a genetic programming algorithm, a small set of functions for creating musical sequences, and a user interface which allows the user to rate individual sequences. With this user interactive technique it was possible to generate pleasant tunes over runs of 20 individuals over 10 generations. As the user is the bottleneck in interactive systems, the system takes rating data from a users run and uses it to train a neural network based automatic rater, or auto rater, which can replace the user in bigger runs. Using this auto rater we were able to make runs of up to 50 generations with 500 individuals per generation. The best of run pieces generated by the auto raters were pleasant but were not, in general, as nice as those generated in user interactive runs.","School of Computer Science,   The University of Birmingham","Rains Apt. 9A 704 Campus Dr. Stanford, CA. 94305",3
32,A General Approach to Performance Analysis and Optimization of Asynchronous Circuits,Tak Kwan Lee,,California Institute of Technology,"Pasadena, California",6
33,A Facilitator Method for Upstream Design Activities with Diverse Stakeholders,Regina M. Gonzales and Alexander L. Wolf,"Abstract This paper presents a method that can be used for the elicitation and specification of requirements and high-level design. It supports stakeholder-based modeling, rapid feasibility feedback to marketing, and the interpersonal dynamics that are necessary to develop a product. The method centers on the role of the facilitator, an independent agent whose purpose is to build the Integrated System Model (ISM). The ISM is the result of merging the independent system views from all stakeholders at any given abstraction level. Formulation of this method was based on the real-world experience of developing a complex, high-technology medical product with critical time-to-market pressures. It has proven to be a practical approach to the evolution of requirements definition and provides a necessary link to the marketing aspect of a product.",Software Engineering Research Laboratory   Department of Computer Science   University of Colorado,"Boulder, CO 80309 USA",0
34,Global Optimization Methods for Protein Folding Problems,"Richard H. Byrd, Elizabeth Eskow, Andre van der Hoek, Robert B. Schnabel, Chung-Shang Shao, and Zhihong Zou","Abstract. The problem of finding the naturally occurring structure of a protein is believed to correspond to minimizing the free, or potential, energy of the protein. This is generally a very difficult global optimization problem, with a large number of parameters and a huge number of local minimizers including many with function values near that of the global minimizer. This paper presents a new global optimization method for such problems. The method consists of an initial phase that locates some reasonably low local minimizers of the energy function, followed by the main phase that progresses from the best current local minimizers to even lower local minimizers. The method combines portions that work on small subsets of the parameters, including small-scale global optimizations using stochastic methods, with local minimizations involving all the parameters. In computational tests on the protein polyalanine with up to 58 amino acids (116 internal parameters), the method appears to be very successful in finding the lowest energy structures. The largest case is particularly significant because the lowest energy structures that are found include ones that exhibit interesting tertiary as opposed to just secondary structure.",,,4
35,Markov Decision Processes in Large State Spaces,Lawrence K. Saul and Satinder P. Singh,"Abstract In this paper we propose a new framework for studying Markov decision processes (MDPs), based on ideas from statistical mechanics. The goal of learning in MDPs is to find a policy that yields the maximum expected return over time. In choosing policies, agents must therefore weigh the prospects of short-term versus long-term gains. We study a simple MDP in which the agent must constantly decide between exploratory jumps and local reward mining in state space. The number of policies to choose from grows exponentially with the size of the state space, N . We view the expected returns as defining an energy landscape over policy space. Methods from statistical mechanics are used to analyze this landscape in the thermodynamic limit N ! 1. We calculate the overall distribution of expected returns, as well as the distribution of returns for policies at a fixed Hamming distance from the optimal one. We briefly discuss the problem of learning optimal policies from empirical estimates of the expected return. As a first step, we relate our findings for the entropy to the limit of high-temperature learning. Numerical simulations support the theoretical results.",Center for Biological and Computational Learning   Massachusetts Institute of Technology,"79 Amherst Street, E10-243 Cambridge, MA 02139",2
36,A Simple Algorithm for Nearest Neighbor Search in High Dimensions,Sameer A. Nene and Shree K. Nayar,,Department of Computer Science   Columbia University,"New York, N.Y. 10027",0
37,The Extruded Generalized Cylinder: A Deformable Model for Object Recovery,Thomas O'Donnell ? Xi-Sheng Fang ? Terrence E. Boult ? Alok Gupta,"Abstract There is increasing interest in the recovery of generalized cylinders (GCs) with curved spines. However, existing formulations of such GCs, for example those based on the Frenet-Serret frame or the tube model, suffer serious drawbacks: discontinuities, a lack of expressive power, ""narrowing"" in the plane normal to the spine, non-intuitive twisting behavior, and/or off-axis nonorthogonality of their local coordinate systems. We discuss some of the problems associated with the non-orthogonality of the coordinate system based on the Frenet-Serret frame. This non-orthogonality is induced by torsion effects and we show how to correct for it. We then introduce a new model, the extruded GC (EGC) model, which overcomes all the problems mentioned above. For complex axes, the EGC model is also simpler to understand and use than existing models. The EGC model is further extended by including local surface deformations. Recovery of the deformable EGC via a physically-motivated paradigm is demonstrated on pre-segmented data from a human carotid artery.","? Dept. of Computer Science Siemens Corporate Research, Inc.   Columbia University","755 College Road East New York, N.Y. 10027 Princeton, N.J. 08540",1
38,Predictive Dynamic Load Balancing of Parallel and Distributed Rule and Query Processing,Hasanat M. Dewan Salvatore J. Stolfo  Mauricio Hernandez Jae-Jun Hwang,"Abstract Expert Databases are environments that support the processing of rule programs against a disk resident database. They occupy a position intermediate between active and deductive databases, with respect to the level of abstraction of the underlying rule language. The operational semantics of the rule language influences the problem solving strategy, while the architecture of the processing environment determines efficiency and scalability. In this paper, we present elements of the PARADISER architecture and its kernel rule language, PARULEL. The PARADISER environment provides support for parallel and distributed evaluation of rule programs, as well as static and dynamic load balancing protocols that predictively balance a computation at runtime. This combination of features results in a scalable database rule and complex query processing architecture. We validate our claims by analyzing the performance of the system for two realistic test cases. In particular, we show how the performance of a parallel implementation of transitive closure is significantly improved by predictive dynamic load balancing.","Department of Computer Science   Columbia University,","New York, NY 10027",5
39,On the Area Bisectors of a Polygon,Karl-Friedrich Bohringer,"Abstract We consider the family of lines that are area bisectors of a polygon (possibly with holes) in the plane. We say that two bisectors of a polygon P are combinatorially distinct if they induce different partitionings of the vertices of P . We derive an algebraic characterization of area bisectors. We then show that there are simple polygons with n vertices that have (n 2 ) combinatorially distinct area bisectors (matching the obvious upper bound), and present an output-sensitive algorithm for computing an explicit representation of all the bisectors of a given polygon. Our study is motivated by the development of novel, flexible feeding devices for parts positioning and orienting. The question of determining all the bisectors of polygonal parts arises in connection with the development of efficient part positioning strategies when using these devices.",Cornell University,Author's current address:,4
40,Asymptotically Tight Bounds for Performing BMMC Permutations on Parallel Disk Systems,Thomas H. Cormen Thomas Sundquist Leonard F. Wisniewski,"Abstract We give asymptotically equal lower and upper bounds for the number of parallel I/O operations required to perform bit-matrix-multiply/complement (BMMC) permutations on parallel disk systems. In a BMMC permutation on N records, where N is a power of 2, each (lg N )-bit source address x maps to a corresponding (lg N)-bit target address by the matrix equation = A x c, where matrix multiplication is performed over GF (2). The characteristic matrix A is (lg N )fi(lg N ) and nonsingular over GF (2). Under the Vitter-Shriver parallel-disk model with N records, D disks, B records per block, and M records of memory, we show a universal lower bound of  BD 1 + rank  lg(M=B) parallel I/Os for performing a BMMC permutation, where  is the lower left lg(N=B) fi lg B submatrix of the characteristic matrix. We also present an algo rithm that uses at most 2N BD rank  lg(M=B) + 2 parallel I/Os, which asymptotically matches the lower bound and improves upon the BMMC and bit-permute/complement (BPC) algorithms in [4]. When rank is low, this method is an improvement over the general-permutation bound of fi N lg(N=B) We introduce a new subclass of BMMC permutations, called memoryload-dispersal (MLD) permutations, which can be performed in one pass. This subclass, which is used in the BMMC algorithm, extends the catalog of one-pass permutations appearing in [4]. Although many BMMC permutations of practical interest fall into subclasses that might be explicitly invoked within the source code, we show how to detect in at most N=BD+ l D parallel I/Os whether a given vector of target addresses specifies a BMMC permutation. Thus, one can determine efficiently at run time whether a permutation to be performed is BMMC and then avoid the general-permutation algorithm and save parallel I/Os by using our algorithm.",Department of Mathematics and Computer Science   Dartmouth College,,4
41,The Galley Parallel File System,"Nils Nieuwejaar, David Kotz","Most current multiprocessor file systems are designed to use multiple disks in parallel, using the high aggregate bandwidth to meet the growing I/O requirements of parallel scientific applications. Many multiprocessor file systems provide applications with a conventional Unix-like interface, allowing the application to access multiple disks transparently. This interface conceals the parallelism within the file system, increasing the ease of programmability, but making it difficult or impossible for sophisticated programmers and libraries to use knowledge about their I/O needs to exploit that parallelism. In addition to providing an insufficient interface, most current multiprocessor file systems are optimized for a different workload than they are being asked to support. We introduce Galley, a new parallel file system that is intended to efficiently support realistic scientific multiprocessor workloads. We discuss Galley's file structure and application interface, as well as the performance advantages offered by that interface.","Department of Computer Science,   Dartmouth College,","Hanover, NH 03755-3510",3
42,Relating Comprehension and Production in the Acquisition of Morphology,Michael Gasser,"Abstract Most theories of language processing and acquisition make the assumption that perception and comprehension are related to production, but few have anything say about how. This paper describes a performance-oriented connectionist model of the acquisition of morphology in which production builds on representations which develop during the learning of word recognition. Using artificial language stimuli embodying simple suffixation, prefixation, and template rules, I demonstrate that the model generalizes to novel combinations of roots and inflections for both word recognition and production. I argue that the capacity of connectionist networks to develop intermediate distributed representations which not only enable the solving of the task at hand but also facilitate another task offers a plausible account of how comprehension and production come to share phonological knowledge as words are learned.",Indiana University,,6
43,Coir: A Thread-Model for Supporting Task- and Data- Parallelism in Object-Oriented Parallel Languages,Neelakantan Sundaresan Dennis Gannon,"Abstract Data- and task-parallelism are two important parallel programming models. Object-oriented paradigm in parallelism provides a good way of abstracting out various aspects of computations and computing resources. Using an object-oriented language like C++, one can compose data and control representations into a single active object. We propose a thread model of parallelism that addresses both data and task parallelism. Computation and communication can be overlapped by suspending a thread of computation which is waiting for an event and running an eligible thread of computation in its place. Threads naturally subsume task-parallelism. Threads are encapsulated into thread objects may be grouped into rope objects [22, 20], that span the parallel machine domain, for collective computation and communication. Thus data-parallelism can be supported. Since rope objects are parallel objects, they can be customized, interestingly, in a serial or a parallel manner. Spatial transparency of objects is achieved by global pointer templates. We present results from a prototype system running on the SGI Challenge and the Intel Paragon.",Computer Science Department,215 Lindley Hall,6
44,Finding Genes in DNA with a Hidden Markov Model,John Henderson Steven Salzberg Kenneth H. Fasman,"Abstract This study describes a new Hidden Markov Model (HMM) system for segmenting uncharacterized genomic DNA sequences into exons, introns, and intergenic regions. Separate HMM modules were designed and trained for specific regions of DNA: exons, introns, intergenic regions, and splice sites. The models were then tied together to form a biologically feasible topology. The integrated HMM was trained further on a set of eukaryotic DNA sequences, and tested by using it to segment a separate set of sequences. The resulting HMM system, which is called VEIL (Viterbi Exon-Intron Locator), obtains an overall accuracy on test data of 92% of total bases correctly labelled, with a correlation coefficient of 0.68. Using the more stringent test of exact exon prediction, VEIL correctly located both ends of 46% of the exons. Moreover, more than 50% of the exons it predicts are exactly correct. These results compare favorably to the best previous results for gene structure prediction, and demonstrate the benefits of using HMMs for this problem.",,,4
45,A Unifying Framework for Conceptual Data Modelling Concepts,"P.J.M. Frederiks, A.H.M. ter Hofstede, E. Lippe","Abstract For succesful information systems development, conceptual data modelling is essential. Nowadays many techniques for conceptual data modelling exist, examples are NIAM, FORM, PSM, many (E)ER variants, IFO, and FDM. In-depth comparisons of concepts of these techniques is very difficult as the mathematical formalisations of these techniques, if existing at all, are very different. As such there is a need for a unifying formal framework providing a sufficiently high level of abstraction. In this paper the use of category theory for this purpose is addressed. Well-known conceptual data modelling concepts are discussed from a category theoretic point of view. Advantages and disadvantages of the approach chosen will be outlined.",Department of Information Systems University of Nijmegen,Toernooiveld 1 NL-6525 ED Nijmegen The Netherlands,3
46,Middle Scale Robot Navigation A Case Study,Carl Owen and Ulrich Nehmzow,"Abstract In this paper we present results of experiments carried out with a route learning system for a mobile robot, conducted in a `real world' environment covering distances of several hundred metres. The system uses no odometry and is based on a self-organising mapbuilding process using perceptual landmarks. A performance metric is defined and used to measure the robot's ability to traverse the route.",Department of Computer Science University of Manchester,Manchester M13 9PL United Kingdom,3
47,Error-Correcting Output Codes: A General Method for Improving Multiclass Inductive Learning Programs,Thomas G. Dietterich and Ghulum Bakiri,"Abstract Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt; 2 values (i.e., k ""classes""). The definition is acquired by studying large collections of training examples of the form hx i ; f(x i )i. Existing approaches to this problem include (a) direct application of multiclass algorithms such as the decision-tree algorithms ID3 and CART, (b) application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and (c) application of binary concept learning algorithms with distributed output codes such as those employed by Sejnowski and Rosenberg in the NETtalk system. This paper compares these three approaches to a new technique in which BCH error-correcting codes are employed as a distributed output representation. We show that these output representations improve the performance of ID3 on the NETtalk task and of backpropagation on an isolated-letter speech-recognition task. These results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.",Department of Computer Science Oregon State University,"Corvallis, OR 97331-3202",2
48,Analysis of Algorithms Generalizing B-Spline Subdivision,Jorg Peters Ulrich Reif,"Abstract A new set of tools for verifying smoothness of surfaces generated by stationary subdivision algorithms is presented. The main challenge here is the verification of injectivity of the characteristic map. The tools are sufficiently versatile and easy to wield to allow, as an application, a full analysis of algorithms generalizing bi-quadratic and bicubic B-spline subdivision. In the case of generalized biquadratic subdivision the analysis yields a hitherto unknown sharp bound strictly less than one on the second largest eigenvalue of any smoothly converging subdivision.",,,5
49,Utterance Units in Spoken Dialogue,David R. Traum 1 and Peter A. Heeman 2,"Abstract. In order to make spoken dialogue systems more sophisticated, designers need to better understand the conventions that people use in structuring their speech and in interacting with their fellow con-versants. In particular, it is crucial to discriminate the basic building blocks of dialogue and how they affect the way people process language. Many researchers have proposed the utterance unit as the primary object of study, but defining exactly what this is has remained a difficult issue. To shed light on this question, we consider grounding behavior in dialogue, and examine co-occurrences between turn-initial grounding acts and utterance unit signals that have been proposed in the literal, namely prosodic boundary tones and pauses. Preliminary results indicate high correlation between grounding and boundary tones, with a secondary correlation for longer pauses. We also consider some of the dialogue processing issues which are impacted by a definition of utterance unit.",,,3
50,Scalable Atomic Primitives for Distributed Shared Memory Multiprocessors,Maged M. Michael,"Abstract Our research addresses the general topic of atomic update of shared data structures on large-scale shared-memory multiprocessors. In this paper we consider alternative implementations of the general-purpose single-address atomic primitives fetch and , compare and swap, load linked, and store conditional. These primitives have proven popular on small-scale bus-based machines, but have yet to become widely available on large-scale, distributed shared memory machines. We propose several alternative hardware implementations of these primitives, and then analyze the performance of these implementations for various data sharing patterns. Our results indicate that good overall performance can be obtained by implementing compare and swap in the cache controllers, and by pro viding an additional instruction to load an exclusive copy of a cache line.",Department of Computer Science University of Rochester,"Rochester, NY 14627-0226 USA",4
51,Priors for Infinite Networks,Radford M. Neal,"Abstract Bayesian inference begins with a prior distribution for model parameters that is meant to capture prior beliefs about the relationship being modeled. For multilayer perceptron networks, where the parameters are the connection weights, the prior lacks any direct meaning | what matters is the prior over functions computed by the network that is implied by this prior over weights. In this paper, I show that priors over weights can be defined in such a way that the corresponding priors over functions reach reasonable limits as the number of hidden units in the network goes to infinity. When using such priors, there is thus no need to limit the size of the network in order to avoid ""overfitting"". The infinite network limit also provides insight into the properties of different priors. A Gaussian prior for hidden-to-output weights results in a Gaussian process prior for functions, which can be smooth, Brownian, or fractional Brownian, depending on the hidden unit activation function and the prior for input-to-hidden weights. Quite different effects can be obtained using priors based on non-Gaussian stable distributions. In networks with more than one hidden layer, a combination of Gaussian and non-Gaussian priors appears most interesting.",Department of Computer Science University of Toronto,"10 King's College Road Toronto, Canada M5S 1A4",5
52,Hierarchical Non-linear Factor Analysis and Topographic Maps,Zoubin Ghahramani and Geoffrey E. Hinton,"Abstract We first describe a hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We then show how to incorporate lateral connections into the generative model. The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes, the model develops topo graphically organised local feature detectors.","Dept. of Computer Science, University of Toronto","Toronto, Ontario, M5S 3H5, Canada",3
53,Gap-Definable Counting Classes,Stephen A. Fenner,,Computer Science Department University of Southern Maine,"96 Falmouth Street Portland, Maine 04103",0
54,Q-Learning for Bandit Problems,Michael O. Duff,"Abstract Multi-armed bandits may be viewed as decompositionally-structured Markov decision processes (MDP's) with potentially very-large state sets. A particularly elegant methodology for computing optimal policies was developed over twenty ago by Gittins [Gittins & Jones, 1974]. Gittins' approach reduces the problem of finding optimal policies for the original MDP to a sequence of low-dimensional stopping problems whose solutions determine the optimal policy through the so-called ""Gittins indices."" Katehakis and Veinott [Katehakis & Veinott, 1987] have shown that the Gittins index for a process in state i may be interpreted as a particular component of the maximum-value function associated with the ""restart-in-i"" process, a simple MDP to which standard solution methods for computing optimal policies, such as successive approximation, apply. This paper explores the problem of learning the Git-tins indices on-line without the aid of a process model; it suggests utilizing process-state-specific Q-learning agents to solve their respective restart-in-state-i subproblems, and includes an example in which the online reinforcement learning approach is applied to a problem of stochastic scheduling|one instance drawn from a wide class of problems that may be formulated as bandit problems.",Department of Computer Science University of Massachusetts,"Amherst, MA 01003",2
55,Humans Plus Agents Maintain Schedules Better than Either Alone,Tim Oates and Paul R. Cohen,"Abstract Tracking and evaluating the progress of large, complex plans or schedules as they unfold in real time is extremely difficult for humans. In this paper we present a mixed-initiative system for the task of schedule maintenance in a simulated shipping network. A schedule maintenance agent monitors the network, predicting the occurrence of states that may result in reduced throughput and formulating schedule modifications to avoid those states. The goal is to maximize throughput while minimizing disruptions to the original schedule. We present results of experiments in which human subjects attempt to obtain that goal both with and without the aid of the agent. We found that the human and the agent working together are able to achieve better results than either one working alone. In addition to looking at global performance measures such as throughput, we analyze individual schedule modification decisions made by subjects in an attempt to assign credit for the improvements in performance.","Experimental Knowledge Systems Laboratory Department of Computer Science, Box 34610 Lederle Graduate Research Center University of Massachusetts","Amherst, MA 01003-4610",1
56,Issues in Design-to-time Real-time Scheduling,Alan Garvey,"Abstract Design-to-time real-time scheduling is an alternative to the many flexible computation approaches that are based on anytime algorithms. It builds schedules at runtime that dynamically combine solutions to subproblems, taking advantage of the time available to achieve the best results it can. In this paper we look in detail at a few issues related to design-to-time, including where the approximations we rely on come from, how uncertainty affects the scheduling process and the interface between the sched-uler and its invoker.",Department of Computer Science Pacific Lutheran University,"Tacoma, WA 98447",4
57,Integrated Signal Processing and Signal Understanding 1,"Victor Lesser, Hamid Nawab , Malini Bhandaru, Norman Carver, Zarko Cvetanovic, Izaskun Gallastegi, Frank Klassner","Abstract This report outlines the IPUS paradigm, named for Integrated Processing and Understanding of Signals, which permits sophisticated interaction between theory-based problem solving in signal processing and heuristic problem-solving in signal interpretation. The need for such a paradigm arises in signal understanding domains that require the processing of complicated interacting signals under variable signal-to-noise ratios. One such application is sound understanding, in the context of which we report on a testbed experiment illustrating the functionality of key IPUS architecture components.",Electrical and Computer Engineering Dept. Boston University,"44 Cummington Street Boston, Massachusetts 02125",6
58,DTP: An Efficient Transport Protocol,Dheeraj Sanghi and Ashok K. Agrawala,"Abstract We recently introduced a new flow control scheme, called send-time control, which is based on a deterministic model of virtual circuits in a computer network. In this scheme, the time at which a packet is sent by a source is computed from estimates of round-trip time, traffic in the network and bottleneck service time. In this paper, we describe a new transport protocol, called DTP, which uses send-time control as its flow control scheme. Preliminary measurements of coast-to-coast connections over the Internet show significant performance improvement over TCP, which is the most commonly used transport protocol in the Internet today.","Department of Computer Science, University of Maryland,","College Park, MD 20742, USA",3
59,A SERVER OF DISTRIBUTED DISK PAGES USING A CONFIGURABLE SOFTWARE BUS,"Charles Falkenberg, Paul Hagger and Steve Kelley","ABSTRACT As network latency drops below disk latency, access time to a remote disk will begin to approach local disk access time. The performance of I/O may then be improved by spreading disk pages across several remote disk servers and accessing disk pages in parallel. To research this we have prototyped a data page server called a Page File. This persistent data type provides a set of methods to access disk pages stored on a cluster of remote machines acting as disk servers. The goal is to improve the throughput of database management system or other I/O intensive application by accessing pages from remote disks and incurring disk latency in parallel. This report describes the conceptual foundation and the methods of access for our prototype.",Institute for Advanced Computer Studies and The Department of Computer Science University of Maryland,"College Park, MD 20742",2
60,Finite State Machines and Recurrent Neural Networks - Automata and Dynamical Systems Approaches,"Peter Tino a;b , Bill G. Horne b , C. Lee Giles b;c",,a Department of Computer Science and Engineering Slovak Technical University,"Ilkovicova 3, 812 19 Bratislava, Slovakia",2
61,The Relative Importance of Concurrent Writers and Weak Consistency Models,Pete Keleher,"Abstract This paper presents a detailed comparison of the relative importance of allowing concurrent writers versus the choice of the underlying consistency model. Our comparison is based on single- and multiple-writer versions of a lazy release consistent (LRC) protocol, and a single-writer sequentially consistent protocol, all implemented in the CVM software distributed shared memory system. We find that in our environment, which we believe to be representative of distributed systems today and in the near future, the consistency model has a much higher impact on overall performance than the choice of whether to allow concurrent writers. The multiple writer protocol performs an average of 9% better than the single writer LRC protocol, but 34% better than the single-writer sequentially consistent protocol. Set against this, MW-LRC required an average of 72% memory overhead, compared to 10% overhead for the single-writer protocols.",Department of Computer Science University of Maryland,"College Park, MD 20742-3255",0
62,Exploiting the Temporal Structure of MPEG Video for the Reduction of Bandwidth Requirements,Marwan Krunz and Satish Tripathi,"Abstract We propose a new bandwidth allocation scheme for VBR video traffic in ATM networks. The scheme is tailored to MPEG-coded video sources that require stringent and deterministic quality-of-service guarantees. By exploiting the temporal structure of MPEG sources, we show that our scheme results in an effective bandwidth which, in most cases, is less than the source peak rate. The reduction in the bandwidth requirement is achieved without sacrificing any perceived QoS. Efficient procedures are provided for the computation of the effective bandwidth under heterogeneous MPEG sources. The effective bandwidth strongly depends on the arrangement of the multiplexed streams which is a measure of the degree of synchronization between the GOP patterns of different streams. Assuming that all possible arrangements are equi-probable, we derive an expression for the asymptotic tail distribution of the effective bandwidth. From the tail distribution, we compute several performance measures for the call blocking probability when the allocation is made based on the effective bandwidth. In the case of homogeneous sources, we give a closed-form expression for the `best' arrangement that results in the `optimal' effective bandwidth. Numerical examples based on real MPEG traces are used to demonstrate the advantages of our scheme.",Institute for Advanced Computer Studies Department of Computer Science University of Maryland,"College Park, MD 20742",0
63,An Empirical Study of Delay Jitter Management Policies *,Donald L. Stone Kevin Jeffay,"Abstract: This paper presents an empirical study of several policies for managing the effect of delay jitter on the playout of audio and video in computer-based conferences. The problem addressed is that of managing the fundamental tradeoff between display with low latency and display with few gaps. We describe a particular policy called queue monitoring which observes delay jitter over time and dynamically adjusts display latency in order to support low-latency conferences with an acceptable gap rate. Queue monitoring is evaluated by comparing it with two policies from the literature in a study based on measurements from a computer-based conferencing system. Our results show that queue monitoring performs as well or better than the other policies over the range of observed network loads. More importantly, we show that queue monitoring performs better on those network loads for which the other policies exhibit poor performance.",University of North Carolina at Chapel Hill Department of Computer Science,"Chapel Hill, NC 27599-3175 USA",0
64,Sync: A System for Mobile Collaborative Applications,Jonathan P. Munson and Prasun Dewan,"ABSTRACT Sync is a new Java-based framework for developing collaborative applications for wireless mobile systems. Sync is based on objectoriented replication and offers high-level synchronization-aware classes based on existing Java classes. Programmers may also extend the Sync-provided classes to create new replicated classes, either to add functionality or to modify a classs merge policy. Sync supports fully disconnected operation and employs centralized, asynchronous synchronization. Application programmers use the Sync framework to define conflicts and specify conflict resolution on the basis of the applications structure and semantics. We discuss the general needs of wireless mobile applications, and present a high-function example application that would be useful to mobile users, to be used for illustration throughout the paper. Next we discuss related work, and evaluate each work relative to its ability to support the example application. We then present the Sync framework, motivating each feature with its use in the example application.","Department of Computer Science, University of North Carolina at Chapel Hill",,4
65,Strange Bedfellows: Issues in Object Naming Under Unix,"Douglas B. Orr, Robert W. Mecklenburg and Ravindra Kuramkote","Abstract Naming plays a key role in the design of any system that exports services or resources. Object systems may export many different categories of names: instances, components of records, types, etc. Operating systems export the names of files, devices, and services. Integrating an object base with existing operating system facilities can improve accessibility of the object base resources. We consider the benefits and pitfalls of integrating an object base namespace with the Unix namespace. 1",Department of Computer Science University of Utah,"Salt Lake City, UT 84112 USA",2
66,Testing the FM9001 Microprocessor,"Kenneth L. Albin, Bishop C. Brock, Warren A. Hunt, Jr., Lawrence M. Smith",,"Computational Logic, Inc.","1717 West Sixth Street, Suite 290 Austin, Texas 78703-4776",0
67,Secure Group Communications Using Key Graphs,Chung Kei Wong Mohamed Gouda Simon S. Lam,"Abstract Many emerging applications (e.g., teleconference, real-time information services, pay per view, distributed interactive simulation, and collaborative work) are based upon a group communications model, i.e., they require packet delivery from one or more authorized senders to a very large number of authorized receivers. As a result, securing group communications (i.e., providing confidentiality, integrity, and authenticity of messages delivered between group members) will become a critical networking issue. In this paper, we present a novel solution to the scalability problem of group/multicast key management. We formalize the notion of a secure group as a triple (U; K; R) where U denotes a set of users, K a set of keys held by the users, and R a user-key relation. We then introduce key graphs to specify secure groups. For a special class of key graphs, we present three strategies for securely distributing rekey messages after a join/leave, and specify protocols for joining and leaving a secure group. The rekeying strategies and join/leave protocols are implemented in a prototype group key server we have built. We present measurement results from experiments and discuss performance comparisons. We show that our group key management service, using any of the three rekeying strategies, is scalable to large groups with frequent joins and leaves. In particular, the average measured processing time per join/leave increases linearly with the logarithm of group size.",Department of Computer Sciences University of Texas at Austin,"Austin, TX 78712-1188",4
68,Products of Domain Models,Don Batory,"Abstract We argue that domain models should produce four basic products: identification of reusable software components, definition of software architectures that explain how components can be composed, a demonstration of architecture scalability, and a direct relationship of these results to software generation of target systems.",Department of Computer Sciences The University of Texas,"Austin, Texas 78712",5
69,Verifying adder circuits using powerlists,William Adams,Abstract We define the ripple-carry and the carry-lookahead adder circuits in the powerlist notation and we use the powerlist algebra to prove that these circuits correctly implement addition for natural numbers represented as bit vectors.,Department of Computer Sciences The University of Texas at Austin,"Austin, TX 78712-1188 USA",2
70,Algorithms for Fence Design,Robert-Paul Berretty,"Abstract A common task in automated manufacturing processes is to orient parts prior to assembly. We address sensorless orientation of a polygonal part on a conveyor belt by a sequence of stationary fences across this belt. Since fences can only push against the motion of the belt, it is a challenging problem to compute fence designs which orients a given part. In this paper, we give several polynomial-time, algorithms to compute fence designs which are optimal with respect to various criteria. We address both frictionless and frictional fences. We also compute modular fence designs in which the fence angles are restricted to a discrete set of angles instead of an interval.","University of Utrecht,","Utrecht, The Netherlands",6
71,Software Engineering Beginning In The First Computer Science Course 1,Jane C. Prey James P. Cohoon Greg Fife,"Abstract. The demand for computing and computing power is increasing at a rapid pace. With this demand, the ability to develop, enhance and maintain software is a top priority. Educating students to do competent work in software development, enhancement and maintenance has become a complex problem. Software engineering concepts are typically not introduced in beginning computer science courses. Students do not see software engineering until the third or fourth year of the curriculum. We do not believe students can acquire an adequate software engineering foundation with the present approach. We believe an emphasis on software engineering should begin in the very first course and continue throughout the curriculum. We are redesigning our curriculum to reect this. The first course of the new curriculum is complete. This article focuses on two of the laboratory activities we have developed which deal with specific software engineering concepts.",Department of Computer Science School of Engineering and Applied Sciences University of Virginia,"Charlottesville, VA 22903",5
72,Shade: A Fast Instruction-Set Simulator for Execution Profiling,Bob Cmelik,"Abstract Tracing tools are used widely to help analyze, design, and tune both hardware and software systems. This paper describes a tool called Shade which combines efficient instruction-set simulation with a flexible, extensible trace generation capability. Efficiency is achieved by dynamically compiling and caching code to simulate and trace the application program. The user may control the extent of tracing in a variety of ways; arbitrarily detailed application state information may be collected during the simulation, but tracing less translates directly into greater efficiency. Current Shade implementations run on SPARC systems and simulate the SPARC (Versions 8 and 9) and MIPS I instruction sets. This paper describes the capabilities, design, implementation, and performance of Shade, and discusses instruction set emulation in general.","Sun Microsystems, Inc.",,2
73,A Portable Parallel N-body Solver,E Christopher Lewis Calvin Lin Lawrence Snyder George Turkiyyah,"Abstract We present parallel solutions for direct and fast n-body solvers written in the ZPL language. We describe the direct solver, compare its performance against a sequential C program, and show performance results for two very different parallel machines: the KSR-2 and the Paragon. We also discuss the implementation of the fast solver in ZPL, including factors pertinent to data movement.",,,2
74,User-Level Threads and Interprocess Communication,"Michael J. Feeley, Jeffrey S. Chase, and Edward D. Lazowska","Abstract User-level threads have performance and flexibility advantages over both Unix-like processes and kernel threads. However, the performance of user-level threads may suffer in multipro-grammed environments, or when threads block in the kernel (e.g., for I/O). These problems can be particularly severe in tasks that communicate frequently using IPC (e.g., multithreaded servers), due to interactions between the user-level thread scheduler and the operating system IPC primitives. Efficient IPC typically involves processor handoff that blocks the caller and unblocks a thread in the callee; when combined with user-level threads, this can cause problems for both caller and callee, particularly if the caller thread should subsequently block. In this paper we describe a new user-level thread package, called OThreads, designed to support blocking and efficient IPC for a system based on traditional kernel threads. We discuss the extent to which these problems can be solved at the user level without kernel changes such as scheduler activations. Our conclusion is that problems caused by application-controlled blocking and IPC can be resolved in the user-level thread package, but that problems due to multiprogramming workload and unanticipated blocking such as page faults require kernel changes such as scheduler activations.","Department of Computer Science and Engineering, FR-35 University of Washington","Seattle, WA 98195",2
75,Interface Timing Verification with Combined Max and Linear Constraints,"Elizabeth Walkup, Gaetano Borriello",,Department of Computer Science and Engineering University of Washington,"Seattle, WA 98195",0
76,Fast Rendering of Subdivision Surfaces,Kari Pulli,"Abstract Subdivision surfaces provide a curved surface representation that is useful in a number of applications, including modeling surfaces of arbitrary topological type [5] , fitting scattered data [6] , and geometric compression and automatic level-of-detail generation using wavelets [8] . Subdivision surfaces also provide an attractive representation for fast rendering, since they can directly represent complex surfaces of arbitrary topology. This direct representation contrasts with traditional approaches such as trimmed NURBS, in which tesselating trim regions dominates rendering time, and algebraic implicit surfaces, in which rendering requires resultants, root finders, or other computationally expensive techniques. We present a method for subdivision surface triangulation that is fast, uses minimum memory, and is simpler in structure than a naive rendering method based on direct subdivision. These features make the algorithm amenable to implementation on dedicated geometry engine processors, allowing high rendering performance on appropri ately equipped graphics hardware.",University of Washington,"Seattle, WA",4
77,Random Striping for News on Demand Servers,Juan Alemany and Jayram S. Thathachar,,Department of Computer Science and Engineering University of Washington,"Box 352350 Seattle, WA 98195",5
78,"The Error in Polynomial Tensor-Product, and Chung-Yao, Interpolation",Carl de Boor,"Abstract. A formula for the error in Chung-Yao interpolation announced earlier is proved (by induction). In the process, a bivariate divided difference identity of independent interest is proved. Also, an inductive proof of an error formula for polynomial interpolation by tensor-products is given. The main tool is a (convenient notation for a) multi-variate divided difference.",,,6
79,Recovering Shape by Purposive Viewpoint Adjustment,Kiriakos N. Kutulakos Charles R. Dyer,"Abstract We present an approach for recovering surface shape from the occluding contour using an active (i.e., moving) observer. It is based on a relation between the geometries of a surface in a scene and its occluding contour: If the viewing direction of the observer is along a principal direction for a surface point whose projection is on the contour, surface shape (i.e., curvature) at the surface point can be recovered from the contour. Unlike previous approaches for recovering shape from the occluding contour, we use an observer that purposefully changes viewpoint in order to achieve a well-defined geometric relationship with respect to a 3D shape prior to its recognition. We show that there is a simple and efficient viewing strategy that allows the observer to align their viewing direction with one of the two principal directions for a point on the surface. This strategy depends on only curvature measurements on the occluding contour and therefore demonstrates that recovering quantitative shape information from the contour does not require knowledge of the velocities or accelerations of the observer. Experimental results demonstrate that our method can be easily implemented and can provide reliable shape information from the occluding contour.",Computer Sciences Department University of Wisconsin,"Madison, Wisconsin 53706",4
80,Team Learning of Formal Languages,Sanjay Jain,"Abstract A team of learning machines is a multiset of learning machines. A team is said to successfully learn a concept just in case each member of some nonempty subset, of predetermined size, of the team learns the concept. Team learning of computer programs for computable functions from their graphs has been studied extensively. However, team learning of languages turns out to be a more suitable theoretical model for studying computational limits on multi-agent machine learning. The main reason for this is that language learning can model both learning from positive data and learning from positive and negative data, whereas function learning models only learning from positive and negative data. Some theoretical results about learnability of formal languages by teams of algorithmic machines are surveyed. Some new results about restricted classes of languages are presented. These results are mainly about two issues: redundancy and aggregation. The issue of redundancy deals with the impact of increasing the size of a team and increasing the number of machines required to be successful. The issue of aggregation deals with conditions under which a team may be replaced by a single machine without any loss in learning ability. Scenarios which can be modeled by team learning are also presented.",Dept. of Info. Systems & Computer Science National University of Singapore,"Singapore 0511, Republic of Singapore",0
81,Clustering via Concave Minimization,P. S. Bradley and O. L. Mangasarian W. N. Street,"Abstract The problem of assigning m points in the n-dimensional real space R n to k clusters is formulated as that of determining k centers in R n such that the sum of distances of each point to the nearest center is minimized. If a polyhedral distance is used, the problem can be formulated as that of minimizing a piecewise-linear concave function on a polyhedral set which is shown to be equivalent to a bilinear program: minimizing a bilinear function on a polyhedral set. A fast finite k-Median Algorithm consisting of solving few linear programs in closed form leads to a stationary point of the bilinear program. Computational testing on a number of real-world databases was carried out. On the Wisconsin Diagnostic Breast Cancer (WDBC) database, k-Median training set correctness was comparable to that of the k-Mean Algorithm, however its testing set correctness was better. Additionally, on the Wisconsin Prognostic Breast Cancer (WPBC) database, distinct and clinically important survival curves were extracted by the k-Median Algorithm, whereas the k-Mean Algorithm failed to obtain such distinct survival curves for the same database.",Computer Sciences Department Computer Science Department University of Wisconsin Oklahoma State University,"1210 West Dayton Street 205 Mathematical Sciences Madison, WI 53706 Stillwater, OK 74078",0
82,High-Bandwidth Address Translation for Multiple-Issue Processors,Todd M. Austin Gurindar S. Sohi,"Abstract In an effort to push the envelope of system performance, microprocessor designs are continually exploiting higher levels of instruction-level parallelism, resulting in increasing bandwidth demands on the address translation mechanism. Most current microprocessor designs meet this demand with a multi-ported TLB. While this design provides an excellent hit rate at each port, its access latency and area grow very quickly as the number of ports is increased. As bandwidth demands continue to increase, multi-ported designs will soon impact memory access latency. We present four high-bandwidth address translation mechanisms with latency and area characteristics that scale better than a multi-ported TLB design. We extend traditional high-bandwidth memory design techniques to address translation, developing interleaved and multi-level TLB designs. In addition, we introduce two new designs crafted specifically for high-bandwidth address translation. Piggyback ports are introduced as a technique to exploit spatial locality in simultaneous translation requests, allowing accesses to the same virtual memory page to combine their requests at the TLB access port. Pretranslation is introduced as a technique for attaching translations to base register values, making it possible to reuse a single translation many times. We perform extensive simulation-based studies to evaluate our designs. We vary key system parameters, such as processor model, page size, and number of architected registers, to see what effects these changes have on the relative merits of each approach. A number of designs show particular promise. Multi-level TLBs with as few as eight entries in the upper-level TLB nearly achieve the performance of a TLB with unlimited bandwidth. Piggyback ports combined with a lesser-ported TLB structure, e.g., an interleaved or multi-ported TLB, also perform well. Pretranslation over a single-ported TLB performs almost as well as a same-sized multi-level TLB with the added benefit of decreased access latency for physically indexed caches.",Computer Sciences Department University of Wisconsin-Madison,"1210 W. Dayton Street Madison, WI 53706",0
83,OPTIMAL PROCESSOR ASSIGNMENT FOR PARALLEL DATABASE DESIGN,"SHAHRAM GHANDEHARIZADEH , ROBERT R. MEYER , GARY L. SCHULTZ AND  JONATHAN YACKEL","Abstract. The computing time benefits of parallelism in database systems (achieved by using multiple processors to execute a query) must be weighed against communication, startup, and termination overhead costs that increase as a function of the number of processors used. We consider problems of minimizing overhead subject to allocating data among the processors according to specified loads. We present lower bounds for these combinatorial problems and demonstrate how processors may be optimally assigned for some problem classes.",,,4
84,1995 Computer Science Department MQP Review,Robert E. Kinicki Craig E. Wills,"Abstract This report presents results of a peer review of MQPs conducted within the Computer Science Department during the Summer of 1995 as part of a campus-wide MQP review. The goal of the report is to assess whether the department MQPs are accomplishing their educational goals. The report identifies problems that need to be addressed and trends that need to be continued to make the MQPs a worthwhile learning experience. It reflects data and evaluations for 27 MQPs, involving 43 computer science students, that were completed between the Summer of 1994 and the Spring of 1995. The report also makes comparisons to similar reviews done in 1991 and 1993. Overall, the large majority of the projects are meeting the educational goals of the department as good learning experiences. The reviews indicate the overall quality of the projects is good, about the same as in 1993 and a little better than 1991. The report draws a number of conclusions about the success of the projects based upon the data collected and evaluations done for this review. The report concludes with recommendations for future projects.",Computer Science Department Worcester Polytechnic Institute,"Worcester, MA 01609",2
85,HPF and MPI Implementation of the NAS Parallel Benchmarks  Supported by Integrated Program Engineering Tools,Christian Cl emencon Karsten M. Decker Vaibhav R. Deshpande  Akiyoshi Endo Josef Fritscher Paulo A. R. Lorenzo Norio Masuda Andreas Muller Roland R uhl William Sawyer Brian J. N. Wylie Frank Zimmermann,"Abstract: High Performance Fortran (HPF) compilers and communication libraries with the standardized Message Passing Interface (MPI) are becoming widely available, easing the development of portable parallel applications on distributed-memory parallel processor systems. The recently developed Annai tool environment supports programming, debugging and tuning of both HPF- and MPI-based applications. Considering code development and subsequent maintenance time to be as important as ultimate performance, we address how sequential Fortran-77 versions of the familiar NAS Parallel Benchmark kernels can be expediently parallelized with appropriate tool support. While automatic parallelization of scientific applications written in traditional sequential languages remains largely impractical, Annai provides users with high-level language extensions and integrated program engineering support tools. In this paper, Annai support is demonstrated primarily focusing on the MG (multigrid) kernel, with complementary examples selected from the other four kernels. Respectable performance and good scalability in most cases are obtained with this straightforward par-allelization strategy, even without recourse to platform-specific optimizations or major program transformations.","Centro Svizzero di Calcolo Scientifico (CSCS/SCSC) and NEC European Supercomputer Systems, Swiss Branch","CH-6928 Manno, Switzerland",4
86,Confluent Preorder Parsing,"HO, Kei Shiu Edward and CHAN, Lai Wan","Abstract In this paper, syntactic parsing is discussed in the context of connectionism. A new model - the Confluent Preorder Parser (CPP), is proposed which exemplifies the holistic parsing paradigm. Holistic parsing has the advantage that little assumption has to be made concerning the detailed parsing algorithm, which is often unknown or debatable, especially when human language understanding is concerned. In the CPP, syntactic parsing is achieved by transforming in a oneshot manner, from the connectionist representation of the sentence to the connectionist representation of the preorder traversal of its parse tree, instead of the parse tree itself. As revealed by the simulation experiments, generalization performance is excellent (as high as 90%). Besides, the CPP is also capable of parsing erroneous sentences and resolving syntactic ambiguities. A systematic study is conducted to explore the range of factors which can affect the effectiveness of it. This error-recovery capability is especially useful in natural language processing when incomplete or even ungrammatical sentences are to be dealt with.",Department of Computer Science The Chinese University of Hong Kong,"Shatin, N.T., Hong Kong",1
87,Learning stable concepts in domains with hidden changes in context,Michael Harries,"Abstract This paper presents Splice, a batch meta-learning system, designed to learn locally stable concepts in domains with hidden changes in context. The majority of machine learning algorithms assume that target concepts remain stable over time. In many domains this assumption is invalid. For example, financial prediction, medical diagnosis, and network performance are domains in which target concepts may not remain stable. Unstable target concepts are often due to changes in a hidden context. Existing works on learning in the presence of hidden changes in con text use an incremental learning approach.","Department of Artificial Intelligence School of Computer Science and Engineering  University of NSW,",Australia,4
88,A Space of Presentation Emphasis Techniques for Visualizing Graphs,Emanuel G. Noik,"Abstract The graph topo-visual formalism has been shown to be well-suited to the task of visualizing complex relations on a set of elements. Unfortunately, most visual formalisms do not scale very well. This observation is particularly true of graphs, which even when hand-drawn by an artist, are seldom meaningful when the number of nodes or links exceeds a very modest threshold typically only a few hundred elements. This severe limitation has prompted many researchers to seek alternative visualization techniques that may eliminate, or, at the very least, raise this threshold. In this paper we analyze these recent efforts, describe an abstract space of presentation emphasis techniques, and locate the current approaches within this space. The contributions of this paper are several: (1) a significant portion of recent work is collected and reviewed; (2) a common set of criteria and a taxonomy of graph views are proposed; these, (3) permit a more direct comparison of previous work; which helps to, (4) identify common shortcomings and limitations; which in turn, (5) suggest future directions.",Computer Systems Research Institute University of Toronto,"6 King's College Road Toronto, Ontario, Canada m4s 1a1",6
89,The Semantics of the C Programming Language,Yuri Gurevich and James K. Huggins,,"EECS Department, University of Michigan,","Ann Arbor, MI 48109-2122, USA",0
90,Defining the Java Virtual Machine as Platform for Provably Correct Java Compilation ?,Egon Borger 1 and Wolfram Schulte 2,"Abstract. We provide concise abstract code for running the Java Virtual Machine (JVM) to execute compiled Java programs, and define a general compilation scheme of Java programs to JVM code. These definitions, together with the definition of an abstract interpreter of Java programs given in our previous work [3], allow us to prove that any compiler that satisfies the conditions stated in this paper compiles Java code correctly. In addition we have validated our JVM and compiler specification through experimentation. The modularity of our definitions for Java, the JVM and the compilation scheme exhibit orthogonal language, machine and compiler components, which fit together and provide the basis for a stepwise and provably correct design-for-reuse. As a by-product we provide a challenging realistic case study for mechanical verification of a compiler correctness proof.","1 Universita di Pisa, Dipartimento di Informatica,","I-56125 Pisa, Italy",4
91,"Reasoning about Other Agents: Philosophy, Theory, and Implementation.",Piotr J. Gmytrasiewicz and Edmund H. Durfee,"Abstract Drawing on on our work in the area of Distributed Artificial Intelligence, we propose the rudiments of a view of multiagent reasoning that relates current philosophical intuitions, theoretical foundations, and preliminary implementation. The philosophical position we take is a combination of Daniel Dennett's philosophy of the ladder of per-sonhood (consisting of rationality, intentionality, stance, reciprocity, communication, and consciousness) on one hand, and the utilitarian philosophy of selfish utility maximization on the other hand. The theories we incorporate are logics of knowledge and belief, which in addressing the multiagent issues can be developed based on a recursive version of the Kripke structure, and the related fields of utility, decision and game theories. Our preliminary implementation, the Recursive Modeling Method (RMM), lets an agent coordinate its actions with the actions of other agents, cooperate with them when appropriate, and rationally choose an optimal form of communication with them.","Department of Computer Science Hebrew University,","Jerusalem, Israel",2
92,Using MICE to Study Intelligent Dynamic Coordination,Edmund H. Durfee and Thomas A. Montgomery,"Abstract We describe a flexible experimental testbed, called MICE, for distributed artificial intelligence research. We argue that the adoption of MICE (or some other standard testbed) by the distributed artificial intelligence community can draw together the community and permit a much greater level of exchange of ideas, formalisms, and techniques. MICE allows an experimenter to specify the constraints and characteristics of an environment in which agents are simulated to act and interact, and does not assume any particular implementation of an agent's reasoning architecture. MICE therefore provides a platform for investigating and evaluating alternative reasoning architectures and coordination mechanisms in many different simulated environments. We outline the design of MICE and illustrate its flexibility by describing simulated environments that model the coordination issues in domains such as predators chasing prey, predators attacking each other, agents fighting a fire, and diverse robots that are working together. In addition, we note that MICE's ability to simulate multi-agent environments makes it an ideal platform for studying reasoning in dynamic worlds; we can associate functionality to arbitrary objects in order to trigger changes in the environment. We conclude by discussing the status of MICE and how we are using MICE in our current research.",Department of Electrical Engineering and Computer Science University of Michigan,"Ann Arbor, Michigan 48109",4
93,Observational Uncertainty in Plan Recognition Among Interacting Robots,Marcus J. Huber Edmund H. Durfee,"Abstract Plan recognition is the process of observing another agent's behavior(s) and inferring what, and possibly why, the agent is acting as it is. Plan recognition becomes a very important means of acquiring such information about other agents in situations and domains where explicit communication is either very costly, dangerous, or impossible. Performing plan recognition in a physical domain (i.e. the real world) forces the world's ubiquitous uncertainty upon the observing agent because of the necessity to use real sensors to make the observations. We have developed a multiple resolution, hierarchical plan recognition system to coordinate the motion of two interacting mobile robots. Uncertainty arises in the system from dead reckoning errors that accumulate while the robots are moving, as well as by errors in the computer vision system that is used to detect the other agent's behaviors. Based upon belief networks, the plan recognition system gracefully degrades in performance as the level of uncertainty about observations increase.",Distributed Intelligent Agents Group (DIAG) Artificial Intelligence Laboratory The University of Michigan,"Ann Arbor, Michigan 48109-2110",6
94,Hierarchical Path Views: A Model Based on Fragmentation and Transportation Road Types,"Yun-Wu Huangy, Ning Jingz, and Elke A. Rundensteinery","Abstract Efficient path query processing necessary for route guidance has been identified as one of the key requirements for Intelligent Transportation Systems (ITS) applications. While precomputing the view of all shortest paths provides the most efficient path retrieval, the view maintenance and storage costs become unrealistic for large ITS networks. Based on ITS road type classification, we propose a hierarchical path view approach, in which the path view maintenance and storage costs are dramatically reduced at the cost of negligible loss of path optimality. Comparing with the traditional ITS path finding approaches that use A or hierarchical A , our hierarchical approach is superior in three areas: 1) path search is more efficient, 2) the connecting point from the low-level roads to the high-level roads and vice versa are dynamically determined based on the most recent traffic, 3) within one region, the high-level traffic can be dynamically rerouted through the low-level roads. In this paper, we conduct experiments to gain insight into the performance of our proposed algorithms and model, as well as to contrast the difference in computational resource requirements between the hierarchical path view and the A  algorithms.","Dept. of Electrical Engineering and Computer Science, Univ. of Michigan,","Ann Arbor, MI48109",6
95,Optimal Factory Scheduling using Stochastic Dominance A*,Peter R. Wurman and Michael P. Wellman,"Abstract We examine a standard factory scheduling problem with stochastic processing and setup times, minimizing the expectation of the weighted number of tardy jobs. Because the costs of operators in the schedule are stochastic and sequence dependent, standard dynamic programming algorithms such as A* may fail to find the optimal schedule. The SDA* (Stochastic Dominance A*) algorithm remedies this difficulty by relaxing the pruning condition. We present an improved state-space search formulation for these problems and discuss the conditions under which stochastic scheduling problems can be solved optimally using SDA*. In empirical testing on randomly generated problems, we found that in 70%, the expected cost of the optimal stochastic solution is lower than that of the solution derived using a deterministic ap proximation, with comparable search effort.",University of Michigan Artificial Intelligence Laboratory,"1101 Beal Avenue Ann Arbor, MI, 48109-2110",0
96,Dynamic Generation and Refinement of Concept Hierarchies for Knowledge Discovery in Databases,Jiawei Han and Yongjian Fu,"Abstract Concept hierarchies organize data and concepts in hierarchical forms or in certain partial order, which helps expressing knowledge and data relationships in databases in concise, high level terms, and thus, plays an important role in knowledge discovery processes. Concept hierarchies could be provided by knowledge engineers, domain experts or users, or embedded in some data relations. However, it is sometimes desirable to automatically generate some concept hierarchies or adjust some given hierarchies for particular learning tasks. In this paper, the issues of dynamic generation and refinement of concept hierarchies are studied. The study leads to some algorithms for automatic generation of concept hierarchies for numerical attributes based on data distributions and for dynamic refinement of a given or generated concept hierarchy based on a learning request, the relevant set of data and database statistics. These algorithms have been implemented in the DBLearn knowledge discovery system and tested against large relational databases. The experimental results show that the algorithms are efficient and effective for knowledge discovery in large databases.",School of Computing Science Simon Fraser University,"Burnaby, B.C., Canada V5A 1S6",0
97,SUPPORTING TECHNOLOGY TRANSFER OF FORMAL TECHNICAL REVIEW THROUGH A COMPUTER SUPPORTED COLLABORATIVE REVIEW SYSTEM,Philip M. Johnson,"Abstract Formal technical review (FTR) is an essential component of all modern software quality assessment, assurance, and improvement techniques, and is acknowledged to be the most cost-effective form of quality improvement when practiced effectively. However, traditional FTR methods such as inspection are very difficult to adopt in organizations: they introduce substantial new up-front costs, training, overhead, and group process obstacles. Sustained commitment from high-level management along with substantial resources is often necessary for successful technology transfer of FTR. Since 1991, we have been designing and evaluating a series of versions of a system called CSRS: an instrumented, computer-supported cooperative work environment for formal technical review. The current version of CSRS includes an FTR method definition language, which allows organizations to design their own FTR method, and to evolve it over time. This paper describes how our approach to computer supported FTR can address some of the issues in technology transfer of FTR.",Department of Information and Computer Sciences University of Hawaii,"Honolulu, HI 96822",6
98,Combinatory Differential Fields: An Algebraic Approach to Approximate Computation and Constructive Analysis,Karl Aberer,"Abstract The algebraic structure of combinatory differential fields is constructed to provide a semantics for computations in analysis. In this setting programs, approximations, limits and operations of analysis are represented as algebraic terms. Analytic algorithms can be derived by algebraic methods. The main tool in this construction are combinatory models which are inner algebras of Engeler graph models. As an universal domain of denotational semantics the lattice structure of the graph models allows to give a striking simple semantics for computations with approximations. As models of combinatory algebra they provide all essential computational constructs, including recursion. Combinatory models are constructed as extensions of first order theories. The classical first order theory to describe analysis is the theory of differential fields. It turns out that two types of computational constructs, namely composition and piecewise definition of functions, are preferably introduced as extensions of the differential fields theory. Combinatory differential fields are then the combinatory models of these enriched differential fields. We show for basic algorithms of computational analysis how their combinatory counterparts are derived in the algebraic setting. We illustrate how these algorithms are suitable to be implemented in a computer algebra environment like mathematica.",,"International Computer Science Institute, Berkeley, CA 94704.",5
99,An Efficient Parallel Algorithm for Computing a Maximal Independent Set in a Hypergraph of Dimension 3,Elias Dahlhaus 1 Marek Karpinski 2 Peter Kelsen 3,Abstract The paper considers the problem of computing a maximal independent set in a hypergraph (see [3] and [7]). We present an efficient deterministic NC algorithm for finding a maximal independent set in a hypergraph of dimension 3: the algorithm runs in time O(log 4 n) time on n + m processors of an EREW PRAM and is optimal up to a polylogarithmic factor. Our algorithm adapts the technique of Goldberg and Spencer ([5]) for finding a maximal independent set in a graph (or hypergraph of dimension 2). It is the first efficient NC algorithm for finding a maximal independent set in a hypergraph of dimension greater than 2.,"Department of Computer Science, University of Bonn,",5300 Bonn 1.,0
100,Modeling a Copier Paper Path: A Case Study in Modeling Transportation Processes,Vineet Gupta Peter Struss,"Abstract We present a compositional model of paper transportation in a photocopier that is meant to support different problem solving tasks like simulation and diagnosis, and to be applicable to a variety of configurations. Therefore, we try to avoid making hard-wired implicit assumptions about design principles and possible scenarios. In order to simplify our analysis, the model abstracts away from the physical forces and reasons only about velocities. Nonetheless, it succeeds in determining essential features of the motion of the sheet of paper like buckling and tearing. The framework provided is quite generic and can be used as a starting point for developing models of other transportation domains.","Xerox Palo Alto Research Center,","3333 Coyote Hill Road, Palo Alto CA 94304 USA.",6
101,Smoothing and Multiplexing Tradeoffs for Deterministic Performance Guarantees to VBR Video,Edward W. Knightly and Paola Rossaro,"Abstract The burstiness of variable bit rate traffic makes it difficult to both efficiently utilize network resources and provide end-to-end network performance guarantees to the traffic sources. Generally, smoothing or shaping traffic sources at the entrance of the network reduces their burstiness to allow higher utilization within the network. However, this buffering introduces an additional delay so that, in effect, lossless smoothing trades queueing delay inside the network for smoothing delay at the network edge. In this paper, we consider the net effect of smoothing on end-to-end performance guarantees where a no-loss, no-delay-violation deterministic guarantee is provided with the D-BIND traffic model. We analytically quantify these tradeoffs and provide a set of general rules for determining under which conditions smoothing provides a net gain. We also empirically investigate these tradeoffs using traces of MPEG compressed video.","Also with EECS Department, U.C. Berkeley",,0
102,Managing ABR Capacity in Reservation-based Slotted Networks,Roya Ulrich and Pieter Kritzinger,"Abstract For slotted networks carrying full multi-media traffic to work successfully, it is essential that connection setup and management is done well under all traffic conditions. Major challenges remain with the current state of the technology, however, particularly on how one copes with traffic bursts. Existing reservation-based networks do not allow the user to dynamically adjust his bandwidth requirements on demand. In this paper we propose a new scheme, called the reservoir scheme, which allows dynamic and distributed resource allocation. The basic idea behind the scheme is to reserve bandwidth with a guaranteed bit rate for each virtual circuit. The user is allowed to decentrally allocate additional bandwidth from an Available Bit Rate (ABR) reservoir to satisfy dynamic changes of Variable Bit Rate (VBR) traffic. The duration and bandwidth of this dynamic access are negotiated in the call setup phase and do not require any renegotiation with the service provider so that this solution overcomes the rigidity of current static bandwidth reservation schemes. The additional management requirements are low compared to other dynamic bandwidth reservation schemes. We also describe an analytic model and simulation which we used to determine whether it would be practical to apply the proposed scheme in a slotted network.",INTERNATIONAL COMPUTER SCIENCE INSTITUTE,"I 1947 Center St. * Suite 600 * Berkeley, California 94704-1198 *",2
103,A Management Platform for Global Area ATM Networks,Roya Ulrich,"Abstract Technological progress has made providing numerous new services to large number of users possible. Concurrently, we also experience an increased interest in real-time and interactive applications, e. g. teleseminaring, video conferencing and application sharing, in particular, because of the worldwide and decentralized character of today's research and development organizations. The International Computer Science Institute (ICSI) is a participant of the first transatlantic ATM link which is an integral part of the Multimedia Applications on Intercontinental Highways (MAY) Project. Additionally, ICSI is attached to the Bay Area Gigabit Network (BAGNet) providing ATM connectivity at the best-effort basis. Both projects provide platforms to identify the key research and development topics in cooperative real-time communication. The technical report gives a brief introduction to the ATM infrastructure at ICSI and addresses challenging management issues of multimedia applications in such global area ATM networks. We explore three management areas: performance, configuration, and fault management with respect to the user's point of view. Finally, we introduce a management platform and tools we have been developing which help the user to better predict the quality of service provided and to recover from faults occurred in the system or during a transmission.",INTERNATIONAL COMPUTER SCIENCE INSTITUTE,"I 1947 Center St. * Suite 600 * Berkeley, California 94704-1198",1
104,On-line Load Balancing for Related Machines,Piotr Berman  Moses Charikar  Marek Karpinski,"Abstract We consider the problem of scheduling permanent jobs on related machines in an on-line fashion. We design a new algorithm that achieves the competitive ratio of 3 + p 8 5:828 for the deterministic version, and 3:31= ln 2:155 4:311 for its randomized variant, improving the previous competitive ratios of 8 and 2e 5:436. We also prove lower bounds of 2:4380 on the competitive ratio of deterministic algorithms and 1:8372 on the competitive ratio of randomized algorithms for this problem.","Dept. of Computer Science & Eng., Pennsylvania State University,","University Park, PA16802, USA",1
105,On Learning Soccer Strategies,"Rafa l Sa lustowicz, Marco Wiering, Jurgen Schmidhuber","Abstract. We use simulated soccer to study multiagent learning. Each team's players (agents) share action set and policy but may behave differently due to position-dependent inputs. All agents making up a team are rewarded or punished collectively in case of goals. We conduct simulations with varying team sizes, and compare two learning algorithms: TD-Q learning with linear neural networks (TD-Q) and Probabilistic Incremental Program Evolution (PIPE). TD-Q is based on evaluation functions (EFs) mapping input/action pairs to expected reward, while PIPE searches policy space directly. PIPE uses an adaptive probability distribution to synthesize programs that calculate action probabilities from current inputs. Our results show that TD-Q has difficulties to learn appropriate shared EFs. PIPE, however, does not depend on EFs and finds good policies faster and more reliably.","IDSIA,","Corso Elvezia 36, 6900 Lugano, Switzerland",1
106,Induction as Knowledge Integration,Benjamin D. Smith,"Abstract Two key issues for induction algorithms are the accuracy of the learned hypothesis and the computational resources consumed in inducing that hypothesis. One of the most promising ways to improve performance along both dimensions is to make use of additional knowledge. Multi-strategy learning algorithms tackle this problem by employing several strategies for handling different kinds of knowledge in different ways. However, integrating knowledge into an induction algorithm can be difficult when the new knowledge differs significantly from the knowledge the algorithm already uses. In many cases the algorithm must be rewritten. This paper presents KII, a Knowledge Integration framework for Induction, that provides a uniform mechanism for integrating knowledge into induction. In theory, arbitrary knowledge can be integrated with this mechanism, but in practice the knowledge representation language determines both the knowledge that can be integrated, and the costs of integration and induction. By instantiating KII with various set representations, algorithms can be generated at different trade-off points along these dimensions. One instantiation of KII, called RS-KII, is presented that can implement hybrid induction algorithms, depending on which knowledge it utilizes. RS-KII is demonstrated to implement AQ-11 (Michalski 1978), as well as a hybrid algorithm that utilizes a domain theory and noisy examples. Other algorithms are also possible.",Jet Propulsion Laboratory California Institute of Technology,"4800 Oak Grove Drive M/S 525-3660 Pasadena, CA 91109-8099",2
107,Generality and Difficulty in Genetic Programming: Evolving a Sort,"Kenneth E. Kinnear, Jr.","Abstract Genetic Programming is applied to the task of evolving general iterative sorting algorithms. A connection between size and generality was discovered. Adding inverse size to the fitness measure along with correctness not only decreases the size of the resulting evolved algorithms, but also dramatically increases their generality and thus the effectiveness of the evolution process. In addition, a variety of differing problem formulations are investigated and the relative probability of success for each is reported. An example of an evolved sort from each problem formulation is presented, and an initial attempt is made to understand the variations in difficulty resulting from these differing problem formulations.",Adaptive Computing Technology,"62 Picnic Rd. Boxboro, MA 01719 USA",6
108,The Treadmill: Real-Time Garbage Collection Without Motion Sickness,Henry G. Baker,"A simple real-time garbage collection algorithm is presented which does not copy, thereby avoiding some of the problems caused by the asynchronous motion of objects. This in-place ""treadmill"" garbage collection scheme has approximately the same complexity as other nonmoving garbage collectors, thus making it usable in a high-level language implementation where some pointers cannot be traced. The treadmill is currently being used in a Lisp system built in Ada.","Nimble Computer Corporation,","16231 Meadow Ridge Way, Encino, CA 91436",5
109,Efficient Optimistic Concurrency Control Using Loosely Synchronized Clocks,Atul Adya Robert Gruber Barbara Liskov Umesh Maheshwari,"Abstract This paper describes an efficient optimistic concurrency control scheme for use in distributed database systems in which objects are cached and manipulated at client machines while persistent storage and transactional support are provided by servers. The scheme provides both serializability and external consistency for committed transactions; it uses loosely synchronized clocks to achieve global serialization. It stores only a single version of each object, and avoids maintaining any concurrency control information on a per- object basis; instead, it tracks recent invalidations on a per-client basis, an approach that has low in-memory space overhead and no per-object disk overhead. In addition to its low space overheads, the scheme also performs well. The paper presents a simulation study that compares the scheme to adaptive callback locking, the best concurrency control scheme for client-server object-oriented database systems studied to date. The study shows that our scheme outperforms adaptive callback locking for low to moderate contention workloads, and scales better with the number of clients. For high contention workloads, optimism can result in a high abort rate; the scheme presented here is a first step toward a hybrid scheme that we expect to perform well across the full range of workloads.","Laboratory for Computer Science, Massachusetts Institute of Technology,","545 Technology Square, Cambridge, MA 02139",6
110,Context-Insensitive Alias Analysis Reconsidered,Erik Ruf,,Microsoft Research Advanced Technology Division Microsoft Corporation,"One Microsoft Way Redmond, WA 98052",1
111,Behavior Near Zero of the Distribution of GCV Smoothing Parameter Estimates 1,by Grace Wahba and Yuedong Wang,,DEPARTMENT OF STATISTICS University of Wisconsin,"1210 West Dayton St. Madison, WI 53706",2
112,Toward a Lexicalized Grammar for Interlinguas,CLARE VOSS,"Abstract. In this paper we present one aspect of our research on machine translation (MT): capturing the grammatical and computational relation between (i) the interlingua (IL) as defined declaratively in the lexicon and (ii) the IL as defined procedurally by way of algorithms that compose and decompose pivot IL forms. We begin by examining the interlinguas in the lexicons of a variety of current IL-based approaches to MT. This brief survey makes it clear that no consensus exists among MT researchers on the level of representation for defining the IL. In the section that follows, we explore the consequences of this missing formal framework for MT system builders who develop their own lexical-IL entries. The lack of software tools to support rapid IL respecification and testing greatly hampers their ability to modify representations to handle new data and new domains. Our view is that IL-based MT research needs both (a) the formal framework to specify possible IL grammars and (b) the software support tools to implement and test these grammars. With respect to (a), we propose adopting a lexicalized grammar approach, tapping research results from the study of tree grammars for natural language syntax. With respect to (b), we sketch the design and functional specifications for parts of ILustrate, the set of software tools that we need to implement and test the various IL formalisms that meet the requirements of a lexicalized grammar. In this way, we begin to address a basic issue in MT research, how to define and test an interlingua as a computational language | without building a full MT system for each possible IL formalism that might be proposed.","Department of Computer Science, University of Maryland,","College Park, MD 20742",2
113,GROWING RADIAL BASIS FUNCTION NETWORKS,"E. BLANZIERI , P. KATENKAMP flfl and A. GIORDANA flflfl","Abstract. This paper presents and evaluates two algorithms for incrementally constructing Radial Basis Function Networks, a class of neural networks which looks more suitable for adtaptive control applications than the more popular backpropagation networks. The first algorithm has been derived by a previous method developed by Fritzke, while the second one has been inspired by the CART algorithm developed by Breiman for generation regression trees. Both algorithms proved to work well on a number of tests and exhibit comparable performances. An evaluation on the standard case study of the Mackey-Glass temporal series is reported.","Centro di Scienza Cognitiva, Universita di Torino,","Via Lagrange 3, 10100 Torino, Italy.",1
114,On Scheduling Two Classes of Real Time Traffic With Identical Deadlines,Sridhar Pingali,"Abstract The problem of scheduling two classes of real-time traffic with correlated time constraints is considered. Three scheduling disciplines are studied: a priority discipline which gives strict priority to one class of traffic, a threshold-based scheme in which priority is given to one class of traffic when the minimum laxity of its queued packets falls below some threshold, and a ""balancing"" scheme which assigns priority on the basis of the differences in minimum laxities in the two classes of traffic. Analytic results are obtained by using a discrete time model to obtain the state occupancy probabilities for the system. Here, the state is defined using the laxities of the queued real time packets. Parameters are defined to study the tradeoff in the performance of the two classes of traffic. Results are obtained to demonstrate how the balancing scheme permits us to achieve significant improvement in the performance of one class of traffic with only minimal effect on the performance of other class. A video application is suggested for this work.",Dept. of Electrical and Computer Engineering University of Masschusetts,"Amherst, MA 01003",5
115,The Effectiveness of Affinity-Based Scheduling in Multiprocessor Networking,"James D. Salehi, James F. Kurose, and Don Towsley","Abstract Techniques for avoiding the high memory overheads found on many modern shared-memory multiprocessors are of increasing importance in the development of high-performance multiprocessor protocol implementations. One such technique is processor-cache affinity scheduling, which can significantly lower packet latency and substantially increase protocol processing throughput [20]. In this paper, we evaluate several aspects of the effectiveness of affinity-based scheduling in multiprocessor network protocol processing, under packet-level and connection-level par-allelization approaches. Specifically, we evaluate the performance of the scheduling technique 1) when a large number of streams are concurrently supported, 2) when processing includes copying of uncached packet data, 3) as applied to send-side protocol processing, and 4) in the presence of stream burstiness and source locality, two well-known properties of network traffic. We find that affinity-based scheduling performs well under these conditions, emphasizing its robustness and general effectiveness in multiprocessor network processing. In addition, we explore a technique which improves the caching behavior and available packet-level concurrency under connection-level parallelism, and find performance improves dramatically.","Computer Science Department, University of Massachusetts,","Amherst MA 01003, USA",5
116,Optimal Smoothing of Stored Video and the Impact on Network Resource Requirements fly,"James D. Salehi, Zhi-Li Zhang, James F. Kurose, and Don Towsley","Abstract VBR compressed video is known to exhibit significant, multiple-time-scale bit rate variability. In this paper, we consider the transmission of stored video from a server to a client across a high speed network, and explore how the client buffer space can be used most effectively toward reducing the variability of the transmitted bit rate. We present two basic results. First, we show how to achieve the greatest possible reduction in rate variability when sending stored video to a client with given buffer size. We formally establish the optimality of our optimal smoothing approach, and illustrate its performance over a set of long MPEG-1 encoded video traces. Second, we evaluate the impact of optimal smoothing on the network resources needed for video transport, under two network service models: Deterministic Guaranteed service [1, 11] and Renegotiated CBR (RCBR) service [9, 8]. Under both models, we find the impact of optimal smoothing to be dramatic.",Department of Computer Science University of Massachusetts,"Amherst, MA 01003, U.S.A",5
117,Paging Tradeoffs in Distributed-Shared-Memory Multiprocessors,"Douglas C. Burger, Rahmat S. Hyder, Barton P. Miller, David A. Wood","Abstract Massively parallel processors have begun using commodity operating systems that support demand-paged virtual memory. To evaluate the utility of virtual memory, we measured the behavior of seven shared-memory parallel application programs on a simulated distributed-shared-memory machine. Our results (i) confirm the importance of gang CPU scheduling, (ii) show that a page-faulting processor should spin rather than invoke a parallel context switch, (iii) show that our parallel programs frequently touch most of their data, and (iv) indicate that memory, not just CPUs, must be ""gang scheduled"". Overall, our experiments demonstrate that demand paging has limited value on current parallel machines because of the applications' synchronization and memory reference patterns and the machines' high page-fault and parallel-context-switch overheads.",Computer Sciences Department University of Wisconsin-Madison,"1210 W. Dayton Street Madison, WI 53706 USA",6
118,Supporting Irregular Distributions in FORTRAN 90D/HPF Compilers,Ravi Ponnusamy yz Yuan-Shin Hwang Raja Das  Joel Saltz Alok Choudhary Geoffrey Fox,"Abstract This paper presents methods that make it possible to efficiently support irregular problems using data parallel languages. The approach involves the use of a portable, compiler-independent, runtime support library called CHAOS. The CHAOS runtime support library contains procedures that * support static and dynamic distributed array partitioning, * partition loop iterations and indirection arrays, * remap arrays from one distribution to another, and * carry out index translation, buffer allocation and communication schedule generation. The CHAOS runtime procedures are used by a prototype Fortran 90D compiler as runtime support for irregular problems. This paper also presents performance results of compiler-generated and hand-parallelized versions of two stripped down applications codes. The first code is derived from an unstructured mesh computational fluid dynamics flow solver and the second is derived from the molecular dynamics code CHARMM. A method is described that makes it possible to emulate irregular distributions in HPF by reordering elements of data arrays and renumbering indirection arrays. The results suggest that an HPF compiler could use reordering and renumbering extrinsic functions to obtain performance comparable to that achieved by a compiler for a language (such as Fortran 90D) that directly supports irregular distributions.",UMIACS and Computer Science Department Northeast Parallel Architectures Center University of Maryland Syracuse University,"College Park, MD 20742 Syracuse, NY 13244",5
119,High Performance Verification Algorithms,by Jagesh V. Sanghavi,,Engineering Electrical Engineering and Computer Sciences,,6
120,THE PAPIA2 MACHINE: HARDWARE AND SOFTWARE ARCHITECTURE,"A. Biancardi, V. Cantoni, M. Ferretti and M. Mosconi","ABSTRACT This paper presents the overall structure of PAPIA2, a pyramid system belonging to the family of massive parallel machines. It embeds the topology of the quad-pyramid into a highly regular, fault tolerant, eight-connected proces sor array by means of specially reconfigurable near-neighbor interconnections. The system comes with a fully-fledged software environment designed to optimize the use of machine resources. The highly interactive graphic tools help in understanding the machine's capabilities, provide a valuable testbed for the machine instruction set, and offer a suitable context for monitoring program execution.",Dipartimento di Informatica e Sistemistica University of Pavia,"Via Abbiategrasso 209 I-27100 PAVIA, ITALY",5
121,Simulations with an Evolvable Fitness Formula,Henrik Hautop Lund Domenico Parisi,"Abstract The concept of a fitness formula as a property of an organism is proposed. In artificial life simulations with organisms living in an environment, the fitness formula can be interpreted as the ability of organisms to extract energy from potential food sources distributed in the environment. In simulations where the goal of the genetic algorithm is that of developing systems which exhibit a certain type of behavior in a particular environment, the fitness formula becomes an independent variable which can be manipulated in order to obtain the desired behavior. The fitness formula can be viewed as an evolvable trait of organisms, and therefore not fixed and decided by the researcher. Simulations with fixed and evolvable fitness formulae show that the fitness formula, the sensory apparatus, and the behavior of organisms may co-evolve and be co-adapted.","Institute of Psychology National Research Council,","Viale Marx 15, 00137 Rome, Italy",5
122,"Layered, Server-based Support for Object-Oriented Application Development",Guruduth Banavar Douglas Orr Gary Lindstrom,"Abstract This paper advocates the idea that the physical modularity (file structure) of application components supported by conventional OS environments can be elevated to the level of logical modularity, which in turn can directly support application development in an object-oriented manner. We demonstrate this idea through a system-wide server that manages the manipulation of such components effectively. The server is designed to be a fundamental operating system service responsible for binding and mapping component instances into client address spaces. We show how this model solves some longstanding problems with the management of application components in existing application development environments. We demonstrate that this model's effectiveness derives from its support for the cornerstones of OO programming: classes and their instances, encapsulation, and several forms of inheritance.","Department of Computer Science University of Utah,","Salt Lake City, UT 84112 USA",1
123,Dynamic Management of Guaranteed Performance Multimedia Connections,"Colin Parris, Hui Zhang , and Domenico Ferrari","Abstract Most of the solutions proposed to support real-time (i.e. guaranteed performance) communication services in packet-switching networks adopt a connection-oriented and reservation-oriented approach. In such an approach, resource allocation and route selection decisions are made before the start of the communication on the basis of resource availability and real-time network load at that time, and are usually kept for the duration of the communication. This rather static resource management approach has certain limitations: it does not take into account (a) the dynamics of the communicating clients; (b) the dynamics of the network state; and (c) the tradeoff between quality of service and network availability, thus affecting the availability and flexibility of the real-time network services. Availability is the ability of the network to accommodate as many real-time clients as possible, while flexibility is the ability to adapt the real-time services to changing network state and client demands. In this paper, we present the Dynamic Connection Management (DCM) scheme, which addresses these issues by providing the network with the capability to dynamically modify the performance parameters and the routes of any existing real-time connection. With these capabilities, DCM can be used to increase the availability and flexibility of the guaranteed performance service offered to the clients.",Computer Science Division University of California at Berkeley,"Berkeley, CA 94720",6
124,Probabilistic Independence Networks for Hidden Markov Probability Models,"Padhraic Smyth, David Heckerman, and Michael Jordan","bstract Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper contains a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach.",MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES,,6
125,Networks and the Best Approximation Property,Federico Girosi and Tomaso Poggio,"Abstract Networks can be considered as approximation schemes. Multilayer networks of the backpropagation type can approximate arbitrarily well continuous functions (Cybenko, 1989; Funahashi, 1989; Stinchcombe and White, 1989). We prove that networks derived from regularization theory and including Radial Basis Functions (Poggio and Girosi, 1989), have a similar property. From the point of view of approximation theory, however, the property of approximating continuous functions arbitrarily well is not sufficient for characterizing good approximation schemes. More critical is the property of best approximation. The main result of this paper is that multilayer networks, of the type used in backpropagation, are not best approximation. For regularization networks (in particular Radial Basis Function networks) we prove existence and uniqueness of best approximation.",MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL INFORMATION PROCESSING WHITAKER COLLEGE,,1
126,,Jonathan E. Hazan and Richard G. Morgan,"Abstract Programmers using imperative languages have a number of well-established debugging tools available to them; functional programmers have few, if any, tools available. Many of the tools and techniques developed for debugging functional programs are based on those for imperative programming and lack a theoretical basis relevant to functional programming. In addition, the techniques used are typically very time-consuming. A theoretical foundation on which to base the study of errors and debugging in functional programming is presented in this report. Using this theoretical foundation, a set of program transformation schemes has been developed which facilitate the location of the type of error which results in an evaluation-time error message and the termination of evaluation. A brief description of the practical experience ob tained using the tool is also presented.","Artificial Intelligence Systems Research Group Computer Science Division School of Engineering and Computer Science University of Durham,","DH1 3LE, UK",5
127,SEQUOIA 2000 -- A REFLECTION ON THE FIRST THREE YEARS,Michael Stonebraker,"Abstract This paper describes the SEQUOIA 2000 project and its implementation efforts during the first three years. Included are the objectives we had, how we chose to address them and some of the lessons we learned from this endeavor.","EECS Department University of California, Berkeley",,1
128,Parallelization of Linearized Applications in Fortran D,Lorie M. Liebrock Ken Kennedy,,Center for Research on Parallel Computation Rice University,"P.O. Box 1892 Houston, TX 77251-1892",6
129,Deterministic Parallel Fortran,K. Mani Chandy Ian Foster,"Abstract We describe Fortran M, message-passing extensions to Fortran 77 that provide deterministic execution and information hiding while preserving desirable properties of message passing.",,,5
130,Optimizing Fortran90D/HPF for Distributed-Memory Computers,by Gerald H. Roth,,RICE UNIVERSITY,,5
131,Using Multiple Node Types to Improve the Performance of DMP (Dynamic Multilayer Perceptron),Tim L. Andersen and Tony R. Martinez,"ABSTRACT This paper discusses a method for training multilayer perceptron networks called DMP2 (Dynamic Multilayer Perceptron 2). The method is based upon a divide and conquer approach which builds networks in the form of binary trees, dynamically allocating nodes and layers as needed. The focus of this paper is on the effects of using multiple node types within the DMP framework. Simulation results show that DMP2 performs favorably in comparison with other learning algorithms, and that using multiple node types can be beneficial to network performance.","Computer Science Department, Brigham Young University,","Provo, Utah 84602",5
132,The weakest reasonable memory model,by Matteo Frigo,,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,,1
133,ON THE POWER OF TWO-POINTS BASED SAMPLING,Benny Chor Oded Goldreich flfl,"Abstract | The purpose of this note is to present a new sampling technique and to demonstrate some of its properties. The new technique consists of picking two elements at random, and deterministically generating (from them) a long sequence of pairwise independent elements. The sequence is guarantees to intersect, with high probability, any set of non-negligible density.",MIT Laboratory for Computer Science,"Cambridge, Massachusetts 02139",5
134,Eventually-Serializable Data Services,Alan Fekete David Gupta Victor Luchangco Nancy Lynch Alex Shvartsman,"Abstract We present a new specification for distributed data services that trade-off immediate consistency guarantees for improved system availability and efficiency, while ensuring the long-term consistency of the data. An eventually-serializable data service maintains the operations requested in a partial order that gravitates over time towards a total order. It provides clear and unambiguous guarantees about the immediate and long-term behavior of the system. To demonstrate its utility, we present an algorithm, based on one of Ladin, Liskov, Shrira, and Ghemawat [12], that implements this specification. Our algorithm provides the interface of the abstract service, and generalizes their algorithm by allowing general operations and greater flexibility in specifying consistency requirements. We also describe how to use this specification as a building block for applications such as directory services.",,,1
135,Proactive RSA,Yair Frankel Peter Gemmell Philip D. MacKenzie Moti Yung x,"Abstract The notion of ""proactive security"" of basic primitives and cryptosystems that are distributed amongst various servers, was introduced in order to tolerate a very strong ""mobile adversary."" This adversary may corrupt all participants throughout the lifetime of the system in a non-monotonic fashion (i.e. recoveries are possible) but the adversary is unable to corrupt too many participants during any short time period [OstrovskyYung]. The notion assures increased security and availability of the cryptographic primitive. We present a proactive RSA system in which a threshold of servers applies the RSA signature (or decryption) function in a distributed manner; RSA is perhaps the most important trapdoor function in use. Employing new combinatorial and elementary number theoretic techniques, our protocol enables the dynamic updating of the servers (which hold the RSA key distributively); it is secure even when a linear number of the servers are corrupted during any time period (linear redundancy); it efficiently ""self-maintains"" the security of the function and its messages (ciphertexts or signatures); and it enables continuous availability, namely, correct function application using the shared key is possible at any time. We present an efficient way in which l servers can share an RSA private function so that, given 0 &lt; &lt; t &lt; 1: * Proactive (Dynamic) Robustness: A gateway G can combine information from any set of lt (honest) servers to deduce the RSA signature for any authorized message at any period. * Proactive Security (against mobile adversary): Our protocol is secure against a polynomial time adversary who controls the gateway G and time-variant sets of up to minfl(1 t ); lg servers, and can obtain the shares of up to l servers (including those that it corrupts). * Uniform Boundedness: The share-size is always bounded by the size of an RSA private key (i.e., logarithmically in N ). We also present special practical instances based on designs; some of these instances were recently implemented as part of a highly secure application testbed at Sandia National Laboratories. A major technical difficulty in ""proactivizing"" RSA was the fact that the servers have to update the ""distributed representation"" of an RSA key, while not learning the order of the group from which keys are drawn (in order not to compromise the RSA security).","Sandia National Labs,","P.O Box 5800, Albuquerque, NM 87185-1110,",1
136,Facial Expression Recognition using a Dynamic Model and Motion Energy,Irfan A. Essa and Alex P. Pentland,"Abstract Previous efforts at facial expression recognition have been based on the Facial Action Coding System (FACS), a representation developed in order to allow human psychologists to code expression from static facial mugshots. In this paper we develop new, more accurate representations for facial expression by building a video database of facial expressions and then probabilistically characterizing the facial muscle activation associated with each expression using a detailed physical model of the skin and muscles. This produces a muscle-based representation of facial motion, which is then used to recognize facial expressions in two different ways. The first method uses the physics-based model directly, by recognizing expressions through comparison of estimated muscle activations. The second method uses the physics-based model to generate spatio-temporal motion-energy templates of the whole face for each different expression. These simple, biologically-plausible motion energy templates are then used for recognition. Both methods show substantially greater accuracy at expression recognition than has been previously achieved.","Perceptual Computing Group, The Media Laboratory, Massachusetts Institute of Technology","Cambridge, MA 02139, U.S.A.",1
137,Multimodal Person Recognition using Unconstrained Audio and Video,"Tanzeem Choudhury, Brian Clarkson, Tony Jebara, Alex Pentland",Abstract We propose a person identification technique that can recognize and verify people from unconstrained video and audio. We do not expect fully frontal face image or clean speech as our input. Our recognition algorithm can detect and compensate for pose variation and changes in the auditory background and also select the most reliable video frame and audio clip to use for recognition. We also use 3D depth information of a human head to detect the presence of an actual person as opposed to an image of that person. Our system achieves 100% recognition and verification rates on natural real-time input with 26 registered clients.,Perceptual Computing Group MIT Media Laboratory,"Cambridge, MA 02139",1
138,PARTIAL SHAPE MATCHING USING GENETIC ALGORITHMS,Ender Ozcan and Chilukuri K. Mohan,"Abstract Shape recognition is a challenging task when images contain overlapping, noisy, occluded, partial shapes. This paper addresses the task of matching input shapes with model shapes described in terms of features such as line segments and angles. The quality of matching is gauged using a measure derived from attributed shape grammars. We apply genetic algorithms to the partial shape-matching task. Preliminary results, using model shapes with 6 to 70 features each, are extremely encouraging.","Department of Electrical Engineering and Computer Science, Syracuse University,","2-120 Center for Science and Technology,",1
139,Finding pattern matchings for permutations,Louis Ibarra,"Abstract Given a permutation P of f1; : : : ; kg and T of f1; : : : ; ng, the pattern matching problem for per mutations is to determine whether there is a length k subsequence of T whose elements are ordered in the same way as the elements of P . We present an O(kn 4 ) time and O(kn 3 ) space algorithm for finding a match of P into T or determining that no match exists, given that P is separable, i.e. contains neither (2, 4, 1, 3) nor (3, 1, 4, 2) as a subpattern.","Dept. of Computer Science Hill Center, Busch Campus Rutgers University","Piscataway, NJ 08855",1
140,An Algorithm for Bayesian Belief Network Construction from Data,"Jie Cheng, David A. Bell, Weiru Liu","Abstract This paper presents an efficient algorithm for constructing Bayesian belief networks from databases. The algorithm takes a database and an attributes ordering (i.e., the causal attributes of an attribute should appear earlier in the order) as input and constructs a belief network structure as output. The construction process is based on the computation of mutual information of attribute pairs. Given a data set which is large enough and has a DAG-Isomorphic probability distribution, this algorithm guarantees that the perfect map [1] of the underlying dependency model is generated, and at the same time, enjoys the time complexity of O N( ) 2 on conditional independence (CI) tests. To evaluate this algorithm, we present the experimental results on three versions of the well-known ALARM network database, which has 37 attributes and 10,000 records. The correctness proof and the analysis of computational complexity are also presented. We also discuss the features of our work and relate it to previous works.",School of Information and Software Engineering University of Ulster at Jordanstown,"Northern Ireland, UK, BT37 0QB",1
141,Evaluation of Architectural Support for Global Address-Based Communication in Large-Scale Parallel Machines,"Arvind Krishnamurthy , Klaus E. Schauser , Chris J. Scheiman , Randolph Y. Wang , David E. Culler , and Katherine Yelick","Abstract Large-scale parallel machines are incorporating increasingly sophisticated architectural support for user-level messaging and global memory access. We provide a systematic evaluation of a broad spectrum of current design alternatives based on our implementations of a global address language on the Thinking Machines CM-5, Intel Paragon, Meiko CS-2, Cray T3D, and Berkeley NOW. This evaluation includes a range of compilation strategies that make varying use of the network processor; each is optimized for the target architecture and the particular strategy. We analyze a family of interacting issues that determine the performance tradeoffs in each implementation, quantify the resulting latency, overhead, and bandwidth of the global access operations, and demonstrate the effects on application performance.",,,4
142,,Extended Capabilities for Visual Cryptography,"Abstract An extended visual cryptography scheme, EVCS for short, for an access structure ( Qual ; Forb ) on a set of n participants, is a technique to encode n images in such a way that when we stack together the transparencies associated to participants in any set X 2 Qual we get the secret message with no trace of the original images, but any X 2 Forb has no information on the shared image. Moreover, after the original images are encoded they are still meaningful, that is, any user will recognize the image on his transparency. The main contributions of this paper are the following: * A trade-off between the contrast of the reconstructed image and the contrast of the image on each transparency for (k; k)-threshold EVCS (in a (k; k)-threshold EVCS the image is visible if and only if k transparencies are stacked together). This yields a necessary and sufficient condition for the existence of (k; k)-threshold EVCS for the values of such contrasts. In case a scheme exists we explicitly construct it. * A general technique to implement extended visual cryptography schemes, which uses hypergraph colourings. This technique yields (k; k)-threshold EVCS which are optimal with respect to the pixel expansion. Finally, we discuss some applications of this technique to various interesting classes of access structures by using relevant results from the theory of hypergraph colourings.","1 Dipartimento di Informatica e Scienze dell'Informazione, Universita di Genova,","via Dodecaneso 35, 16146 Genova, Italy",4
143,Automation Tools for NonDestructive Inspection of Aircraft: Promise of Technology Transfer from the Civilian to the Military Sector,"Chris Seher 1 , Mel Siegel 2 , and William M. Kaufman 3","Abstract The FAA Aging Aircraft Research Program is supporting the development of a robotic mobile nondestructive inspection (NDI) instrument deployment tool at Carnegie Mellon University (CMU) with the active participation of USAir. The program has spawned several new relationships and entities: an alliance with an ARPA-funded research program at CMU having the capability to add 3D-stereoscopic enhanced visual inspection capability, a start-up company organized to commercialize the combined technologies, and State of Pennsylvania funding to foster this commercialization. As a result of these activities and connections the civilian sector appears to be ahead of the military sector in important aspects of automation for deployment of aircraft inspection equipment. A partnership between the university researchers, the airline operator, the start-up company, and the state government is thus emerging as the likely agent for transfer of the civilian-developed technology to the military sector.","1 Federal Aviation Administration, Technical Center,",Atlantic City NJ 08201,0
144,Feature selection for classification based on text hierarchy,Dunja Mladenic and Marko Grobelnik,"Abstract This paper describes automatic document categorization based on large text hierarchy. We handle the large number of features and training examples by taking into account hierarchical structure of examples and using feature selection for large text data. We experimentally evaluate feature subset selection on real-world text data collected from the existing Web hierarchy named Yahoo. In our learning experiments naive Bayesian classifier was used on text data using feature-vector document representation that includes word sequences (n-grams) instead of just single words (unigrams). Experimental evaluation on real-world data collected form the Web shows that our approach gives promising results and can potentially be used for document categorization on the Web. Additionally the best result on our data is achieved for relatively small feature subset, while for larger subset the performance substantially drops. The best performance among six tested feature scoring measure was achieved by the feature scoring measure called Odds ratio that is known from information retrieval.","Department of Intelligent Systems, J.Stefan Institute,","Jamova 39, 1111 Ljubljana, Slovenia",3
145,An Introduction to Software Architecture,David Garlan and Mary Shaw,,School of Computer Science Carnegie Mellon University,"Pittsburgh, PA 15213-3890",6
146,Agile Application-Aware Adaptation for Mobility,"Brian D. Noble, M. Satyanarayanan, Dushyanth Narayanan, James Eric Tilton, Jason Flinn, Kevin R. Walker","Abstract In this paper we show that application-aware adaptation, a collaborative partnership between the operating system and applications, offers the most general and effective approach to mobile information access. We describe the design of Odyssey, a prototype implementing this approach, and show how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it. We present the results of our evaluation of Odyssey, indicating performance improvements up to a factor of 5 on a benchmark of three applications concurrently using remote services over a network with highly variable bandwidth.",School of Computer Science Carnegie Mellon University,,6
147,RECENT ADVANCES IN JANUS: A SPEECH TRANSLATION SYSTEM,"M.Woszczyna, N.Coccaro, A.Eisele, A.Lavie, A.McNair, T.Polzin, I.Rogina, C.P.Rose,T.Sloboda, M.Tomita, J.Tsutsumi, N.Aoki-Waibel, A.Waibel, W. Ward","ABSTRACT We present recent advances from our efforts in increasing coverage, robustness, generality and speed of JANUS, CMU's speech-to-speech translation system. JANUS is a speaker-independent system which translates spoken utterances in English and also in German into one of German, English or Japanese. The system has been designed around the task of conference registration (CR). It has initially been built based on a speech database of 12 read dialogs, encompassing a vocabulary of around 500 words. We have since been expanding the system along several dimensions to improve speed, robustness and coverage and to move toward spontaneous input.",Carnegie Mellon University University of Karlsruhe,,0
148,A Combinatorial Approach to Trajectory Planning for Binary Manipulators,David S. Lees  Gregory S. Chirikjian,"Abstract Binary manipulators are powered by actuators which have only two stable states. Therefore, they can reach only a discrete (but possibly large) number of locations. Compared to a manipulator built with continuous actuators, a binary manipulator provides reasonable performance, and is relatively inexpensive (up to an order of magnitude cheaper). The number of states of a binary manipulator grows exponentially with the number of actuators. This makes the calculation of its inverse kinematics quite difficult. This paper presents a combinatorial method for computing the inverse kinematics of a binary manipulator that reduces the search space to a manageable size. It also creates extremely smooth motions that follow a specified trajectory very accurately (in both position and orientation), despite the discrete nature of binary actuation.","Department of Mechanical Engineering, Johns Hopkins University,","Baltimore, MD 21218",5
149,CC ++ : A Declarative Concurrent Object Oriented Programming Notation,K. Mani Chandy Carl Kesselman,"Abstract CC ++ is Compositional C ++ , a parallel object-oriented notation that consists of C ++ with six extensions. The goals of the CC ++ project are to provide a theory, notation and tools for developing reliable scalable concurrent program libraries, and to provide a framework for unifying: 1. distributed reactive systems, batch-oriented numeric and sym bolic applications, and user-interface systems, 2. declarative programs and object-oriented imperative programs, and 3. deterministic and nondeterministic programs. This paper is a brief description of the motivation for CC ++ , the extensions to C ++ , a few examples of CC ++ programs with reasoning about their correctness, and an evaluation of CC ++ in the context of other research on concurrent computation. A short description of C ++ is provided.",California Institute of Technology,,4
150,Building Scalable Parallel Processors Using Networked Computers - A Tutorial For Synergy V2.0,Yuan Shi,,Temple University SYNERGY,,0
151,Projections: A Preliminary Performance Tool for Charm,Amitabh B. Sinha Laxmikant V. Kale,"Abstract The advent and acceptance of massively parallel machines has made it increasingly important to have tools to analyze the performance of programs running on these machines. Current day performance tools suffer from two drawbacks: they are not scalable and they lose specific information about the user program in their attempt for generality. In this paper, we present Projections, a scalable performance tool, for Charm that can provide program-specific information to help the users better understand the behavior of their programs.",Department of Computer Science Department of Computer Science University of Illinois University of Illinois,"Urbana, IL 61801 Urbana, IL 61801",3
152,Comparison of Distributed Concurrency Control Protocols on a Distributed Database Testbed,Chia-Shiang Shih and Asit Dan,,"ECE Department, University of Massachusetts","Amherst, MA 01003",4
153,On computing global similarity in images,S. Ravela and R. Manmatha,"Abstract The retrieval of images based on their visual similarity to an example image is an important and fascinating area of research. Here, a method to characterize visual appearance for determining global similarity in images is described. Images are filtered with Gaussian derivatives and geometric features are computed from the filtered images. The geometric features used here are curvature and phase. Two images may be said to be similar if they have similar distributions of such features. Global similarity may, therefore, be deduced by comparing histograms of these features. This allows for rapid retrieval and examples from collection of gray-level and trademark images are shown.","Computer Science Department University of Massachusetts,","Amherst, MA 01003",4
154,"Raising Roofs, Crashing Cycles, and Playing Pool: Applications of a Data Structure for Finding Pairwise Interactions",David Eppstein Jeff Erickson,"Abstract The straight skeleton of a polygon is a variant of the medial axis, introduced by Aichholzer et al., defined by a shrinking process in which each edge of the polygon moves inward at a fixed rate. We construct the straight skeleton of an n-gon with r reflex vertices in time O(n 1+"" + n 8=11+"" r 9=11+"" ), for any fixed "" &gt; 0, improving the previous best upper bound of O(nr log n). Our algorithm simulates the sequence of collisions between edges and vertices during the shrinking process, using a technique of Eppstein for maintaining extrema of binary functions to reduce the problem of finding successive interactions to two dynamic range query problems: (1) maintain a changing set of triangles in IR 3 and answer queries asking which triangle would be first hit by a query ray, and (2) maintain a changing set of rays in IR 3 and answer queries asking for the lowest intersection of any ray with a query triangle. We also exploit a novel characterization of the straight skeleton as a lower envelope of triangles in IR 3 . The same time bounds apply to constructing non-self-intersecting offset curves with mitered or beveled corners, and similar methods extend to other problems of simulating collisions and other pairwise interactions among sets of moving objects.","Department of Information and Computer Science, University of California,","Irvine, CA 92697, USA;",1
155,From Ordinal to Euclidean Reconstruction with Partial Scene Calibration,Daphna Weinshall,"Abstract Since uncalibrated images permit only projective reconstruction, metric information requires either camera or scene calibration. We propose a stratified approach to projective reconstruction, in which gradual increase in domain information for scene calibration leads to gradual increase in 3D information. Our scheme includes the following steps: (1) Register the images with respect to a reference plane; this can be done using limited scene information, e.g., the knowledge that two pairs of lines on the plane are parallel. We show that this calibration is sufficient for ordinal reconstruction sorting the points by their height over the reference plane. (2) If available, use the relative height of two additional out-of-plane points to compute the height of the remaining points up to constant scaling. Our scheme is based on the dual epipolar geometry in the reference frame, which we develop below. We show good results with five sequences of real images, using mostly scene calibration that can be inferred directly from the images themselves.",Inst. of Computer Sci. Hebrew University,"91904 Jerusalem, Israel",2
156,On the Analysis of Indexing Schemes,Joseph M. Hellerstein,"Abstract We consider the problem of indexing general database workloads (combinations of data sets and sets of potential queries). We define a framework for measuring the efficiency of an indexing scheme for a workload based on two characterizations: storage redundancy (how many times each item in the data set is stored), and access overhead (how many times more blocks than necessary does a query retrieve). Using this framework we present some initial results, showing upper and lower bounds and trade-offs between them in the case of multi-dimensional range queries and set queries.","Division of Computer Science UC Berkeley,","Berkeley, CA 94720",2
157,Hilbert Series of Group Representations and Grobner Bases for Generic Modules,by Shmuel Onn 1,,DIMACS Rutgers University,"Piscataway, New Jersey 08855-1179",2
158,A Randomized Linear-Time Algorithm for Finding Minimum Spanning Trees,Philip N. Klein  Robert E. Tarjan,Abstract We present a randomized linear-time algorithm for finding a minimum spanning tree in a connected graph with edge weights. The algorithm is a modification of one proposed by Karger and uses random sampling in combination with a recently discovered linear-time algorithm for verifying a minimum spanning tree. Our computational model is a unit-cost random-access machine with the restriction that the only operations allowed on edge weights are binary comparisons.,,,3
159,HIGH PERFORMANCE IMPLEMENTATION OF SERVER DIRECTED I/O,BY MAHESH SUBRAMANIAM,,"University of Illinois at Urbana-Champaign,","Urbana, Illinois",3
160,Efficient Scheduling of Branching Computations on Rings of Processors: An Empirical Study,Lixin Gao Dawn E. Gregory Arnold L. Rosenberg Paul R. Cohen,"Abstract We empirically analyze and compare two simple, deterministic policies for scheduling dynamically evolving tree-structured computations on parallel architectures that are rings of identical processing elements (PEs). Our computations have each task either halt or spawn two independent children; they abstract, for instance, computations generated by multigrid methods. Our simpler policy, called koso, has each PE keep one child of a spawning task and pass the other to its clockwise neighbor in the ring; our more sophisticated policy, called koso ? , operates similarly, but allows child-passing only from a more heavily loaded PE to a more lightly loaded one. Both policies execute waiting tasks in increasing order of their depths in the evolving task-tree. Based on partial (mathematical) analyses of our policies' behaviors, we conjectured that both yield good parallel speedup on large classes of the computations we study, but that policy koso ? outperforms policy koso in many important situations. Not having been able to verify these conjectures analytically, we study them in this paper via a suite of carefully designed and analyzed experiments. Our experiments largely substantiate both of our conjectures. We find that both policies give close to optimal parallel speedup on large classes of computations, and that koso ? outperforms koso on these computations, except on very small processor rings. We believe that our methodology of experimental design and analysis will prove useful in other such studies.",Department of Computer Science University of Massachusetts,"Amherst, Mass. 01003, USA",3
161,The Interaction of Architecture and Operating System Design,"Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Edward D. Lazowska","Abstract Today's high-performance RISC microprocessors have been highly tuned for integer and floating point application performance. These architectures have paid less attention to operating system requirements. At the same time, new operating system designs often have overlooked modern architectural trends which may unavoidably change the relative cost of certain primitive operations. The result is that operating system performance is well below application code performance on contemporary RISCs. This paper examines recent directions in computer architecture and operating systems, and the implications of changes in each domain for the other. The requirements of three components of operating system design are discussed in detail: interprocess communication, virtual memory, and thread management. For each component, we relate operating system functional and performance needs to the mechanisms available on commercial RISC architectures such as the MIPS R2000 and R3000, Sun SPARC, IBM RS6000, Motorola 88000, and Intel i860. Our analysis reveals a number of specific reasons why the performance of operating system primitives on RISCs has not scaled with integer performance. In addition, we identify areas in which architectures could better (and cost-effectively) accommodate operating system needs, and areas in which operating system design could accommodate certain necessary characteristics of cost-effective high-performance microprocessors.",Department of Computer Science and Engineering University of Washington,"Seattle, WA 98195",3
162,Translucent Sums: A Foundation for Higher-Order Module Systems,Mark Lillibridge,,School of Computer Science Carnegie Mellon University,"Pittsburgh, PA 15213",6
163,A Tutorial on Visual Servo Control,Seth Hutchinson,"Abstract This paper provides a tutorial introduction to visual servo control of robotic manipulators. Since the topic spans many disciplines our goal is limited to providing a basic conceptual framework. We begin by reviewing the prerequisite topics from robotics and computer vision, including a brief review of coordinate transformations, velocity representation, and a description of the geometric aspects of the image formation process. We then present a taxonomy of visual servo control systems. The two major classes of systems, position-based and image-based systems, are then discussed. Since any visual servo system must be capable of tracking image features in a sequence of images, we include an overview of feature-based and correlation-based methods for tracking. We conclude the tutorial with a number of observations on the current directions of the research field of visual servo control.",Department of Electrical and Computer Engineering The Beckman Institute for Advanced Science and Technology University of Illinois at Urbana-Champaign,"405 N. Mathews Avenue Urbana, IL 61801",4
164,"Belief Networks, Hidden Markov Models, and Markov Random Fields: a Unifying View",Padhraic Smyth,"Abstract The use of graphs to represent independence structure in multivariate probability models has been pursued in a relatively independent fashion across a wide variety of research disciplines since the beginning of this century. This paper provides a brief overview of the current status of such research with particular attention to recent developments which have served to unify such seemingly disparate topics as probabilistic expert systems, statistical physics, image analysis, genetics, decoding of error-correcting codes, Kalman filters, and speech recognition with Markov models.","Information and Computer Science Department University of California, Irvine",CA 92697-3425.,5
165,Once Upon an Object... Computationally-Augmented Toys for Storytelling,Jennifer W. Glos and Marina Umaschi,"Abstract We are developing design principles applying tangible interfaces to storytelling. This paper describes an underlying philosophy and three resultant designs for computer-mediated toys, exploring how the merging of physical objects with computer technology can enhance childrens storytelling. Each prototype aims to develop a specific set of both oral and written storytelling skills, as well as collaboration, sharing, and the notion of revision. By creating narratives, children learn about culture and identity, and develop a sense of self. In addition, narrative can be used as a gateway to draw girls into technology. The use of multi-sensory interfaces allows for richer interaction.",MIT Media Lab,"20 Ames Street, E15-320R/N Cambridge, MA 02139 USA",4
166,MPI-FM: High Performance MPI on Workstation Clusters,Mario Lauria,"Abstract Despite the emergence of high speed LANs, the communication performance available to applications on workstation clusters still falls short of that available on MPPs. A new generation of efficient messaging layers is needed to take advantage of the hardware performance and to deliver it to the application level. Communication software is the key element in bridging the communication performance gap separating MPPs and workstation clusters. MPI-FM is a high performance implementation of MPI for networks of workstations connected with a Myrinet network, built on top of the Fast Messages (FM) library. Based on the FM version 1.1 released in Fall 1995, MPI-FM achieves a minimum one-way latency of 19 s and a peak bandwidth of 17.3 MByte/s with common MPI send and receive function calls. A direct comparison using published performance figures shows that MPI-FM running on SPARCstation 20 workstations connected with a relatively inexpensive Myrinet network outperforms the MPI implementations available on the IBM SP2 and the Cray T3D, both in latency and in bandwidth, for messages up to 2 KByte in size.","Dipartimento di Informatica e Sistemistica Universita di Napoli ""Federico II""","via Claudio 21 80125 Napoli, Italy",3
167,New-Value Logging in the Echo Replicated File System,"Andy Hisgen, Andrew Birrell, Charles Jerian, Timothy Mann, Garret Swart",,Systems Research Center,"130 Lytton Avenue Palo Alto, California 94301",4
168,Using Global Consistency to Recognise Euclidean Objects with an Uncalibrated Camera,D.A. Forsyth J.L. Mundy A. Zisserman C.A. Rothwell,"Abstract A recognition strategy consisting of a mixture of indexing on invariants and search, allows objects to be recog-nised up to a Euclidean ambiguity with an uncalibrated camera. The approach works by using projective invariants to determine all the possible projectively equivalent models for a particular imaged object; then a system of global consistency constraints is used to determine which of these projectively equivalent, but Euclidean distinct, models corresponds to the objects viewed. These constraints follow from properties of the imaging geometry. In particular, a recognition hypothesis is equivalent to an assertion about, among other things, viewing conditions and geometric relationships between objects, and these assertions must be consistent for hypotheses to be correct. The approach is demonstrated to work on images of real scenes consisting of polygonal objects and polyhedra.",Computer Science General Electric Robotics Research Group Robotics Research Group University of Iowa Center for Research and Development Oxford University Oxford University,"Iowa City, IA 52242 Schenectady, NY 12345 Oxford, UK Oxford, UK",0
169,Importing Pre-packaged Software into Lisp: Experience with Arbitrary-Precision Floating-Point Numbers,Richard J. Fateman,"Abstract We advocate the use of Common Lisp as a powerful glue for building scientific computing environments. Naturally one then has to address mixing pre-existing (non Lisp) code into this system. We provide a specific example as an elaborate FORTRAN system written by David Bailey for arbitrary-precision floating-point numeric calculation. We discuss the advantages and disadvantages of wholesale importing into Lisp. A major advantage is being able to use state-of-the art packaged software sooner, while overcoming the disadvantages caused by FORTRAN's traditional batch orientation and weak storage model. In this paper we emphasize in particular how effective use of imported systems may require one to address the contrast between the functional (Lisp-like) versus state-transition-based (Fortran-like) approaches to dealing with compound objects. While our example is high-precision floats, other highly useful packages including those for simulation, PDE solutions, signal processing, statistical computation, etc. may also benefit by similar consideration.",University of California at Berkeley,,0
170,,,"Abstract TCP is a reliable transport protocol tuned to perform well in traditional networks made up of wired links with stationary hosts. Networks with wireless links and mobile hosts violate many of the assumptions made by TCP, causing degraded performance. In this paper, we describe a simple protocol that improves TCP performance by modifying network-layer software only at a basestation without violating end-to-end TCP semantics. The main idea is to cache packets at the basestation and perform local retransmissions. Simulations of this protocol show that it is significantly more robust in the presence of multiple packet losses in a single transmission window as compared to TCP. This enables our protocol to tolerate at least 10 times as high an error rate without any performance degradation.",,,3
171,Empirical Study of a Dataflow Language on the CM-5,David E. Culler Seth Copen Goldstein Klaus Erik Schauser Thorsten von Eicken,"Abstract: This paper presents empirical data on the behavior of large dataflow programs on a distributed memory multiprocessor. The programs, written in the dataflow language Id90, are compiled via a Threaded Abstract Machine (TAM) for the CM-5. TAM refines dataflow execution models by addressing critical constraints that modern parallel architectures place on the compilation of general-purpose parallel programming languages. It exposes synchronization, scheduling, and network access so that the compiler can optimize against the cost of these operations. The data presented in this paper evaluates the TAM approach in compiling dataflow languages on stock hardware. We present data on the instruction mix, speedup, scheduling behavior, and locality of large ID90 programs. It is shown that the TAM scheduling hierarchy is able to tolerate long communication latencies, especially when some degree of I-structure locality is present. We investigate how frame allocation strategies, k-bounded loops, and I-structure caching and distribution together affect the overall efficiency. Finally we document some scheduling anomalies.","Computer Science Division Department of Electrical Engineering and Computer Sciences College of Engineering University of California, Berkeley",,2
172,Comparing Data Forwarding and Prefetching for Communication-Induced Misses in Shared-Memory MPs 1,David Koufaty 2 and Josep Torrellas,"Abstract As the difference in speed between processor and memory system continues to increase, it is becoming crucial to develop and refine techniques that enhance the effectiveness of cache hierarchies. Two such techniques are data prefetching and data forwarding. With prefetching, a processor hides the latency of cache misses by requesting the data before it actually needs it. With forwarding, a producer processor hides the latency of communication-induced cache misses in the consumer processors by sending the data to the caches of the latter. These two techniques are complementary approaches to hiding the latency of communication-induced misses. This paper compares the effectiveness of data forwarding and data prefetching to hide communication-induced misses. Although both techniques require comparable hardware support, forwarding usually has a lower instruction overhead. We evaluate prefetching and forwarding algorithms in a paralleliz-ing compiler using execution-driven simulations of a shared-memory multiprocessor. Both data forwarding and prefetch-ing reduce the execution time of applications significantly (30-40% on average). Forwarding performs better on average, while prefetching is more robust to changes in cache and memory parameters. Finally, we propose two ways of integrating the two techniques. The integration of the two techniques reduces the execution time even more (43-48% on average) and is very robust.","Department of Computer Science University of Illinois at Urbana-Champaign,",IL 61801,3
173,An Analysis of Message Sequence Charts,Peter B. Ladkin Stefan Leue,,University of Berne Institute for Informatics and Applied Mathematics,"Langgassstrasse 51 CH-3012 Bern, Switzerland",4
174,SDL and MSC Based Test Case Generation - An Overall View of the SAMSTAG Method,Jens Grabowski,,,,2
175,Generation of task-specific segmentation procedures as a model selection task,Ralf Herbrich and Tobias Scheffer,"Abstract In image segmentation problems, there is usually a vast amount of filter operations available, a subset of which has to be selected and instantiated in order to obtain a satisfactory segmentation procedure for a particular domain. In supervised segmentation, a mapping from features, such as filter outputs for individual pixels, to classes is induced automatically. However, since the sample size required for supervised learning grows exponentially in the number of features it is not feasible to learn a segmentation procedure from a large amount of possible filters. But we argue that automatic model selection methods are able to select a region model in terms of some filters. We propose a wrapper algorithm that performs this task. We present results on artificial textured images (Brodatz) and report on our experiences with x-ray images.","Technische Universitat Berlin, Artificial Intelligence Research Group,","Sekr. FR 5-8, Franklinstr. 28/29. D-10587 Berlin, Germany",2
176,Nonlinear Poisson Solve for Boltzmann Electrons,"K. L. Cartwright , J. P. Verboncoeur , and C. K.Birdsall","Abstract Kinetic simulation of plasmas in which equilibrium occurs over ion timescales poses a computational challenge due to the disparate timescales of the electron plasma frequency (~ 10 9 ), the ion plasma frequency (~ 10 7 ), ion transit frequency (~ 10 6 ), and the ionization frequency (~ 10 7 ). Hybrid electrostatic PIC algorithms are presented in which the electrons reach thermodynamic equilibrium with the ions each time step. There are two different approximations for the electrons. First, the nonlinear Boltzmann relationship for the electrons can be applied to the bulk of a plasma. Second, there is a truncated Maxwellian which is used in sheaths; this approximation truncates the electron distribution at the wall potential. The error associated with neglecting this second approximation in the sheath is small. The collision cross section, (E), can be a tabulated or fitted function; the method is implemented with He cross sections. These approximations neglect effects faster than ion time-scales, decreasing the computer time used by over an order of magnitude; however, they increase the complexity of the boundary conditions and the simulation is no longer self-consistent. Theoretical ramifications of these approximations are examined, and results are compared with full","Electronics Research Laboratory University of California,","Berkeley, CA 94720",6
177,Update-in-place Analysis for Sets,Chung Yung,"Abstract This survey paper describes the current approaches on the update-in-place analysis for sets. Pure functional languages do not allow mutations, destructive updates, or selective updates so that straightforward implementations of functional language compilers may induce large amounts of copying to preserve program semantics. The unnecessary copying of data can increase both the execution time and the memory requirements of an application. Introducing sets to functional languages as a primitive data constructor posts a new problem of update-in-place analysis in functional languages. Moreover, most of the compiler optimization techniques depend on the side-effects and the update-in-place analysis serves as the premise of applying such optimization techniques. Among other compiler optimization techniques, finite differencing captures common yet distinctive program constructions of costly repeated calculations and transforms them into more efficient incremental program constructions. This dissertation is an attempt to explore the update-in-place analysis for sets in functional languages in order to apply finite differencing to compiling pure functional languages. In this survey paper, we will describe the approaches of update-in-place analysis and the finite differencing techniques.",Computer Science Department Courant Institute of Mathematical Sciences New York University,,1
178,Improving the Performance of Radial Basis Function Networks by Learning Center Locations,Dietrich Wettschereck and Thomas Dietterich,,Department of Computer Science Oregon State University,"Corvallis, OR 97331-3202",4
179,"System Administration: Monitoring, Diagnosing, and Repairing",Eric Anderson,"We first describe the general goals of system administration explaining the relationship to monitoring, diagnosing, and repairing (MDR). Then, we describe the functional and environmental concerns for a MDR system, and use these concerns to explain how previous approaches have failed to fully address the problem. Next, we describe the major pieces of our approach, and identify the research questions associated with each piece. Finally we present a method for testing the system when it is created.",,,3
180,RTsynchronizer: Language Support for  Real-Time Specifications in Distributed Systems,Shangping Ren and Gul A. Agha,"Abstract We argue that the specification of an object's functional behavior and the timing constraints imposed on it may be separated. Specifically, we describe RTsynchronizer, a high-level programming language construct for specifying real-time constraints between objects in a distributed concurrent system. During program execution, RTsynchronizers affect the scheduling of distributed objects to enforce real-time relations between events. Objects in our system are defined in terms of the actor model extended with timing assumptions. Separation of the functional behaviors of actors and the timing constraints on patterns of actor invocation provides at least three important advantages. First, it simplifies code development by separating design concerns. Second, multiple timing constraints can be independently specified and composed. And finally, a specification of timing constraints can be reused even if the representation of the functional behavior of actors has changed, and conversely. A number of examples are given to illustrate the use of RTsynchronizers. These examples illustrate how real-time constraints for periodic events, simultaneous events, exception handling, and producer-consumer may be specified.",Department of Computer Science,1304 W. Springfield Avenue,3
181,Billing Users and Pricing for TCP,"Richard J. Edell, Nick McKeown and Pravin P. Varaiya","Abstract| This paper presents a system for billing users for their TCP traffic. This is achieved by postponing the establishment of connections while the user is contacted, verifying in a secure way that they are prepared to pay. By presenting the user with cost and price information, the system can be used for cost recovery and to encourage efficient use of network resources. The system requires no changes to existing protocols or applications and can be used to recover costs between cooperating sites. Statistics collected from a four day trace of traffic between the University of Califor-nia, Berkeley and the rest of the Internet demonstrate that such a billing system is practical and introduces acceptable latency. An implementation based on the BayBridge prototype router is described. Our study also indicates that pricing schemes may be used to control network congestion either by rescheduling time-insensitive traffic to a less expensive time of the day, or by smoothing packet transfers to reduce traffic peaks.",,,3
182,"A Comparative Study of Three Paradigms for Object Recognition Bayesian Statistics, Neural Networks and Expert Systems.","J. K. Aggarwal, Joydeep Ghosh, Dinesh Nair and Ismail Taha","Abstract Object recognition, which involves the classification of objects into one of many a priori known object types, and determining object characteristics such as pose, is a difficult problem. A wide range of approaches have been proposed and applied to this problem with limited success. This paper presents a brief comparative study of methods from three different paradigms for object recognition: Bayesian, Neural Network and Expert Systems.","Computer and Vision Research Center The University of Texas at Austin,","Austin, TX, USA",6
183,DART/HYESS Users Guide,Jerome H. Friedman,Abstract This note provides information for using the Fortran program [Friedman (1996b)] that imple ments the recursive covering approach to local learning described in Friedman (1996a).,Department of Statistics and Stanford Linear Accelerator Center Stanford University,,3
184,A PRELIMINARY STUDY OF HIERARCHICAL FINITE STATE MACHINES WITH MULTIPLE CONCURRENCY MODELS,"by Alain Girault, Bilung Lee, and Edward A. Lee",,DEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE UNIVERSITY OF CALIFORNIA,"BERKELEY, CALIFORNIA 94720",3
185,Switching through Singularities,C. J. Tomlin and S. S. Sastry,"Abstract Asymptotic tracking is studied for systems in which the relative degree is not well defined, meaning that the control law derived from exact input-output linearization has singularities in the state space. We propose a tracking control law which switches between approximate tracking [1] close to the singularities, and exact tracking away from the singularities, and we study the applicability of this law based on the behavior of the system's zero dynamics at the switching boundary. As in [1], the ball and beam example is used to motivate the study.","Department of Electrical Engineering and Computer Sciences University of California,",Berkeley CA 94720,1
186,Construction of Fuzzy Linguistic Model,Tak-Kuen John Koo,"Abstract Using linguistic variables to describe the behavior of a hybrid system, which consists of a discrete event system and a continuous system, could make the design of the controller and verification of the system perform on an unified framework. In this paper, we show the construction of a fuzzy linguistic model from a given mathematical model of a physical system. By considering the state-space realization of the model, the system construction problem can be transformed into a function approximation problem. We propose to use projection theorem in obtaining an optimal fuzzy system which is the best approximation of a given nonlinear function in L 2 (U ) space. We show that the existence and uniqueness of the optimal solution is assured when the Fuzzy Basis Functions(FBFs) are linearly independent. We demonstrate that the dependence of the basis functions can be examined by checking the condition of the Gram determinant associated to the FBFs. Finally, a method is proposed in converting the optimal coefficients into fuzzy sets to obtain a fuzzy linguistic model.",Robotics and Intelligent Machines Laboratory,"211-85 Cory Hall,",3
187,A Fault Tolerant Control Architecture for Automated Highway Systems,"John Lygeros, Datta N. Godbole, Mireille Broucke","Abstract We propose a hierarchical control architecture for dealing with faults and adverse environmental conditions on an Automated Highway System (AHS). Our design extends a previous control architecture that works under normal conditions of operation. The faults that are considered in our design are classified according to the capabilities remaining on the vehicle or roadside after the fault has occurred. Information about these capabilities is used by supervisors in each of the layers of the architecture to select appropriate control strategies. We outline the extended control strategies that are needed by these supervisors and, in certain cases, give examples of their detailed operation.","Department of Electrical Engineering and Computer Sciences University of California, Berkeley","Berkeley, CA 94720",3
188,SmartATMS : A SIMULATOR FOR AIR TRAFFIC MANAGEMENT SYSTEMS,Tak-Kuen John Koo Yi Ma George J. Pappas Claire Tomlin,"ABSTRACT Air Traffic Management Systems (ATMS) of the future will feature Free Flight, in which aircraft choose their own routes, altitude, and speed, and automated conflict resolution methods in which aircraft will coordinate to resolve conflicts. The resulting distributed control architecture is a hybrid system, with mixed discrete event and continuous time dynamics. SmartATMS is an object oriented modeling and simulation facility which accounts for these hybrid issues and will serve as a uniform modeling framework for the design and evaluation of various ATMS concepts.",Robotics and Intelligent Machines Laboratory Department of Electrical Engineering and Computer Sciences University of California at Berkeley,"Berkeley, CA 94720",4
189,FROM KNOWLEDGE TO BELIEF,By Daphne Koller,,,,2
190,Tioga: Providing Data Management Support for Scientific Visualization Applications,"Michael Stonebraker, Jolly Chen, Nobuko Nathan, Caroline Paxson, Jiang Wu","Abstract We present a user interface paradigm for database management systems that is motivated by scientific visualization applications. Our graphical user interface includes a ""boxes and arrows"" notation for database access and a flight simulator model of movement through information space. We also provide means to specify a hierarchy of abstracts of data of different types and resolutions, so that a ""zoom"" capability can be supported. The underlying DBMS support for this system is described and includes the compilation of query plans into megaplans, new algorithms for data buffering, and provisions for a guaranteed rate of data delivery. The current state of the Tioga implementation is also described.","Computer Science Division, EECS Department University of California","Berkeley, CA 94720",2
191,GEN++ | an analyzer generator for C++ programs,Prem Devanbu,,Articifial Intelligence Principles Research Department,,6
192,Dealing With Disaster: Surviving Misbehaved Kernel Extensions,"Margo I. Seltzer, Yasuhiro Endo, Christopher Small, Keith A. Smith",,Harvard University,,1
193,Efficient Restoration of Multicolor Images with Independent Noise,Yuri Boykov Olga Veksler Ramin Zabih,"Abstract We consider the problem of maximum a posteriori (MAP) restoration of multicolor images where each pixel has been degraded by independent arbitrary noise. We assume that the prior distribution is given by a Markov random field with only pairwise site interactions. Two classes of site interactions are considered: two-valued site interactions, which form a generalized Potts model; and linear site interactions. We give efficient algorithms based on graph cuts for both classes. The MAP estimate for a generalized Potts model can be computed by solving a multiway minimum cut problem on a graph. While this graph problem is computationally intractable, there are fast algorithms for computing provably good approximations. The MAP estimate with linear site interactions can be computed exactly by solving a minimum cut problem on a graph. This can be performed in nearly linear time.","Cornell University,",USA,2
194,Client/Server Architectures for Business Information Systems A Pattern Language,Klaus Renzel,"Abstract: This paper presents several patterns for distributing business information systems that are structured according to a layered architecture. 2 Each distribution pattern cuts the architecture into different client and server components. All the patterns presented give an answer to the same question: How do I distribute a business information system? However, the consequences of applying the patterns are very different with regards to the forces influencing distributed systems design.",sd&m GmbH & Co. KG Project ARCUS 1,"Thomas-Dehler-Str. 27 D-81737 Mnchen, Germany",5
195,Semantic Query Caching for Heterogeneous Databases,Parke Godfrey,"Abstract Query caching can play a vital role in heterogeneous, multi-database environments. Answers to a query that are available in cache at the local client can be returned to the user quickly, while the rest of the query is evaluated. The use of caches can optimize query evaluation. By caching certain sensitive data locally, caches can be used to answer the parts of queries that involve the sensitive data, so it need not be shipped across the network. Most prior cache schemes have been tuple-based or page-based. It is unclear, however, how these might be adapted for multi-databases. We explore a more flexible semantic query caching (SQC) approach. In SQC, caches are the answer sets of previous queries, labeled by the query expressions that produced them. We promote developing the technology, based on logic, to manipulate semantic caches, to determine when and how caches can be used to answer subsequent queries, and to optimize via cache use.",U.S. Army Research Laboratory,"2800 Powder Mill Road Adelphi, Maryland 20783-1197 U.S.A.",4
196,Finding Maximum Flows in Undirected Graphs Seems Easier than Bipartite Matching +L,David R. Karger and Matthew S. Levine,"Abstract Consider an n-vertex, m-edge, undirected graph with maximum flow value v. We give a method to find augmenting paths in such a graph in amortized sub-linear (O(n p v)) time per path. This lets us improve the time bound of the classic augmenting path algorithm to O(m + nv 3=2 ) on simple graphs. The addition of a blocking flow subroutine gives a simple, deterministic O(nm 2=3 v 1=6 )-time algorithm. We also use our technique to improve known randomized algorithms, giving O(m+nv 5=4 )-time and O(m+n 11=9 v)-time algorithms for capacitated undirected graphs. For simple graphs, in which v n, the last bound is O(n 2:2 ), improving on the best previous bound of O(n 2:5 ), which is also the best known time bound for bipartite matching.",,,0
197,Electronic Lottery Tickets as Micropayments,Ronald L. Rivest,"Abstract. We present a new micropayment scheme based on the use of ""electronic lottery tickets."" This scheme is exceptionally efficient since the bank handles only winning tickets, instead of handling each micro payment.",MIT Lab for Computer Science (RSA / Security Dynamics),,4
198,Compiler Technology for Portable Checkpoints,Volker Strumpen,"Abstract We have implemented a prototype compiler called porch that transforms C programs into C programs supporting portable checkpoints. Portable checkpoints capture the state of a computation in a machine-independent format that allows the transfer of computations across binary incompatible machines. We introduce source-to-source compilation techniques for generating code to save and recover from such portable checkpoints automatically. These techniques instrument a program with code that maps the state of a computation into a machine-independent representation and vice versa. In particular, the following problems are addressed: (1) providing stack environment portability, (2) enabling conversion of complex data types, and (3) rendering pointers portable. Experimental results show that the overhead of checkpointing is reasonably small, even if data representation conversion is required for portability.",Laboratory for Computer Science Massachusetts Institute of Technology,"Cambridge, MA 02139",3
199,Task Driven Perceptual Organization for Extraction of Rooftop Polygons,Christopher Jaynes Frank Stolle Robert Collins,"Abstract A new method for extracting planar polygonal rooftops in monocular aerial imagery is proposed. Through bottom-up and top-down construction of perceptual groups, polygons in a single aerial image can be robustly extracted. Orthogonal corners and lines are extracted and hierarchically related using perceptual grouping techniques. Top-down feature verification is used so that features, and links between the features, are verified with local information in the image and weighed in a graph structure according to the underlying support for each feature. Cycles in the graph correspond to possible building rooftop hypotheses. Virtual features are hypothesized for the perceptual completion of partial rooftops. Extraction of the ""best"" grouping of features into a building rooftop hypothesis is posed as a graph search problem. The maximally weighted, independent set of cycles in the graph is extracted as the final set of roof boundaries.",Computer Science Department University of Massachusetts,"Amherst, MA 01003",4
200,Determining 3-D Hand Motion,James Davis Mubarak Shah,"Abstract This paper presents a glove-free method for tracking hand movements using a set of 3-D models. In this approach, the hand is represented by five cylindrical models which are fit to the third phalangeal segments of the fingers. Six 3-D motion parameters for each model are calculated that correspond to the movement of the fingertips in the image plane. Trajectories of the moving models are then established to show the 3-D nature of hand motion.",Media Lab Computer Vision Lab Massachusetts Institute of Technology University of Central Florida,"Cambridge, MA 02139 Orlando, FL 32826",4
201,Efficient Gate Delay Modeling for Large Interconnect Loads,Andrew B. Kahng and Sudhakar Muddu,"Abstract With fast switching speeds and large interconnect trees (MCMs), the resistance and inductance of interconnect has a dominant impact on logic gate delay. In this paper, we propose a new P model for distributed RC and RLC interconnects to estimate the driving point admittance at the output of a CMOS gate. Using this model we are able to compute the gate delay efficiently, within 25% of SPICE-computed delays. Our parameters depend only on total interconnect tree resistance and capacitance at the output of the gate. Previous effective load capacitance methods [7, 9], applicable only for distributed RC interconnects, are based on P model parameters obtained via a recursive admittance moment computation. Our model should be useful for iterative optimization of performance-driven routing or for estimation of gate delay and rise times in high-level synthesis.","UCLA Computer Science Department,","Los Angeles, CA 90095-1596 USA",3
202,Robust IP Watermarking Methodologies for Physical Design,"Andrew B. Kahng, Stefanus Mantik, Igor L. Markov, Miodrag Potkonjak, Paul Tucker , Huijuan Wang and Gregory Wolfe","Abstract Increasingly popular reuse-based design paradigms create a pressing need for authorship enforcement techniques that protect the intellectual property rights of designers. We develop the first intellectual property protection protocols for embedding design watermarks at the physical design level. We demonstrate that these protocols are transparent with respect to existing industrial tools and design flows, and that they can embed watermarks into real-world industrial designs with very low implementation overhead (as measured by such standard metrics as wirelength, layout area, number of vias, routing congestion and CPU time). On several industrial test cases, we obtain extremely strong, tamper-resistant proofs of authorship for placement and routing solutions.","UCLA Computer Science Dept.,","Los Angeles, CA 90095-1596",3
203,Quick Simulation of ATM Buffers with On-off Multiclass Markov Fluid Sources 1,G. Kesidis,"Abstract The problem we address is how to quickly estimate by simulation the loss in a buffer with multiclass on-off Markov fluid sources. We generate the Markov fluids with the altered rate matrices given in [11], instead of the originals, to speed up the simulation. Likelihood ratios are used to recover an estimate of the loss for the original traffic parameters.","E & CE Dept, University of Waterloo,","Waterloo, Ontario, N2L 3G1, Canada.",2
204,Inferring Reduced Ordered Decision Graphs of Minimal Description Length,Arlindo L. Oliveira  Alberto Sangiovanni-Vincentelli,,"Dept. of EECS, UC Berkeley,",Berkeley CA 94720,4
205,VIS : A System for Verification and Synthesis,Robert K. Brayton Gary D. Hachtel Alberto Sangiovanni-Vincentelli   Fabio Somenzi Adnan Aziz Szu-Tsung Cheng Stephen Edwards Sunil Khatri  Yuji Kukimoto Abelardo Pardo Shaz Qadeer Rajeev K. Ranjan  Shaker Sarwary Thomas R. Shiple Gitanjali Swamy Tiziano Villa,,,,0
206,Using Don't Cares in Logic Minimization for LUT-Based FPGAs,Philip Chong  13327872,"Abstract Don't care information has proven to be useful in logic minimization. Here, the use of don't care information in network collapsing for mapping to LUT-based FPGAs is explored. Results are shown which indicate that this approach does not result in appreciable improvements in network size.",,,3
207,Modeling TCP Throughput: A Simple Model and its Empirical Validation,Jitendra Padhye Victor Firoiu Don Towsley Jim Kurose,"Abstract In this paper we develop a simple analytic characterization of the steady state throughput, as a function of loss rate and round trip time for a bulk transfer TCP flow, i.e., a flow with an unlimited amount of data to send. Unlike the models in [6, 7, 10], our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more timeout events than fast retransmit events. Our measurements demonstrate that our model is able to more accurately predict TCP throughput and is accurate over a wider range of loss rates. This material is based upon work supported by the National Science Foundation under grants NCR-95-08274, NCR-95-23807 and CDA-95-02639. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",Department of Computer Science  University of Massachusetts,"LGRC, Box 34610 Amherst, MA 01003-4610 USA",2
208,Use of Architecture-Altering Operations to Dynamically Adapt a Three-Way Analog Source Identification Circuit to Accommodate a New Source,John R. Koza,"ABSTRACT The problem of source identification involves correctly classifying an incoming signal into a category that identifies the signal's source. The problem is difficult because information is not provided distinguishing characteristics and because successive signals from the same source differ. The source identification problem can be made more difficult by dynamically changing the repertoire of sources while the problem is being solved. We used genetic programming to evolve both the topology and the sizing (numerical values) for each component of an analog electrical circuit that can correctly classify an incoming analog electrical signal into three categories. Then, the repertoire of sources was dynamically changed by adding a new source during the run. The paper describe show the enabled genetic programming to adapt, during the run, to the changed environment. Specifically, a three-way source identification circuit was evolved and then adapted into a four-way classifier, during the run, thereby successfully handling the additional new source.",Computer Science Dept. Stanford University,"Stanford, California 94305-9020",3
209,Learning to Retrieve Information,Brian Bartell,"Abstract Information retrieval differs significantly from function approximation in that the goal is for the system to achieve the same ranking function of documents relative to queries as the user: the outputs of the system relative to one another must be in the proper order. We hypothesize that a particular rank-order statistic, Guttman's point alienation, is the proper objective function for such a system, and demonstrate its efficacy by using it to find the optimal combination of retrieval experts. In application to a commercial retrieval system, the combination performs 47% better than any single expert.","Encylopdia Britannica and Institute for Neural Computation Computer Science & Engineering University of California, San Diego","La Jolla, California 92093",3
210,Higher Bandwidth X,John Danskin,"Abstract Network bandwidth has always been a key issue for multimedia protocols. Many potential users of networked multimedia protocols will continue to have low bandwidth network connections for some time: copper wire ISDN, infra-red, cellular modems, etc.. Compression provides potential relief for users of slow networks by increasing effective bandwidth. HBX introduces a new technique, based on arithmetic coding and statistical modeling, for compressing structured data. Applied to the X networked graphics protocol, this technique yields 4.5:1 compression across a representative set of traces, performing twice as well as the popular LZW-based Xremote compression protocol. HBX's coding techniques are generally applicable to the graphics and imaging subset of multimedia protocols. Future work will determine whether HBX's coding techniques can be applied to audio and video streams as well.",Princeton University Computer Science Department,"Princeton NJ, 08540",2
211,,David Laur and Pat Hanrahan,"Abstract This paper presents a progressive refinement algorithm for volume rendering which uses a pyramidal volume representation. Besides storing average values, the pyramid stores estimated error, so an oct-tree can be fit to the pyramid given a user-supplied precision. This oct-tree is then drawn using a set of splats, or footprints, each scaled to match the size of the projection of a cell. The splats themselves are approximated with RGBA Gouraud-shaded polygons, so that they can be drawn efficiently on modern graphics workstations. The result is a real-time rendering algorithm suitable for interactive applications.",Princeton University,"Princeton, NJ 08544, USA",4
212,Robust Meshes from Multiple Range Maps,Kari Pulli Tom Duchamp Hugues Hoppe  John McDonald Linda Shapiro Werner Stuetzle,Abstract This paper presents a method for modeling the surface of an object from a sequence of range maps. Our method is based on a volumetric approach that produces a compact surface without boundary. It provides robustness through the use of interval analysis techniques and computational efficiency through hierarchical processing using octrees.,"University of Washington,","Seattle, WA",0
213,Dis-equality Constraints in Linear/Integer Programming,Mozafar T. Hajian,Abstract We have proposed an extension to the definition of general integer linear programs (ILP) to accept dis-equality constraints explicitly. A new class of logical variables is introduced to transform the extended ILP in general form to standard form. Branch and Bound algorithm is modified to solve this new class of ILP.,"IC-Parc, William Penny Laboratory, Imperial College,","London, SW7 2AZ.",0
214,A Parallel Implementation of an MPEG1 Encoder: Faster Than Real-Time!,"Ke Shen , Lawrence A. Rowe and Edward J. Delp",ABSTRACT In this paper we present an implementation of an MPEG1 encoder on the Intel Touchstone Delta and Intel Paragon parallel computers. We describe the unique aspects of mapping the algorithm onto the parallel machines and present several versions of the algorithms. We will show that I/O contention can be a bottleneck relative to performance. We will also describe how the Touchstone Delta and Paragon can be used to compress video sequences faster than real-time.,Computer Vision and Image Processing Laboratory School of Electrical Engineering Purdue University,"West Lafayette, Indiana",6
215,Structuring Graphical Paradigms in TkGofer,Koen Claessen,"Abstract In this paper we describe the implementation of several graphical programming paradigms (Model View Controller, Fudgets, and Functional Animations) using the GUI library TkGofer. This library relies on a combination of monads and multiple-parameter type classes to provide an abstract, type safe interface to Tcl/Tk. We show how choosing the right abstractions makes the given implementations surprisingly concise and easy to understand.",OGI and Utrecht University,,0
216,Disk Packings and Planar Separators,Daniel A. Spielman,"Abstract We demonstrate that the geometric separator algorithm of Miller, Teng, Thurston, and Vavasis finds a 3=4-separator of size 1:84 p n for every n node planar graph. Our bound is derived from an analysis of disk packings on the sphere.",U.C. Berkeley/MIT,,3
217,Building Interpreters by Composing Monads,Guy L. Steele Jr.,"Abstract: We exhibit a set of functions coded in Haskell that can be used as building blocks to construct a variety of interpreters for Lisp-like languages. The building blocks are joined merely through functional composition. Each building block contributes code to support a specific feature, such as numbers, continuations, functions calls, or nondeterminism. The result of composing some number of building blocks is a parser, an interpreter, and a printer that support exactly the expression forms and data types needed for the combined set of features, and no more. The data structures are organized as pseudomonads, a generalization of monads that allows composition. Functional composition of the building blocks implies type composition of the relevant pseudomonads. Our intent was that the Haskell type resolution system ought to be able to deduce the approprate data types automatically. Unfortunately there is a deficiency in current Haskell implementations related to recursive data types: circularity must be reflected statically in the type definitions. We circumvent this restriction by applying a purpose-built program simplifier that performs partial evaluation and a certain amount of program algebra. We construct a wide variety of interpreters in the style of Wadler by starting with the building blocks and a page of boiler-plate code, writing three lines of code (one to specify the building blocks and two to (redundantly) specify type compositions), and then applying the simplifier. The resulting code is acceptable Haskell code. We have tested a dozen different interpreters with various combinations of features. In this paper we discuss the overall code structuring strategy, exhibit several building blocks, briefly describe the partial evaluator, and present a number of automatically generated interpreters.",Thinking Machines Corporation,"245 First Street Cambridge, Massachusetts 02142",3
218,Theory and Design of Multidimensional QMF Sub-Band Filters From 1-D Filters Using Transforms,I.A. Shah A.A.C. Kalker,"Abstract The paper presents the general theory of designing multidimensional Quadrature Mirror Filters (QMF), for use in sub-band coding (SBC) systems, using the McClellan transform [1]. It was recently shown that McClellan transform could be used to generate 2-D diamond shape QMF filters [2]. In this paper we will formalize the proofs of the diamond shape case, and generalize it to other shapes, sampling rasters and dimensions. Examples are given of two dimensional diamond shape filters and three dimensional tetrad filters designed using the technique.","Philips Research Laboratories,","P.O. Box 80.000, 5600 JA Eindhoven, The Netherlands",4
219,"Quantized Overcomplete Expansions in R N Analysis, Synthesis, and Algorithms","Vivek K Goyal, Student Member, IEEE, Martin Vetterli, Fellow, IEEE, and Nguyen T. Thao, Member, IEEE","Abstract|Coefficient quantization has peculiar qualitative effects on representations of vectors in R N with respect to overcomplete sets of vectors. These effects are investigated in two settings: frame expansions (representations obtained by forming inner products with each element of the set) and matching pursuit expansions (approximations obtained by greedily forming linear combinations). In both cases, based on the concept of consistency, it is shown that traditional linear reconstruction methods are suboptimal, and better consistent reconstruction algorithms are given. The proposed consistent reconstruction algorithms were in each case implemented, and experimental results are included. For frame expansions, results are proven to bound distortion as a function of frame redundancy r and quantization step size for linear, consistent, and optimal reconstruction methods. Taken together, these suggest that optimal reconstruction methods will yield O(1=r 2 ) MSE, and that consistency is sufficient to insure this asymptotic behavior. A result on the asymptotic tightness of random frames is also proven. Applicability of quantized matching pursuit to lossy vector compression is explored. Experiments demonstrate the likelihood that a linear reconstruction is inconsistent, the MSE reduction obtained with a nonlinear (consistent) reconstruction algorithm, and generally competitive performance at low bit rates.",,,3
220,The Role of Learning in Autonomous Robots,Rodney A. Brooks,"Abstract Applications of learning to autonomous agents (simulated or real) have often been restricted to learning a mapping from perceived state of the world to the next action to take. Often this is couched in terms of learning from no previous knowledge. This general case for real autonomous robots is very difficult. In any case, when building a real robot there is usually a lot of a priori knowledge (e.g., from the engineering that went into its design) which doesn't need to be learned. We describe the behavior-based approach to autonomous robots, and then examine four classes of learning problems associated with such robots.",MIT Artificial Intelligence Laboratory,"545 Technology Square Cambridge, MA 02139",3
221,A Perturbation Scheme for Spherical Arrangements with Application to Molecular Modeling,Dan Halperin,"Abstract We describe a software package for computing and manipulating the subdivision of a sphere by a collection of (not necessarily great) circles and for computing the boundary surface of the union of spheres. We present problems that arise in the implementation of the software and the solutions that we have found for them. At the core of the paper is a novel perturbation scheme to overcome degeneracies and precision problems in computing spherical arrangements while using floating point arithmetic. The scheme is relatively simple, it balances between the efficiency of computation and the magnitude of the perturbation, and it performs well in practice. We report and discuss experimental results. Our package is a major component in a larger package aimed to support geometric queries on molecular models; it is currently employed by chemists working in `rational drug design.' The spherical subdivisions are used to construct a geometric model of a molecule where each sphere represents an atom. We also give an overview of the molecular modeling package and detail additional features and implementation issues.",Tel Aviv University,"Tel Aviv 69978, Israel.",2
222,Taxonomic Syntax for First Order Inference,DAVID MCALLESTER and ROBERT GIVAN,"Abstract: We identify a new polynomial time decidable fragment of first order logic and present a general method for using polynomial time inference procedures in knowledge representation systems. Our results indicate that a non-standard ""taxonomic"" syntax is essential in constructing natural and powerful polynomial time inference procedures. The central role of taxonomic syntax in our polynomial time inference procedures provides technical support for the often expressed intuition that knowledge is better represented in terms of taxonomic relationships than classical first order formulas. To use our procedures in a knowledge representation system we define a ""Socratic proof system"" which is complete for first order inference and which can be used as a semi-automated interface to a first order knowledge base.","Massachusetts Institute of Technology, Cambridge Massachusetts","545 Technology Square, Cambridge Mass, 02139,",2
223,Anatomical origin and computational role of diversity in the response properties of cortical neurons,Kalanit Grill Spectory Shimon Edelmany Rafael Malachz,"Abstract The maximization of diversity of neuronal response properties has been recently suggested as an organizing principle for the formation of such prominent features of the functional architecture of the brain as the cortical columns and the associated patchy projection patterns (Malach, 1994). We report a computational study of two aspects of this hypothesis. First, we show that maximal diversity is attained when the ratio of dendritic and axonal arbor sizes is equal to one, as it has been found in many cortical areas and across species (Lund et al., 1993; Malach, 1994). Second, we show that maximization of diversity leads to better performance in two case studies: in systems of receptive fields implementing steerable/shiftable filters, and in matching spatially distributed signals, a problem that arises in visual tasks such as stereopsis, motion processing, and recognition.",Departments of Applied Mathematics and Computer Science and Neurobiology,"Rehovot 76100, Israel",1
224,Parametric Models are Versatile: The Case of Model Based Optimization,P. Fua,"Abstract Model-Based Optimization (MBO) is a paradigm in which an objective function is used to express both geometric and photometric constraints on features of interest. A parametric model of a feature (such as a road, a building, or coastline) is extracted from one or more images by adjusting the model's state variables until a minimum value of the objective function is obtained. The optimization procedure yields a description that simultaneously satisfies (or nearly satisfies) all constraints, and, as a result, is likely to be a good model of the feature.",Artificial Intelligence Center SRI International,"333 Ravenswood Avenue Menlo Park, California 94025",2
225,Cooperative Bayesian and Case-Based Reasoning for Solving Multiagent Planning Tasks,David W. Aha & Li Wu Chang,"Abstract We describe an integrated problem solving architecture named INBANCA in which Bayesian networks and case-based reasoning (CBR) work cooperatively on multiagent planning tasks. This includes two-team dynamic tasks, and this paper concentrates on simulated soccer as an example. Bayesian networks are used to characterize action selection whereas a case-based approach is used to determine how to implement actions. This paper has two contributions. First, we survey integrations of case-based and Bayesian approaches from the perspective of a popular CBR task decomposition framework, thus explaining what types of integrations have been attempted. This allows us to explain the unique aspects of our proposed integration. Second, we demonstrate how Bayesian nets can be used to provide environmental context, and thus feature selection information, for the case-based reasoner.","Navy Center for Applied Research in AI Naval Research Laboratory,","Code 5510 Washington, DC 20375",3
226,"THE LOAD, CAPACITY AND AVAILABILITY OF QUORUM SYSTEMS",MONI NAOR AND AVISHAI WOOL,"Abstract. A quorum system is a collection of sets (quorums) every two of which intersect. Quorum systems have been used for many applications in the area of distributed systems, including mutual exclusion, data replication and dissemination of information Given a strategy to pick quorums, the load L(S) is the minimal access probability of the busiest element, minimizing over the strategies. The capacity Cap(S) is the highest quorum accesses rate that S can handle, so Cap(S) = 1=L(S). The availability of a quorum system S is the probability that at least one quorum survives, assuming that each element fails independently with probability p. A tradeoff between L(S) and the availability of S is shown. We present four novel constructions of quorum system, all featuring optimal or near optimal load, and high availability. The best construction, based on paths in a grid, has a load of O(1= p and a failure probability of exp(( p n)) when the elements fail with probability p &lt; 1 2 . Moreover, even in the presence of faults, with exponentially high probability the load of this system is still O(1= n). The analysis of this scheme is based on Percolation Theory.",,,6
227,A SPEECH-BASED ROUTE ENQUIRY SYSTEM BUILT FROM GENERAL-PURPOSE COMPONENTS 1,"Ian Lewin , Martin Russell , David Carter , Sue Browning , Keith Ponting and Stephen Pulman","ABSTRACT The adaptation of existing general-purpose speech recognition and language understanding systems can greatly reduce the cost of developing applications. However, the components must have appropriate characteristics for this to be possible. Work is in progress to adapt two task-independent components, the AURIX speech recognizer and the CLARE language processor to create a system allowing spoken queries of the PC-based Autoroute route planning package.","SRI International,","23 Millers Yard, Cambridge, CB2 1RQ, UK",3
228,Needles in a Haystack : Plan Recognition in Large Spatial Domains Involving Multiple Agents,Mark Devaney and Ashwin Ram,"Abstract While plan recognition research has been applied to a wide variety of problems, it has largely made identical assumptions about the number of agents participating in the plan, the observability of the plan execution process, and the scale of the domain. We describe a method for plan recognition in a real-world domain involving large numbers of agents performing spatial maneuvers in concert under conditions of limited observability. These assumptions differ radically from those traditionally made in plan recognition and produce a problem which combines aspects of the fields of plan recognition, pattern recognition, and object tracking. We describe our initial solution which borrows and builds upon research from each of these areas, employing a pattern-directed approach to recognize individual movements and generalizing these to produce inferences of large-scale behavior.",College of Computing Georgia Institute of Technology,"Atlanta, GA 30332-0280",2
229,The State of the Art in Ontology Design: A Survey and Comparative Review,Natalya Fridman Noy Carole D. Hafner,"Abstract In this paper we develop a framework for comparing ontologies, and place a number of the more prominent ontologies into it. We have selected 10 specific projects for this study, including general ontologies, domain specific ones, and one knowledge representation system. The comparison framework includes general characteristics such as the purpose of an ontology, its coverage (general or domain-specific), its size, and the formalism used. It also includes the design process used in creating an ontology and the methods used to evaluate it. Characteristics that describe the content of an ontology include taxonomic organization, types of concepts covered, top-level divisions, internal structure of concepts, representation of part-whole relations, and the presence and nature of additional axioms. Finally we consider what experiments or applications have used the ontologies. Knowledge sharing and reuse will require a common framework to support interoperability of independently created ontologies. Our study shows there is great diversity in the way ontologies are designed and the way they represent the world. By identifying the similarities and differences among existing ontologies, we clarify the range of alternatives in creating a standard framework for ontology design.",College of Computer Science Northeastern University,"Boston, MA 02115",2
230,RESONANCE AND THE PERCEPTION OF MUSICAL METER,Edward W. Large John F. Kolen,"Abstract Many connectionist approaches to musical expectancy and music composition let the question of What next? overshadow the equally important question of When next?. One cannot escape the latter question, one of temporal structure, when considering the perception of musical meter. We view the perception of metrical structure as a dynamic process where the temporal organization of external musical events synchronizes, or entrains, a listeners internal processing mechanisms. This article introduces a novel connectionist unit, based upon a mathematical model of entrainment, capable of phase and frequency-locking to periodic components of incoming rhythmic patterns. Networks of these units can self-organize temporally structured responses to rhythmic patterns. The resulting network behavior embodies the perception of metrical structure. The article concludes with a discussion of the implications of our approach for theories of metrical structure and musical expectancy.",The Ohio State University,,3
231,Refining Interactions in a Distributed System,Neelam Soundarajan,,Computer and Information Science The Ohio State University,"2015 Neil Avenue Columbus, OH 43210 USA",3
232,AUCTION-DRIVEN COORDINATION FOR PLANTWIDE OPTIMIZATION,Rinaldo A. Jose and Lyle H. Ungar,"Abstract Model predictive control strategies generally focus on controlling plant outputs to setpoints; in industry, however, a more desirable goal is maximizing a plants profitability. In principle, this can be done by creating a plant model and maximizing profit with respect to the market prices of the plants inputs and outputs, but in practice, such centralized approaches often cannot effectively be applied at the operations time scale due to the size and complexity of the problem. One solution is to use decentralized optimization at the unit operations level by tearing process streams and coordinating the resulting pieces. Such optimization, however, requires that unit inputs and outputs be priced. We show that a traditional Lagrangean-based approach to this pricing fails for simple systems. Instead, we define slack resources over the torn process streams and price them using auctions. Unlike Lagrange multipliers, slack resource prices contain useful information and can be used to make decisions regarding capital improvements, thus providing a strong tie between the operations and management layers in chemical plants.",University of Pennsylvania,"Philadelphia, PA 19104",3
233,Constructing Cyberspace: Virtual Reality and Hypermedia,Keith Andrews,"Abstract Large-scale, distributed hypermedia information systems allow fast, structured access to very large, dynamic information bases. The highly perceptual nature of a virtual reality interface has the power to take users both inside information and inside its structure. Combining the two takes us a step towards cyberspace, William Gibson's vision of a virtual model of all the world's interconnected data. This paper reviews current work on the boundary of virtual reality and hypermedia.","Institute for Information Processing and Computer Supported New Media (IICM) Graz University of Technology,","A-8010 Graz, Austria.",3
234,Type classes in Haskell,"Cordelia Hall, Kevin Hammond, Simon Peyton Jones and Philip Wadler","Abstract This paper defines a set of type inference rules for resolving overloading introduced by type classes. Programs including type classes are transformed into ones which may be typed by the Hindley-Milner inference rules. In contrast to other work on type classes, the rules presented here relate directly to user programs. An innovative aspect of this work is the use of second-order lambda calculus to record type information in the program.",Glasgow University,,5
235,"Crooked Functions, Bent Functions, and Distance Regular Graphs",T.D. Bending D. Fon-Der-Flaass,"Abstract Let V and W be n-dimensional vector spaces over GF (2). A mapping Q : V ! W is called crooked if it satisfies the following three properties: Q(0) = 0; Q(x) + Q() + Q() + Q(x + + ) 6= 0 for any three distinct x; ; ; Q(x) + Q() + Q() + Q(x + a) + Q( + a) + Q( + a) 6= 0 if a 6= 0 (x; ;  arbitrary). We show that every crooked function gives rise to a distance regular graph of diameter 3 having = 0 and = 2 which is a cover of the complete graph. Our approach is a generalization of a recent construction found by de Caen, Mathon, and Moorhouse. We study graph-theoretical properties of the resulting graphs, including their automorphisms. Also we demonstrate a connection between crooked functions and bent functions.","School of Mathematical Sciences, Queen Mary and Westfield College,","London E1 4NS, U.K.",1
236,On the Maximum Tolerable Noise for Reliable Computation by Formulas,William Evans*,"Abstract: It is shown that if a formula is constructed from noisy 2-input NAND gates, with each gate failing independently with probability "", then reliable computation can or cannot take place according as "" is less than or greater than "" 0 = (3  p",Department of Computer Science The University of Arizona,"Tucson, AZ 85721-0077, USA",3
237,Virtual Radios,"Vanu Bose, Mike Ismert, Matt Welborn, John Guttag","Abstract Conventional software radios take advantage of vastly improved A/D converters and DSP hardware. Our approach, which we refer to as virtual radios, also depends upon high performance A/D converters. However, rather than use DSPs, we have chosen to ride the curve of rapidly improving workstation hardware. We use wideband digitization and then perform all of the digital signal processing in user space on a general purpose workstation. This approach allows us to experiment with new approaches to signal processing that exploit the hardware and software resources of the workstation. Furthermore, it allows us to experiment with different ways of structuring systems in which the radio component of communication devices are integrated with higher-level applications. This paper describes the design and performance of an environment we have constructed that facilitates building virtual radios and of two applications built using that environment. The environment consists of an I/O subsystem that provides high bandwidth low latency user-level access to digitized signals and a programming environment that provides an infrastructure for building applications. The applications, which exemplify some of the benefits of virtual radios, are a software cellular receiver and a novel wireless network interface.",Software Devices and Systems Group Laboratory for Computer Science Massachusetts Institute of Technology,,1
238,Myrinet - A Gigabit-per-Second Local-Area Network,"Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su","Abstract. Myrinet is a new type of local-area network (LAN) based on the technology used for packet communication and switching within ""massively-parallel processors"" (MPPs). Think of Myrinet as an MPP message-passing network that can span campus dimensions, rather than as a wide-area telecommunications network that is operating in close quarters. The technical steps toward making Myrinet a reality included the development of (1) robust, 25m communication channels with flow control, packet framing, and error control; (2) self-initializing, low-latency, cut-through switches; (3) host interfaces that can map the network, select routes, and translate from network addresses to routes, as well as handle packet traffic; and (4) streamlined host software that allows direct communication between user processes and the network.","Myricom, Inc.","325 N. Santa Anita Ave. Arcadia, CA 91006",3
239,Virtual Memory Architecture in SunOS,Robert A. Gingell Joseph P. Moran William A. Shannon,"ABSTRACT A new virtual memory architecture for the Sun implementation of the UNIX operating system is described. Our goals included unifying and simplifying the concepts the system used to manage memory, as well as providing an implementation that fit well with the rest of the system. We discuss an architecture suitable for environments that (potentially) consist of systems of heterogeneous hardware and software architectures. The result is a page-based system in which the fundamental notion is that of mapping process addresses to files.","Sun Microsystems, Inc.","2550 Garcia Ave. Mountain View, CA 94043",5
240,A Survey of Collective Communication in Wormhole-Routed Massively Parallel Computers,"Philip K. McKinley, Yih-jia Tsai, and David F. Robinson",,,,0
241,Feature Correspondence by Interleaving Shape and Texture Computations,David Beymer,"Abstract The correspondence problem in computer vision is basically a matching task between two or more sets of features. In this paper, we introduce a vectorized image representation, which is a feature-based representation where correspondence has been established with respect to a reference image. The representation consists of two image measurements made at the feature points: shape and texture. Feature geometry, or shape, is represented using the (x; ) locations of features relative to the some standard reference shape. Image grey levels, or texture, are represented by mapping image grey levels onto the standard reference shape. Computing this representation is essentially a correspondence task, and in this paper we explore an automatic technique for ""vectorizing"" face images. Our face vectorizer alternates back and forth between computation steps for shape and texture, and a key idea is to structure the two computations so that each one uses the output of the other. In addition to describing the vectorizer, an application to the problem of facial feature detection will be presented.","Artificial Intelligence Laboratory, and Center for Biological and Computational Learning Massachusetts Institute of Technology","Cambridge, MA 02139, USA",4
242,Narrowing Interval Bounds,Joseph D. Darcy,"1. Abstract Interval arithmetic is an automated attempt to give guaranteed upper and lower bounds of a numerical computation in the face of uncertainly in the input data and floating point roundoff during the calculation. While a simple interval equivalent of a rational function can be readily synthesized, the bounds from this construction may be too pessimistically large to be useful. This paper surveys a variety of techniques for refining the interval bounds. An appendix identifies issues with realizing floating point based interval arithmetic on current IEEE 754 compliant processors.",,,5
243,A New O(n 2 ) Algorithm for the Symmetric Tridiagonal Eigenvalue/Eigenvector Problem,by Inderjit Singh Dhillon,,"UNIVERSITY of CALIFORNIA, BERKELEY",,0
244,Using Skeletons for Nonholonomic Path Planning among Obstacles,Brian Mirtich  John Canny,"Abstract This paper describes a practical path planner for nonholonomic robots in environments with obstacles. The planner is based on building a one-dimensional, maximal clearance skeleton through the configuration space of the robot. However rather than using the Eu-clidean metric to determine clearance, a special metric which captures information about the nonholonomy of the robot is used. The robot navigates from start to goal states by loosely following the skeleton; the resulting paths taken by the robot are of low ""complexity."" We describe how much of the computation can be done off-line once and for all for a given robot, making for an efficient planner. The focus is on path planning for mobile robots, particularly the planar two-axle car, but the underlying ideas are quite general and may be applied to planners for other nonholonomic robots.",Computer Science Division University of California,"Berkeley, CA 94720",4
245,Belief Revision: A Critique,Nir Friedman,"Abstract The problem of belief changehow an agent should revise her beliefs upon learning new informationhas been an active area of research in both philosophy and artificial intelligence. Many approaches to belief change have been proposed in the literature. Our goal is not to introduce yet another approach, but to examine carefully the rationale underlying the approaches already taken in the literature, and to highlight what we view as methodological problems in the literature. The main message is that to study belief change carefully, we must be quite explicit about the ontology or scenario underlying the belief change process. This is something that has been missing in previous work, with its focus on postulates. Our analysis shows that we must pay particular attention to two issues which have often been taken for granted: The first is how we model the agent's epistemic state. (Do we use a set of beliefs, or a richer structure, such as an ordering on worlds? And if we use a set of beliefs, in what language are these beliefs are expressed?) The second is the status of observations. (Are observations known to be true, or just believed? In the latter case, how firm is the belief?) For example, we argue that even postulates that have been called beyond controversy are unreasonable when the agent's beliefs include beliefs about her own epistemic state as well as the external world. Issues of the status of observations arise particularly when we consider iterated belief revision, and we must confront the possibility of revising by ' and then by :'.",Computer Science Department Stanford University,"Gates Building 1A Stanford, CA 94305-9010",2
246,A Fast Algorithm for Incremental Distance Calculation,Ming C. Lin and John F. Canny,"Abstract A simple and efficient algorithm for finding the closest points between two convex polyhedra is described here. Data from numerous experiments tested on a broad set of convex polyhedra on &lt; 3 show that the running time is roughly constant for finding closest points when nearest points are approximately known and is linear in total number of vertices if no special initialization is done. This algorithm can be used for collision detection, computation of the distance between two polyhedra in three-dimensional space, and other robotics problems. It forms the heart of the motion planning algorithm of [1].","University of California, Berkeley","Berkeley, CA 94720",4
247,High-Performance Sorting on Networks of Workstations,Andrea C. Arpaci-Dusseau,"Abstract We report the performance of NOW-Sort, a collection of sorting implementations on a Network of Workstations (NOW). We find that parallel sorting on a NOW is competitive to sorting on the large-scale SMPs that have traditionally held the performance records. On a 64-node cluster, we sort 6.0 GB in just under one minute, while a 32-node cluster finishes the Datamation benchmark in 2.41 seconds. Our implementations can be applied to a variety of disk, memory, and processor configurations; we highlight salient issues for tuning each component of the system. We evaluate the use of commodity operating systems and hardware for parallel sorting. We find existing OS primitives for memory management and file access adequate. Due to aggregate communication and disk bandwidth requirements, the bottleneck of our system is the workstation I/O bus.","Computer Science Division University of California, Berkeley",,2
248,The Interaction of Parallel and Sequential Workloads on a Network of Workstations,"Remzi H. Arpaci, Andrea C. Dusseau, Amin M. Vahdat, Lok T. Liu, Thomas E. Anderson, and David A. Patterson","Abstract This paper examines the plausibility of using a network of workstations (NOW) for a mixture of parallel and sequential jobs. Through simulations, our study examines issues that arise when combining these two workloads on a single platform. Starting from a dedicated NOW just for parallel programs, we incrementally relax uniprogramming restrictions until we have a multi-programmed, multi-user NOW for both interactive sequential users and parallel programs. We show that a number of issues associated with the distributed NOW environment (e.g., daemon activity, coscheduling skew) can have a small but noticeable effect on parallel program performance. We also find that efficient migration to idle workstations is necessary to maintain acceptable parallel application performance. Furthermore, we present a methodology for deriving an optimal delay time for recruiting idle machines for use by parallel programs; this recruitment threshold was just 3 minutes for the research cluster we measured. Finally, we quantify the effects of the additional parallel load upon interactive users by keeping track of the potential number of user delays in our simulations. When we limit the maximum number of delays per user, we can still maintain acceptable parallel program performance. In summary, we find that for our workloads a 2:1 rule applies: a NOW cluster of approximately 60 machines can sustain a 32-node parallel workload in addition to the sequential load placed upon it by interactive users.","Computer Science Division University of California, Berkeley","Berkeley, CA 94720",4
249,"""Go With the Winners"" Algorithms",David Aldous,,Department of Statistics University of California,Berkeley CA 94720,4
250,Stochastic Interaction and Linear Logic,Patrick D. Lincoln John C. Mitchell Andre Scedrov x,"Abstract We present stochastic interactive semantics for propositional linear logic without modalities. The framework is based on interactive protocols considered in computational complexity theory, in which a prover with unlimited power interacts with a verifier that can only toss fair coins or perform simple tasks when presented with the given formula or with subsequent messages from the prover. The additive conjunction & is described as random choice, which reflects the intuitive idea that the verifier can perform only ""random spot checks"". This stochastic interactive semantic framework is shown to be sound and complete. Furthermore, the prover's winning strategies are basically proofs of the given formula. In this framework the multiplicative and additive connectives of linear logic are described by means of probabilistic operators, giving a new basis for intuitive reasoning about linear logic and a potential new tool in automated deduction.","SRI International Computer Science Laboratory,",Menlo Park CA 94025 USA.,5
251,Form(ers) Over Function(s): The KOLA Reference Manual,Mitch Cherniack,"0.1 Still To Do * 6 more fold proofs, precondition proofs * Adjust primitives for Int, Str and Char to be synchronized with the Theta operators for these types * Add floats * Add section describing preconditions * Add section on semantic optimizations, nested query optimization * Fix awk script to generate Latex version of Larch scripts without typeface glitches",,,1
252,The AQUA Approach to Querying Lists and Trees in Object-Oriented Databases,Bharathi Subramanian Theodore W. Leung,"Abstract Relational database systems and most object-oriented database systems provide support for queries. Usually these queries represent retrievals over sets or multisets. Many new applications for databases, such as multimedia systems and digital libraries, need support for queries on complex bulk types such as lists and trees. In this paper we describe an object-oriented query algebra for lists and trees. The operators in the algebra preserve the ordering between the elements of a list or tree, even when the result list or tree contains an arbitrary set of nodes from the original tree. We also present predicate languages for lists and trees which allow order-sensitive queries because they use pattern matching to examine groups of list or tree nodes rather than individual nodes. The ability to decompose predicate patterns enables optimizations that make use of indices.",Brown University Brown University,,3
253,LOCALIZED TEMPORAL REASONING USING SUBGOALS AND ABSTRACT EVENTS,"Shieu-Hong Lin, Thomas Dean 1","We are concerned with temporal reasoning problems where there is uncertainty about the order in which events occur. The task of temporal reasoning is to derive an event sequence consistent with a given set of ordering constraints to achieve a goal. Previous research shows that the associated decision problems are hard even for very restricted cases. In this paper, we investigate locality in event ordering and causal dependencies. We present a localized temporal reasoning algorithm that uses subgoals and abstract events to exploit locality. The computational efficiency of our algorithm for a problem instance is quantified by the inherent locality in the instance. We theoretically demonstrate the substantial improvement in performance gained by exploiting locality. This work provides a solid evidence of the usefulness of localized reasoning in exploiting locality.","Department of Computer Science, Brown University,","Providence, RI 02912",5
254,How good are genetic algorithms at finding large cliques: an experimental study,Bob Carter,"Abstract This paper investigates the power of genetic algorithms at solving the MAX-CLIQUE problem. We measure the performance of a standard genetic algorithm on an elementary set of problem instances consisting of embedded cliques in random graphs. We indicate the need for improvement, and introduce a new genetic algorithm, the multi-phase annealed GA, which exhibits superior performance on the same problem set. As we scale up the problem size and test on ""hard"" benchmark instances, we notice a degraded performance in the algorithm caused by premature convergence to local minima. To alleviate this problem, a sequence of modifications are implemented ranging from changes in input representation to systematic local search. The most recent version, called union GA, incorporates the features of union cross-over, greedy replacement, and diversity enhancement. It shows a marked speed-up in the number of iterations required to find a given solution, as well as some improvement in the clique size found. We discuss issues related to the SIMD implementation of the genetic algorithms on a Thinking Machines CM-5, which was necessitated by the intrinsically high time complexity (O(n 3 )) of the serial algorithm for computing one iteration. Our preliminary conclusions are: (1) a genetic algorithm needs to be heavily customized to work ""well"" for the clique problem; (2) a GA is computationally very expensive, and its use is only recommended if it is known to find larger cliques than other algorithms; (3) although our customization effort is bringing forth continued improvements, there is no clear evidence, at this time, that a GA will have better success in circumventing local minima.",Boston University Computer Science Department,"Boston, MA 02215",3
255,Implementation and Performance Evaluation of TCP Boston A Fragmentation-tolerant TCP Protocol for ATM Networks,Azer Bestavros,"ABSTRACT: In this paper, we overview the implementation of TCP Boston a novel fragmentation-tolerant transport protocol, especially suited for ATM's 53-byte cell-oriented switching architecture. TCP Boston integrates a standard TCP/IP protocol, such as Reno or Vegas, with a powerful redundancy control mechanism based on AIDAan adaptive version of Rabin's IDA dispersal and reconstruction algorithms. Our results show that TCP Boston improves TCP/IP's performance over ATMs for both network-centric metrics (e.g., effective throughput) and application-centric metrics (e.g., response time).",Computer Science Department Boston University,"Boston, MA 02215",4
256,Generating CAD-models of teeth,Peter Johannes Neugebauer,"In this paper, we present an approach for the reconstruction of teeth model. Therefore, a tooth model has to be scanned from several directions with a 3D-laser scanner. Several views are necessary because of shadows and occluded areas in the range images. Then, all acquired range views are combined to build a CAD-model of the tooth. The idea is that every part of the surface should be visible in at least one view. The reconstruction process is divided into the steps registration, volume sculpturing and generation of an accurate polygonal representation.","Fraunhofer-Institute for Computer Graphics,","Wilhelminenstrasse 7, 64283 Darmstadt, Germany,",6
257,Bias-Driven Revision of Logical Domain Theories,Moshe Koppel,"Abstract The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the ``ow'' of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair awed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.","Department of Mathematics and Computer Science, Bar-Ilan University,","Ramat-Gan, Israel",0
258,A Middleware Service for Real-Time Push-Pull Communications,Kanaka Juvva and Raj Rajkumar,"Abstract: Current and emerging real-time and multimedia applications like multi-party collaboration, internet telephony and distributed command control systems require the exchange of information over distributed and heterogeneous nodes. Multiple data types including voice, video, sensor data, real-time intelligence data and text are being transported widely across today's information, control and surveillance networks. All such applications can benefit enormously from middleware, operating system and networking services that can support QoS guarantees, high availability, dynamic reconfigurability and scalability. In this paper, we propose a middleware layer called a ""Real-Time Push-Pull Communications Service"" to easily and quickly disseminate information across heterogeneous nodes with an underlying architecture to satisfy the above-mentioned requirements. Push-Pull Communications is an extension of the real-time publisher/subscriber model [4], and represents both ""push"" (data transfer initiated by a sender) and ""pull"" (data transfer initiated by a receiver) communications. Nodes with widely differing processing power and networking bandwidth can coordinate and co-exist by the provision of appropriate and automatic support for transformation on data and supports scaling. Different information sources and sinks can operate at different frequencies and also can choose another (intermedi-ate) node to act as their proxy and and deliver data at the desired frequency. This service has been implemented on RT-Mach, a resource-centric kernel using resource kernel primitives [7]. This paper presents an overview of the design, implementation and preliminary performance evaluation of the model.",Real-Time and Multimedia Laboratory Carnegie Mellon University,Pittsburgh PA 15213,0
259,Building and Refining Abstract Planning Cases by Change of Representation Language,Ralph Bergmann,"Abstract Abstraction is one of the most promising approaches to improve the performance of problem solvers. In several domains abstraction by dropping sentences of a domain description as used in most hierarchical planners has proven useful. In this paper we present examples which illustrate significant drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we propose a more general view of abstraction involving the change of representation language. We have developed a new abstraction methodology and a related sound and complete learning algorithm that allows the complete change of representation language of planning cases from concrete to abstract. However, to achieve a powerful change of the representation language, the abstract language itself as well as rules which describe admissible ways of abstracting states must be provided in the domain model. This new abstraction approach is the core of Paris (Plan Abstraction and Refinement in an Integrated System), a system in which abstract planning cases are automatically learned from given concrete cases. An empirical study in the domain of process planning in mechanical engineering shows significant advantages of the proposed reasoning from abstract cases over classical hierarchical planning.","Centre for Learning Systems and Applications (LSA) University of Kaiserslautern,","P.O.-Box 3049, D-67653 Kaiserslautern, Germany",4
260,Automation Tools for NonDestructive Inspection of Aircraft: Promise of Technology Transfer from the Civilian to the Military Sector,"Chris Seher 1 , Mel Siegel 2 , and William M. Kaufman 3","Abstract The FAA Aging Aircraft Research Program is supporting the development of a robotic mobile nondestructive inspection (NDI) instrument deployment tool at Carnegie Mellon University (CMU) with the active participation of USAir. The program has spawned several new relationships and entities: an alliance with an ARPA-funded research program at CMU having the capability to add 3D-stereoscopic enhanced visual inspection capability, a start-up company organized to commercialize the combined technologies, and State of Pennsylvania funding to foster this commercialization. As a result of these activities and connections the civilian sector appears to be ahead of the military sector in important aspects of automation for deployment of aircraft inspection equipment. A partnership between the university researchers, the airline operator, the start-up company, and the state government is thus emerging as the likely agent for transfer of the civilian-developed technology to the military sector.","1 Federal Aviation Administration, Technical Center,",Atlantic City NJ 08201,5
261,The Candide System for Machine Translation,"Adam L. Berger, Peter F. Brown , Stephen A. Della Pietra, Vincent J. Della Pietra, John R. Gillett, John D. Lafferty, Robert L. Mercer, Harry Printz, Lubos Ures","ABSTRACT We present an overview of Candide, a system for automatic translation of French text to English text. Candide uses methods of information theory and statistics to develop a probability model of the translation process. This model, which is made to accord as closely as possible with a large body of French and English sentence pairs, is then used to generate English translations of previously unseen French sentences. This paper provides a tutorial in these methods, discussions of the training and operation of the system, and a summary of test results.",IBM Thomas J. Watson Research Center,"P.O. Box 704 Yorktown Heights, NY 10598",2
262,A Simplified Account of Polymorphic References,Robert Harper,"Abstract A proof of the soundness of Tofte's imperative type discipline with respect to a structured operational semantics is given. The presentation is based on a semantic formalism that combines the benefits of the approaches considered by Wright and Felleisen, and by Tofte, leading to a particularly simple proof of soundness of Tofte's type discipline.",School of Computer Science Carnegie Mellon University,"Pittsburgh, PA 15213",3
263,New Approximation Techniques for Some Ordering Problems,Satish Rao Andrea W. Richa,"Abstract We describe logarithmic times optimal approximation algorithms for the NP-hard graph optimization problems of minimum linear arrangement, minimum containing interval graph, and minimum storage-time product. This improves on the best previous approximation bounds of Even, Naor, Rao, and Schieber for these problems by an (log log n) factor. Even, Naor, Rao, and Schieber defined ""spreading metrics"" for each of the ordering problems above (and to other problems); for each of these problems, they provided a spreading metric of volume W , such that W is a lower bound on the cost of a solution to the problem. They used this spreading metric to find a solution of cost O(W log n log log n) (for simplicity, assume that all tasks have unit processing time in the minimum storage-time product problem). In this paper, we show how to find a solution within a logarithmic factor times W for these problems. We develop a recursion where at each level we identify cost which, if incurred, yields subproblems with reduced spreading metric volume. Specifically, we present a divide-and-conquer strategy where the cost of a solution to a problem at a recursive level is C plus the cost of a solution to the subproblems at this level, and where the spreading metric volume on the subproblems is less than the original volume by (C= log n). This ensures that the resulting solution has cost O(log n) times the original spreading metric volume. We note that this is an existentially tight bound on the relationship between the spreading metric volume W and the true optimal values for these problems. For planar graphs, we combine a structural theorem of Klein, Plotkin, and Rao with our new recursion technique to show that the spreading metric cost volumes are within an O(log log n) factor of the cost of an optimal solution for the minimum linear arrangement, and the minimum containing interval graph problems.",,,6
264,Implementing Distributed Server Groups for the World Wide Web,"Michael Garland, Sebastian Grassia, Robert Monroe, Siddhartha Puri","Abstract The World Wide Web (WWW) has recently become a very popular facility for the dissemination of information. As a result of this popularity, it is experiencing rapidly increasing traffic load. Single machine servers cannot keep pace with the ever greater load being placed upon them. To alleviate this problem, we have implemented a distributed Web server group. The server group can effectively balance request load amongst its members (within about 10% of optimal), and client response time is no worse than in the single server case. Client response time was not improved because the measured client traffic consumed all available network throughput. The distributed operation of the server groups is completely transparent to standard Web clients.",School of Computer Science Carnegie Mellon University,"Pittsburgh, Pennsylvania 15213-3890",2
265,Performance Measurements of the Multimedia Testbed on Real-Time Mach,"Roger B. Dannenberg, David B. Anderson, Tom Neuendorffer, Dean Rubine","Abstract Multimedia has generated widespread interest in real-time support within general purpose operating systems. Multimedia also places new demands on operating systems for interprocess communication. The Multimedia Testbed is a set of applications that stress consistent low-latency response and efficient interprocess communication for large blocks of data. The Multimedia Testbed was ported to Real-Time Mach in the hopes of providing predictable low-latency response and, consequently, good synchronization and low jitter as required for multimedia applications. Our work compares the performance of Real-Time Mach with that of Mach 3.0. Although the fixed-priority scheduling of Real-Time Mach is a substantial improvement, user threads are still preempted by device drivers, and the overall real-time performance is not suitable for multimedia applications. We discuss areas where Real-Time Mach needs improvement.",School of Computer Science Carnegie Mellon University,"Pittsburgh, PA 15213-3890",5
266,Moving target classification and tracking from real-time video,Alan J. Lipton Hironobu Fujiyoshi Raju S. Patil,"Abstract This paper describes an end-to-end method for extracting moving targets from a real-time video stream, classifying them into predefined categories according to image-based properties, and then robustly tracking them. Moving targets are detected using the pixelwise difference between consecutive image frames. A classi-ficatoin metric is applied these targets with a temporal consistency constraint to classify them into three categories: human, vehicle or background clutter. Once classified, targets are tracked by a combination of temporal differencing and template matching. The resulting system robustly identifies targets of interest, rejects background clutter, and continually tracks over large distances and periods of time despite occlusions, appearance changes and cessation of target motion.",The Robotics Institute. Carnegie Mellon University,"5000 Forbes Avenue, Pittsburgh, PA, 15213",5
267,TIGHT ANALYSES OF TWO LOCAL LOAD BALANCING ALGORITHMS,BHASKAR GHOSH 1 F. T. LEIGHTON 2 BRUCE M. MAGGS 3;4 S. MUTHUKRISHNAN 5 C. GREG PLAXTON 6;7 R. RAJARAMAN 6;7 ANDR EA W. RICHA 3;4 ROBERT E. TARJAN 8 DAVID ZUCKERMAN 6;9,"Abstract. This paper presents an analysis of the following load balancing algorithm. At each step, each node in a network examines the number of tokens at each of its neighbors and sends a token to each neighbor with at least 2d + 1 fewer tokens, where d is the maximum degree of any node in the network. We show that within O(=ff) steps, the algorithm reduces the maximum difference in tokens between any two nodes to at most O((d 2 log n)=ff), where is the global imbalance in tokens (i.e., the maximum difference between the number of tokens at any node initially and the average number of tokens), n is the number of nodes in the network, and ff is the edge expansion of the network. The time bound is tight in the sense that for any graph with edge expansion ff, and for any value , there exists an initial distribution of tokens with imbalance for which the time to reduce the imbalance to even =2 is at least (=ff). The bound on the final imbalance is tight in the sense that there exists a class of networks that can be locally balanced everywhere (i.e., the maximum difference in tokens between any two neighbors is at most 2d), while the global imbalance remains ((d 2 log n)=ff). Furthermore, we show that upon reaching a state with a global imbalance of O((d 2 log n)=ff), the time for this algorithm to locally balance the network can be as large as (n 1=2 ). We extend our analysis to a variant of this algorithm for dynamic and asynchronous networks. We also present tight bounds for a randomized algorithm in which each node sends at most one token in each step.",,,5
268,A Note on Learning from Multiple-Instance  Examples,AVRIM BLUM,"Abstract. We describe a simple reduction from the problem of PAC-learning from multiple-instance examples to that of PAC-learning with one-sided random classification noise. Thus, all concept classes learnable with one-sided noise, which includes all concepts learnable in the usual 2-sided random noise model plus others such as the parity function, are learnable from multiple-instance examples. We also describe a more efficient (and somewhat technically more involved) reduction to the Statistical-Query model that results in a polynomial-time algorithm for learning axis-parallel rectangles with sample complexity ~ O(d 2 r=* 2 ), saving roughly a factor of r over the results of Auer et al. (1997).","School of Computer Science, Carnegie Mellon University,","Pittsburgh, PA 15213",0
269,Interactive Physically-Based Manipulation of Discrete/Continuous Models,Mikako Harada,"Abstract Physically-based modeling has been used in the past to support a variety of interactive modeling tasks including free-form surface design, mechanism design, constrained drawing, and interactive camera control. In these systems, the user interacts with the model by exerting virtual forces, to which the system responds subject to the active constraints. In the past, this kind of interaction has been applicable only to models that are governed by continuous parameters. In this paper we present an extension to mixed con-tinuous/discrete models, emphasizing constrained layout problems that arise in architecture and other domains. When the object being dragged is blocked from further motion by geometric constraints, a local discrete search is triggered, during which transformations such as swapping of adjacent objects may be performed. The result of the search is a nearby state in which the target object has been moved in the indicated direction and in which all constraints are satisfied. The transition to this state is portrayed using simple but effective animated visual effects. Following the transition, continuous dragging is resumed. The resulting seamless transitions between discrete and continuous manipulation allow the user to easily explore the mixed design space just by dragging objects. We demonstrate the method in application to architectural floor plan design, circuit board layout, art analysis, and page layout.",Department of Architecture,"Pittsburgh, PA 15213",2
270,Mixed-Initiative Management of Integrated Process-Planning and Production-Scheduling Solutions,"David W. Hildum, Norman M. Sadeh, Thomas J. Laliberty,  Stephen F. Smith, John McA'Nulty and Dag Kjenstad","Abstract Increased reliance on agile manufacturing techniques has created a demand for systems to solve integrated process-planning and production-scheduling problems in large-scale dynamic environments. To be effective, these systems should provide user-oriented interactive functionality for managing the various user tasks and objectives and reacting to unexpected events. This paper describes the mixed-initiative problem-solving features of IP3S, an Integrated Process-Planning/Production-Scheduling shell for agile manufacturing. IP3S is a blackboard -based system that supports the concurrent development and dynamic revision of integrated process-planning and production-scheduling solutions and the maintenance of multiple problem instances and solutions, as well as other flexible user-oriented decision-making capabilities, allowing the user to control the scope of the problem and explore alternate tradeoffs (what-if scenarios) interactively. The system is scheduled for initial deployment and evaluation in a large and highly dynamic machine shop at Raytheon's Andover manufacturing facility.",Intelligent Coordination and Logistics Laboratory Center for Integrated Manufacturing Decision Systems The Robotics Institute Carnegie Mellon University,Pittsburgh PA 15213-3890,3
271,Operant Conditioning in Skinnerbots,David S. Touretzky,"Abstract Instrumental (or operant) conditioning, a form of animal learning, is similar to reinforcement learning (Watkins, 1989) in that it allows an agent to adapt its actions to gain maximally from the environment while only being rewarded for correct performance. But animals learn much more complicated behaviors through instrumental conditioning than robots presently acquire through reinforcement learning. We describe a new computational model of the conditioning process that attempts to capture some of the aspects that are missing from simple reinforcement learning: conditioned reinforcers, shifting reinforcement contingencies, explicit action sequencing, and state space refinement. We apply our model to a task commonly used to study working memory in rats and monkeys: the DMTS (Delayed Match to Sample) task. Animals learn this task in stages. In simulation, our model also acquires the task in stages, in a similar manner. We have used the model to train an RWI B21 robot.",Computer Science Department & Center for the Neural Basis of Cognition Carnegie Mellon University,"Pittsburgh, PA 15213-3891",0
272,Improving Text Classification by Shrinkage in a Hierarchy of Classes,Andrew McCallum,"Abstract When documents are organized in a large number of topic categories, the categories are often arranged in a hierarchy. The U.S. patent database and Yahoo are two examples. This paper shows that the accuracy of a naive Bayes text classifier can be significantly improved by taking advantage of a hierarchy of classes. We adopt an established statistical technique called shrinkage that smoothes parameter estimates of a data-sparse child with its parent in order to obtain more robust parameter estimates. The approach is also employed in deleted interpolation, a technique for smoothing n-grams in language modeling for speech recognition. Our method scales well to large data sets, with numerous categories in large hierarchies. Experimental results on three real-world data sets from UseNet, Yahoo, and corporate web pages show improved performance, with a reduction in error up to 29% over the tradi tional flat classifier.",Just Research,"4616 Henry Street Pittsburgh, PA 15213",4
273,Remote Access to Interactive Media,Roger B. Dannenberg,"ABSTRACT Digital interactive media augments interactive computing with video, audio, computer graphics and text, allowing multimedia presentations to be individually and dynamically tailored to the user. Multimedia, and particularly continuous media pose interesting problems for system designers, including those of latency and synchronization. These problems are especially evident when multimedia data is remote and must be accessed via networks. Latency and synchronization issues are discussed, and an integrated system, Tactus, is described. Tactus facilitates the implementation of interactive multimedia computer programs by managing latency and synchronization in the framework of an object-oriented graphical user interface toolkit.","Carnegie Mellon University, School of Computer Science","Pittsburgh, PA 15213 USA",0
274,Informing Memory Operations: Providing Memory Performance Feedback in Modern Processors,Mark Horowitz Margaret Martonosi Todd C. Mowry Michael D. Smith,"Abstract Memory latency is an important bottleneck in system performance that cannot be adequately solved by hardware alone. Several promising software techniques have been shown to address this problem successfully in specific situations. However, the generality of these software approaches has been limited because current architectures do not provide a fine-grained, low-overhead mechanism for observing and reacting to memory behavior directly. To fill this need, we propose a new class of memory operations called informing memory operations, which essentially consist of a memory operation combined (either implicitly or explicitly) with a conditional branch-and-link operation that is taken only if the reference suffers a cache miss. We describe two different implementations of informing memory operationsone based on a cache-outcome condition code and another based on low-overhead trapsand find that modern in-order-issue and out-of-order-issue superscalar processors already contain the bulk of the necessary hardware support. We describe how a number of software-based memory optimizations can exploit informing memory operations to enhance performance, and look at cache coherence with fine-grained access control as a case study. Our performance results demonstrate that the runtime overhead of invoking the informing mechanism on the Alpha 21164 and MIPS R10000 processors is generally small enough to provide considerable exibility to hardware and software designers, and that the cache coherence application has improved performance compared to other current solutions. We believe that the inclusion of informing memory operations in future processors may spur even more innovative performance optimizations.",Computer Systems Department of Department of Electrical Division of Laboratory Electrical Engineering and Computer Engineering Applied Sciences Stanford University Princeton University University of Toronto Harvard University,,3
275,A Simple Algorithm for Finding the Maximum Recoverable System State in Optimistic Rollback Recovery Methods,David B. Johnson Peter J. Keleher Willy Zwaenepoel,,Department of Computer Science Rice University,"P.O. Box 1892 Houston, Texas 77251-1892",0
276,Towards a Principled Representation of Discourse Plans,R. Michael Young Johanna D. Moore Martha E. Pollack,"Abstract We argue that discourse plans must capture the intended causal and decompositional relations between communicative actions. We present a planning algorithm, DPOCL, that builds plan structures that properly capture these relations, and show how these structures are used to solve the problems that plagued previous discourse planners, and allow a system to participate effectively and flexibly in an ongoing dialogue. A version of this paper appears in the Proceedings of the Sixteenth Annual Meeting of the Cognitive Science Society, Atlanta, GA, 1994.",,,4
277,"NIFDY: A Low Overhead, High Throughput Network Interface",Timothy Callahan and Seth Copen Goldstein,"Abstract In this paper we present NIFDY, a network interface that uses admission control to reduce congestion and ensures that packets are received by a processor in the order in which they were sent, even if the underlying network delivers the packets out of order. The basic idea behind NIFDY is that each processor is allowed to have at most one outstanding packet to any other processor unless the destination processor has granted the sender the right to send multiple unacknowledged packets. Further, there is a low upper limit on the number of outstanding packets to all processors. We present results from simulations of a variety of networks (meshes, tori, butterflies, and fat trees) and traffic patterns to verify NIFDY's efficacy. Our simulations show that NIFDY increases throughput and decreases overhead. The utility of NIFDY increases as a network's bisection bandwidth decreases. When combined with the increased payload allowed by in-order delivery NIFDY increases total bandwidth delivered for all networks. The resources needed to implement NIFDY are small and constant with respect to network size.",Computer Science Division University of California-Berkeley,,0
278,Exploiting Global Input/Output Access Pattern Classification,Tara M. Madhyastha Daniel A. Reed,,Department of Computer Science University of Illinois,"Urbana, Illinois 61801",3
279,SUPRENUM: Perspectives and Performance,Oliver A. McBryan,"Abstract We describe our impressions of the SUPRENUM project and of its primary supercomputer result, the Suprenum-1 prototype. We comment on the significance of the architecture, its role among contemporary systems and its relevance to current systems. We similarly discuss the SUPRENUM software and its impact on distributed systems. Finally we discuss the successes and failures observed throughout this exciting project and relate these to the organizational decisions on which SUPRENUM was based. As an illustration of Suprenum-1 capabilities, we describe the implementation of a fluid dynamical benchmark on the 256 node Suprenum-1 parallel computer. The benchmark, the Shallow Water Equations, is frequently used as a model for both oceanographic and atmospheric circulation. We describe the steps involved in implementing the algorithm on the Suprenum-1 and we provide details of performance obtained. For such regular grid-based algorithms the system delivers a very impressive fraction (25%) of its theoretical peak rate of 5 Gflops.",Dept. of Computer Science University of Colorado,"Boulder, CO 80309.",4
280,The Sybil Database Integration and Evolution Environment: An Overview,"Roger King, Michael Novak, Christian Och, Richard Osborne","Abstract Sybil is a database integration and evolution environment for supporting large, heterogeneous applications. We are interested in using Sybil to support the data integration and evolution needs of applications that are using legacy databases and are looking to integrate with more modern database systems. The Sybil approach is based on loosely coupling databases or other persistent tools into lightweight alliances tailored for specific applications. Such alliances are built via four sorts of constructs: heterogeneous views, inter-database constraints, inter-database propagations, and integration supported by domain specific information.",Department of Computer Science University of Colorado,"Campus Box 430 Boulder, CO 80309-0430",0
281,Goal-Directed Classification using Linear Machine Decision Trees,Bruce A. Draper Carla E. Brodley Paul E. Utgoff,"Abstract Recent work in feature-based classification has focused on non-parametric techniques that can classify instances even when the underlying feature distributions are unknown. The inference algorithms for training these techniques, however, are designed to maximize the accuracy of the classifier, with all errors weighted equally. In many applications, certain errors are far more costly than others, and the need arises for non-parametric classification techniques that can be trained to optimize task-specific cost functions. This paper reviews the Linear Machine Decision Tree (LMDT) algorithm for inducing multivariate decision trees, and shows how LMDT can be altered to induce decision trees that minimize arbitrary misclassification cost functions (MCFs). Demonstrations of pixel classification in outdoor scenes show how MCFs can optimize the performance of embedded classifiers within the context of larger image understand ing systems.",Department of Computer Science University of Massachusetts,"Amherst, MA., USA. 01003",6
282,Efficient Indexing for Object Recognition Using Large Networks,Mark R. Stevens Charles W. Anderson J. Ross Beveridge,"Abstract Template matching is an effective means of locating vehicles in outdoor scenes, but it tends to be a computationally expensive. To reduce processing time, we use large neural networks to predict, or index, a small subset of templates that are likely to match each window in an image. Results on actual LADAR range images show that limiting the templates to those selected by the neural networks reduces the computation time by a factor of 5 without sacrificing the accuracy of the results.",Department of Computer Science Colorado State University,"Fort Collins, CO 80523",3
283,Internet Telephony: A (Partial) Research Agenda,Jonathan Rosenberg,,Bell Laboratories and Columbia U.,"Rm. 4C-526 101 Crawfords Corner Rd. Holmdel, NJ 07733",5
284,Mining Audit Data to Build Intrusion Detection Models,Wenke Lee Salvatore J. Stolfo Kui W. Mok,"Abstract In this paper we discuss a data mining framework for constructing intrusion detection models. The key ideas are to mine system audit data for consistent and useful patterns of program and user behavior, and use the set of relevant system features presented in the patterns to compute (inductively learned) classifiers that can recognize anomalies and known intrusions. Our past experiments showed that classifiers can be used to detect intrusions, provided that sufficient audit data is available for training and the right set of system features are selected. We propose to use the association rules and frequent episodes computed from audit data as the basis for guiding the audit data gathering and feature selection processes. We modify these two basic algorithms to use axis attribute(s) as a form of item constraints to compute only the relevant (useful) patterns, and an iterative level-wise approximate mining procedure to uncover the low frequency (but important) patterns. We report our experiments in using these algorithms on real-world audit data.",Computer Science Department Columbia University,"500 West 120th Street, New York, NY 10027",4
285,Learning Distributions from Random Walks,Funda Ergun S Ravi Kumar,"Abstract We introduce a new model of distributions generated by random walks on graphs. This model suggests a variety of learning problems, using the definitions and models of distribution learning defined in [6]. Our framework is general enough to model previously studied distribution learning problems, as well as to suggest new applications. We describe special cases of the general problem, and investigate their relative difficulty. We present algorithms to solve the learning problem under various conditions.",Department of Computer Science Cornell University,"Ithaca, NY 14853.",4
286,Embedded Machine Learning Systems for Natural Language Processing: A General Framework,Claire Cardie,"Abstract. This paper presents Kenmore, a general framework for knowledge acquisition for natural language processing (NLP) systems. To ease the acquisition of knowledge in new domains, Kenmore exploits an online corpus using robust sentence analysis and embedded symbolic machine learning techniques while requiring only minimal human intervention. By treating all problems in ambiguity resolution as classification tasks, the framework uniformly addresses a range of subproblems in sentence analysis, each of which traditionally had required a separate computational mechanism. In a series of experiments, we demonstrate the successful use of Kenmore for learning solutions to several problems in lexical and structural ambiguity resolution. We argue that the learning and knowledge acquisition components should be embedded components of the NLP system in that (1) learning should take place within the larger natural language understanding system as it processes text, and (2) the learning components should be evaluated in the context of prac tical language-processing tasks.","Department of Computer Science, Cornell University,","Ithaca NY 14853, USA",6
287,On-Line Search in a Simple Polygon,Jon M. Kleinberg,"Abstract We consider a number of search and exploration problems, from the perspective of robot navigation in a simple polygon. These problems are ""on-line"" in the sense that the robot does not have access to the map of the polygon; it must make decisions as it proceeds, based only on what it has seen so far. For the problem of exploring a simple rectilinear polygon (under the L 1 norm), Deng, Kameda, and Papadimitriou give a 2-competitive deterministic algorithm; we present a randomized exploration algorithm which is 5=4-competitive. Using similar techniques, we are able to give an algorithm for searching an arbitrary, unknown rectilinear polygon. No constant competitive ratio is attainable in this case, but our algorithm is within a constant factor of optimal in the worst case; in a sense, it is a generalization of some of the strategies of Baeza-Yates, Culberson, and Rawlins to a much more general class of search spaces. Finally, we examine a type of polygon for which competitive search is possible | the class of ""streets"" considered by Klein, who gave a 1 + 3 2 -competitive algorithm for the search problem in this case. We present a simple algorithm with a competitive ratio of at most q p 8 (~ 2:61); in rectilinear streets it achieves the optimal competitive ratio of 2.",,,4
288,Experimental Study of Minimum Cut Algorithms,Chandra S. Chekuri,"Abstract Recently, several new algorithms have been developed for the minimum cut problem. These algorithms are very different from the earlier ones and from each other and substantially improve worst-case time bounds for the problem. We conduct experimental evaluation the relative performance of these algorithms. In the process, we develop heuristics and data structures that substantially improve practical performance of the algorithms. We also develop problem families for testing minimum cut algorithms. Our work leads to a better understanding of practical performance of the minimum cut algorithms and produces very efficient codes for the problem.",Computer Science Department Stanford University,"Stanford, CA 94305",1
289,Agent Tcl: Alpha Release 1.1,Robert S. Gray,"Abstract Agent Tcl is a transportable agent system. The agents are written in an extended version of the Tool Command Lanuage (Tcl). Each agent can suspend its execution at an arbitrary point, transport to another machine and resume execution on the new machine. This migration is accomplished with the agent jump command. agent jump captures the current state of the Tcl script and transfers this state to the destination machine. The state is restored on the new machine and the Tcl script continues its execution from the command immediately after the agent jump. In addition to migration, agents can send messages to each other and can establish direct connections. A direct connection is more efficient than message passing for bulk data transfer. Finally, agents can use the Tk toolkit to create graphical user interfaces on their current machine. Agent Tcl is implemented as two components. The first component is an extended Tcl interpreter. The second component is a server which runs on each machine. The server accepts incoming agents, messages and connection requests and keeps track of the agents that are running on its machine. An alpha release of Agent Tcl is available for public use. This documentation describes how to obtain and compile the source code, how to run the server and how to write transportable agents.",Department of Computer Science Dartmouth College,"Hanover, NH 03755",0
290,A Hyperconcentrator Switch for Routing Bit-Serial Messages,Thomas H. Cormen  Charles E. Leiserson,,Laboratory for Computer Science Massachusetts Institute of Technology,"Cambridge, Massachusetts 02139",2
291,Probabilistic Analysis for Combinatorial Functions of Moving Points,Li Zhang Harish Devarajan Julien Basch Piotr Indyk,"Abstract Given a set of n points, what is the description complexity of their convex hull? In our world, this question is understood with an implicit ""in the worst case"", and the answer is n bd=2c where d is the dimension of the underlying space. This is not entirely satisfactory, as this description complexity can vary tremendously depending on the positions of the points. Another approach is to look at the expected description complexity when the points are drawn from a given distribution. This type of analysis, initiated by Renyi and Sulanke [RS63] and pursued by others gets its value from the fact that this expectation is in general much smaller than in the worst case, and, more importantly, in that it often allows one to design algorithms that have expected running times against which worst case aware algorithms cannot compete. For instance, the convex hull of n points drawn independently uniformly at random from a d-dimensional hypercube has expected complexity O(log d1 n), and can be computed in expected linear time. In parallel, in the past decade, a number of papers have considered a setting where points are allowed to move along low degree algebraic trajectories. Different questions have been asked in this context. In particular, Atallah [Ata85], studied the number of times the combinatorial description of the convex hull or closest pair can change, in the worst case (""dynamic computational geometry""). More recently, Basch, Guibas, and Hershberger [BGH97] have designed kinetic data structures to maintain these attributes in an online setting, measuring the quality of a kinetic data structure by the ratio of the worst case number of changes to the configuration of interest, to the worst case number of changes to the data structure itself, for low degree algebraic motions. However, an experimental study undertaken in [BGSZ97] to assess the quality of these data structures in practice shows that the worst case analysis can hide vastly different results in terms of expectation when the point positions and speeds are drawn at random from some distributions. It is this study that motivated the present paper. In this communication, we report several results on the expected number of changes to various combinatorial structures for the case when points are drawn from the uniform distribution on the unit square. These results can be generalized for any dimension d &gt; 2.",Computer Science Department Stanford University,"Stanford, CA94305",2
292,Lexicographic Bit Allocation for MPEG Video,Dzung T. Hoang +L,,Sony Semiconductor of America,"3300 Zanker Road, MS SJ3C3 San Jose, CA 95134",4
293,A Unified Analysis of Value-Function-Based Reinforcement-Learning Algorithms,Csaba Szepesvari,"Abstract Reinforcement learning is the problem of generating optimal behavior in a sequential decision-making environment given the opportunity of interacting with it. Many algorithms for solving reinforcement-learning problems work by computing improved estimates of the optimal value function. We extend prior analyses of reinforcement-learning algorithms and present a powerful new theorem that can provide a unified analysis of value-function-based reinforcement-learning algorithms. The usefulness of the theorem lies in how it allows the asynchronous convergence of a complex reinforcement-learning algorithm to be proven by verifying that a simpler synchronous algorithm converges. We illustrate the application of the theorem by analyzing the convergence of Q-learning, model-based reinforcement learning, Q-learning with multi-state updates, Q-learning for Markov games, and risk-sensitive reinforcement learning.","Research Group on Artificial Intelligence ""Jozsef Attila"" University",,0
294,An Extensible End-to-End Protocol and Framework,K. L. Calvert R. H. Kravets R. D. Krupczak,Abstract We describe a framework for composing end-to-end protocol functions. The framework comprises: a generic model of protocol processing; a metaheader protocol supporting per-packet configuration of protocol function and efficient demultiplexing of incoming data units; and an extensible set of modular protocol functions. This paper describes the pieces of the framework and motivates some of the design decisions.,College of Computing Georgia Institute of Technology,"Atlanta, Georgia, USA",4
295,PARALLEL AND DISTRIBUTED SIMULATION,Richard M. Fujimoto,"ABSTRACT Research and development efforts in the parallel and distributed simulation field over the last 15 years has progressed, largely independently, in two separate camps: the largely academic high performance Parallel And Distributed (discrete event) Simulation (PADS) community, and the DoD-centered Distributed Interactive Simulation (DIS) community. This tutorial gives an overview and comparison of work in these two areas, emphasizing issues related to distributed execution where these fields have the most overlap. Differences in the fundamental assumptions routinely used within each community are contrasted, followed by overviews of work in each community.",College of Computing Georgia Institute of Technology,"Atlanta, Georgia 30332-0280, U.S.A.",1
296,Evaluating Blocking Probability in Generalized Connectors,Ellen Witte Zegura,Abstract| Generalized connectors provide the capability to connect a single input to one or more outputs. Such networks play an important role in supporting any application that involves the distribution of information from one source to many destinations or many sources to many destinations. We present the first analytic model for evaluating blocking probability in generalized connectors. The model allows flexibility in specifying traffic fanout characteristics and network routing algorithms. Equations are derived for computing blocking probability for the important class of series-parallel networks. We investigate the accuracy of the equations by comparing the blocking probability computed using the equations to results from simulation.,,,3
297,Discovery of frequent episodes in event sequences,"Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo",,University of Helsinki Finland,,2
298,A structured document database system,"Pekka Kilpelainen Greger Linden Heikki Mannila 
Erja Nikunen","Abstract We describe a database system for writing, editing, and querying structured documents. The structure of text is described using a context-free grammar. The operations are implemented using a powerful query language. The system supports the use of user-defined multiple views of the documents: one view can contain all the structure explicitly, while another can contain only part of the document and have only part of the structure visible. This makes the system flexible for different editing tasks. The system is implemented in C using a relational database system.",University of Helsinki,,3
299,Unifying Two-View and Three-View Geometry,"SHAI AVIDAN, AMNON SHASHUA","Abstract. The core of multiple-view geometry is governed by the fundamental matrix and the trilinear tensor. In this paper we unify both representations by first re-deriving the fundamental matrix as a rank deficient tensor, and secondly by deriving a unified set of operators that are transparent to the number of views. As a result, we show that the basic building block of the geometry of multiple views is the trilinear tensor of three views and that this tensor specializes to the fundamental matrix (in it's tensor form) in the case of two views. The properties of the tensor (geometric interpretation, contraction properties, etc.) are independent of the number of views (two or three). As a byproduct, every two-view algorithm can be considered as a degenerate three-view algorithm and three-view algorithms can work with either two or three images, all using one standard set of tensor operations. To highlight the usefulness of this paradigm we provide two practical applications. First we present a novel view synthesis algorithm that starts with the fundamental matrix (in its tensor form) and seamlessly move to the general trilinear tensor, all using one set of tensor operations. The second application is a camera stabilization algorithm, originally introduced for three views, now working with two views without modification.","Institute of Computer Science, The Hebrew University,","Jerusalem 91904, Israel",3
300,"Measure, Stochasticity, and the Density  of Hard Languages",Jack H. Lutz and Elvira Mayordomo,,Iowa State University of Science and Technology Department of Computer Science,"226 Atanasoff Ames, IA 50011",4
301,Coordination and Control Structures and Processes: Possibilities for Connectionist Networks (CN),Vasant Honavar & Leonard Uhr,"Abstract The absence of powerful control structures and processes that synchronize, coordinate, switch between, choose among, regulate, direct, modulate interactions between, and combine distinct yet interdependent modules of large connectionist networks (CN) is probably one of the most important reasons why such networks have not yet succeeded at handling difficult tasks (e.g. complex object recognition and description, complex problem-solving, planning). In this paper we examine how CN built from large numbers of relatively simple neuron-like units can be given the ability to handle problems that in typical multi-computer networks and artificial intelligence programs along with all other types of programs are always handled using extremely elaborate and precisely worked out central control (coordination, synchronization, switching, etc.). We point out the several mechanisms for central control of this un-brain-like sort that CN already have built into them albeit in hidden, often overlooked, ways. We examine the kinds of control mechanisms found in computers, programs, fetal development, cellular function and the immune system, evolution, social organizations, and especially brains, that might be of use in CN. Particularly intriguing suggestions are found in the pacemakers, oscillators, and other local sources of the brain's complex partial synchronies; the diffuse, global effects of slow electrical waves and neurohormones; the developmental program that guides fetal development; communication and coordination within and among living cells; the working of the immune system; the evolutionary processes that operate on large populations of organisms; and the great variety of partially competing partially cooperating controls found in small groups, organizations, and larger societies. All these systems are rich in control but typically control that emerges from complex interactions of many local and diffuse sources. We explore how several different kinds of plausible control mechanisms might be incorporated into CN, and assess their potential benefits with respect to their cost.",Computer Sciences Department University of Wisconsin-Madison,,0
302,EFFICIENT COMPILATION AND PROFILE-DRIVEN DYNAMIC RECOMPILATION IN SCHEME,Robert G. Burger,,"Department of Computer Science, Indiana University",,2
303,Using Goals and Experience to Guide Abduction,David B. Leake,"Abstract Standard methods for abductive understanding are neutral to prior experience and current goals. Candidate explanations are built from scratch by backwards chaining, without considering how similar situations were previously explained, and selection of the candidate to accept is based on its likelihood, without considering the information needs beyond routine understanding. Problems arise when applying these methods to everyday understanding: The vast range of possible explanations makes it difficult to control the cost of explanation construction and to assure that the explanations generated will actually be useful. We argue that these problems can be overcome by using goals and experience to guide both explanation generation and evaluation. Our work is within the framework of case-based explanation, which builds explanations by retrieving and adapting prior explanations stored in memory. We substantiate our model by describing mechanisms that enable it to effectively generate good explanations. First, we demonstrate that there exists a theory of anomaly and explanation that can guide retrieval of relevant explanations. Second, we present a plausibility evaluation process that efficiently detects conflicts and confirmations of an explanation's assumptions by prior patterns, making it possible to focus explanation adaptation when retrieved explanations are implausible. Third, we present methods for judging whether explanations provide the information needed to satisfy explainer goals beyond routine understanding. By reflecting experience and goals in the search for explanations, case-based explanation provides a practical mechanism for guiding search towards explanations that are both plausible and useful.","Department of Computer Science, Indiana University","Lindley Hall 215, Bloomington, IN 47405",1
304,Learning to Integrate Multiple Knowledge Sources for Case-Based Reasoning,"David B. Leake, Andrew Kinley, and David Wilson","Abstract The case-based reasoning process depends on multiple overlapping knowledge sources, each of which provides an opportunity for learning. Exploiting these opportunities requires not only determining the learning mechanisms to use for each individual knowledge source, but also how the different learning mechanisms interact and their combined utility. This paper presents a case study examining the relative contributions and costs involved in learning processes for three different knowledge sources|cases, case adaptation knowledge, and similarity information|in a case-based planner. It demonstrates the importance of interactions between different learning processes and identifies a promising method for integrating multiple learning methods to improve case-based reasoning.",Computer Science Department,"Lindley Hall 215,",1
305,3-D Stereo Using Photometric Ratios,Lawrence B. Wolff Elli Angelopoulou,"ABSTRACT We present a novel robust methodology for corresponding a dense set of points on an object surface from photometric values, for 3-D stereo computation of depth. The methodology utilizes multiple stereo pairs of images, each stereo pair taken of exactly the same scene but under different illumination. With just 2 stereo pairs of images taken respectively for 2 different illumination conditions, a stereo pair of ratio images can be produced; one for the ratio of left images, and one for the ratio of right images. We demonstrate how the photometric ratios composing these images can be used for accurate correspondence of object points. Object points having the same photometric ratio with respect to 2 different illumination conditions comprise a well-defined equivalence class of physical constraints defined by local surface orientation relative to illumination conditions. We formally show that for diffuse reection the photometric ratio is invariant to varying camera characteristics, surface albedo, and viewpoint and that therefore the same photometric ratio in both images of a stereo pair implies the same equivalence class of physical constraints. Corresponding photometric ratios along epipolar lines in a stereo pair of images under different illumination conditions is therefore a robust correspondence of equivalent physical constraints, and determination of depth from stereo can be performed without explicitly knowing what these physical constraints being corresponded actually are. This implies a very practical shape-from-stereo methodology applicable to perspective views and not requiring any knowledge whatsoever of illumination conditions. This is particularly practical for determination of 3-D shape on smooth featureless surfaces which has previously been hard to perform using stereo. We demonstrate experimental 3-D shape determination from a dense set of points using our stereo technique on smooth objects of known ground truth shape that are accurate to well within 1% depth accuracy.",Computer Vision Laboratory Department of Computer Science The Johns Hopkins University,"Baltimore, MD 21218",0
306,Deterministic Sorting in Nearly Logarithmic Time on the Hypercube and Related Computers,Robert Cypher,"Abstract This paper presents a deterministic sorting algorithm, called Sharesort, that sorts n records on an n-processor hypercube, shu*e-exchange, or cube-connected cycles in O(log n (log log n) 2 ) time in the worst case. The algorithm requires only a constant amount of storage at each processor. The fastest previous deterministic algorithm for this problem was Batcher's bitonic sort, which runs in O(log 2 n) time.",IBM Almaden Research Center,"650 Harry Rd. San Jose, CA 95120",0
307,"The Royal Tree Problem, a Benchmark for Single and Multi-population Genetic Programming","Bill Punch, Doug Zongker, and Erik Goodman","We report on work done to develop a benchmark problem for genetic programming, both as a difficult problem to test GP abilities and as a platform for tuning GP parameters. This benchmark, the royal tree, is a function that accounts for tree shape as part of its evaluation function, thus it controls for a parameter not often found in the GP literature. It also is a progressive function, allowing the user to set the difficulty of the problem attempted. We not only describe the function, but also report on results of using island parallelism for solving GP problems. The results obtained are somewhat surprising, as it appears that a single large population outperforms a group of smaller populations under all the conditions tested.",,,1
308,Inductive Constraint Logic and the Mutagenesis Problem,Wim Van Laer Hendrik Blockeel Luc De Raedt,"Abstract A novel approach to learning first order logic formulae from positive and negative examples is incorporated in a system named ICL (Inductive Constraint Logic). In ICL, examples are viewed as interpretations which are true or false for the target theory, whereas in present inductive logic programming systems, examples are true and false ground facts (or clauses). Furthermore, ICL uses a clausal representation, which corresponds to a conjunctive normal form where each conjunct forms a constraint on positive examples, whereas classical learning techniques have concentrated on concept representations in disjunctive normal form. We present some experiments with this new system on the mutagenesis problem. These experiments illustrate some of the differences with other systems, and indicate that our approach should work at least as well as the more classical approaches.","Department of Computer Science, Katholieke Universiteit Leuven","Celestijnenlaan 200A, B-3001 Heverlee, Belgium",6
309,Statistical Characteristics and Multiplexing of MPEG Streams,"Marwan Krunz , Ron Sass , and Herman Hughes","Abstract This paper presents a study of the statistical characteristics and multiplexing of Variable-Bit-Rate (VBR) MPEG-coded video streams. Our results are based on 23 minutes of video obtained from the entertainment movie, The Wizard of Oz. The experimental setup which was used to capture, digitize, and compress the video stream is described. Although the study is conducted at the frame level (as opposed to the slice level), it is observed that the inter-frame correlation structure for the frame-size sequence involves complicated forms of pseudo-periodicity that are mainly affected by the compression pattern of the sequence. A simple model for an MPEG traffic source is developed in which frames are generated according to the compression pattern of the original captured video stream. The number of cells per frame is fitted by a lognormal distribution. Simulations are used to study the performance of an ATM multiplexer for MPEG streams.",Department of Electrical Engineering Department of Computer Science Michigan State University,"East Lansing, MI 48824",1
310,An Agent-Based Approach for Robot Vision System,Tak Keung CHENG Leslie KITCHEN Zhi-Qiang LIU James COOPER,,,,3
311,Termination Analysis for Mercury,"Chris Speirs, Zoltan Somogyi and Harald Stndergaard","Abstract Since the late eighties, much progress has been made in the theory of termination analysis for logic programs. However, from a practical point of view, the significance of much of the work on termination is hard to judge, since experimental evaluations rarely get published. Here we describe and evaluate a termination analyzer for Mercury, a strongly typed and moded logic- functional programming language. Mercury's high degree of referential transparency and the guaranteed availability of reliable mode information simplify the termination analysis of Mer- cury compared with that of other logic programming languages. We describe our termination analyzer, which uses a variant of a method developed by Plumer. It deals with full Mercury, including modules, declarative input/output, the foreign language interface, and higher-order features. In spite of these obstacles, it produces high-quality termination information, comparable to the results recently obtained by Lindenstrauss and Sagiv. Most important, in stark contrast with Lindenstrauss and Sagiv's experimental results, our analyzer has a negligible impact on the running time of the compiler of which it is part, even for large programs. This means that the Mercury compiler can produce valuable termination information at no real cost to the programmer.",Department of Computer Science The University of Melbourne,"Parkville, Victoria 3052, Australia",3
312,Methods for Handling Faults and Asynchrony in Parallel Computation,Z. M. Kedem,,,,3
313,The Development of the C Language,Dennis M. Ritchie,"ABSTRACT The C programming language was devised in the early 1970s as a system implementation language for the nascent Unix operating system. Derived from the typeless language BCPL, it evolved a type structure; created on a tiny machine as a tool to improve a meager programming environment, it has become one of the dominant languages of today. This paper studies its evolution.",AT&T Bell Laboratories,"Murray Hill, NJ 07974 USA",2
314,DISTRIBUTED CONTROL IN OPTICAL WDM NETWORKS,X. Yuan R. Gupta R. Melhem,"ABSTRACT This paper describes and evaluates distributed wavelength reservation protocols for all-optical WDM networks. These protocols are essential for applying WDM techniques to large scale all-optical networks. The protocols ensure that the wavelengths on the links along a path are reserved before communication takes place. A message is transmitted using the reserved wavelengths and remains in the optical domain utill it reaches the destination. Based upon the timing at which the reservation is performed, the protocols are classified into two categories: forward reservation protocols and backward reservation protocols. Although forward reservation protocols are simpler, our performance study shows that backward reservation protocols provide better performance.",Department of Computer Science University of Pittsburgh,"Pittsburgh, PA 15260",3
315,"Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors",Edward Rothberg Jaswinder Pal Singh and Anoop Gupta,"Abstract The distribution of resources among processors, memory and caches is a crucial question faced by designers of large-scale parallel machines. If a machine is to solve problems with a certain data set size, should it be built with a large number of processors each with a small amount of memory, or a smaller number of processors each with a large amount of memory? How much cache memory should be provided per processor for cost-effectiveness? And how do these decisions change as larger problems are run on larger machines? In this paper, we explore the above questions based on the characteristics of five important classes of large-scale parallel scientific applications. We first show that all the applications have a hierarchy of well-defined per-processor working sets, whose size, performance impact and scaling characteristics can help determine how large different levels of a multiprocessor's cache hierarchy should be. Then, we use these working sets together with certain other important characteristics of the applications|such as communication to computation ratios, concurrency, and load balancing behavior|to reflect upon the broader question of the granularity of processing nodes in high-performance multiprocessors. We find that very small caches whose sizes do not increase with the problem or machine size are adequate for all but two of the application classes. Even in the two exceptions, the working sets scale quite slowly with problem size, and the cache sizes needed for problems that will be run in the foreseeable future are small. We also find that relatively fine-grained machines, with large numbers of processors and quite small amounts of memory per processor, are appropriate for all the applications.",Intel Supercomputer Systems Division Computer Systems Laboratory,14924 N.W. Greenbrier Parkway,1
316,Irregular Applications under Software Shared Memory,"Liviu Iftode, Jaswinder Pal Singh and Kai Li","Abstract Shared Virtual Memory (SVM) provides an inexpensive way to support the popular shared address space programming model on networks of workstations or personal computers. Despite recent advances in SVM systems, their performance for all but coarse-grained or regular applications is not well understood. Nor is there an understanding of whether and how fine-grained, irregular programs should be written differently for SVM, with its large granularities of communication and coherence, than for the more familiar hardware coherent at cache line granularity. In this paper we try to understand the performance and programming issues for emerging, irregular applications on SVM systems. We examine performance on both an aggressive all-software system as well as one with a little hardware support in the network interface. We also present approaches to improve the performance of irregular applications at both the programming and the system level. As a result of our experiences, we identify a set of guidelines and techniques that pertain specifically to programming SVM systems, beyond the guidelines commonly used for programming hardware-coherent systems as well. We also present a further relaxation of the memory consistency model, called scope consistency, which is particularly effective for such applications.",Department of Computer Science Princeton University,"Princeton, NJ 08544",0
317,Database and Display Algorithms for Interactive Visualization of Architectural Models,by Thomas Allen Funkhouser,,Computer Science,,3
318,Patience is a Virtue: The Effect of Delay on Competitiveness for Admission Control,by Michael H. Goldwasser 1,,Department of Computer Science Princeton University,"Princeton, NJ 08544",1
319,Investigating Genetic Algorithms for Scheduling,Hsiao-Lan Fang,,Department of Artificial Intelligence University of Edinburgh,,1
320,A Color-based Technique for Measuring Visible Loss for Use in Image Data Communication 1,"Melliyal Annamalai, Aurobindo Sundaram and Bharat Bhargava","Abstract The concept of the global information infrastructure and specifically that of the World Wide Web (WWW) has led to users accessing data of different media including images and video data over a wide area network. These data objects have sizes the order of megabytes and communication time is very large. The data size can be reduced without losing information by applying loss-inducing techniques and this will lead to reduction in communication time. Several loss-inducing techniques have been developed and each image is treated differently by each technique. In some cases an acceptable quality of the image is obtained and in some cases it is not. In this paper we develop a color-based technique to quantify the data loss when a loss-inducing technique is applied to an image. This will result in estimating whether the resulting image is indistinguishable from the original with respect to the human eye. We illustrate its use to classify images according to the loss they can tolerate. This avoids redundant communication of a high quality image when a lower quality image can satisfy the application resulting in the conservation and better usage of network resources. We present the technique, the communication time saved, and an experimental evaluation to prove the validity of the technique.",Department of Computer Sciences Purdue University,"W. Lafayette, IN 47906, USA",2
321,PYTHIA: A Knowledge Based System for Intelligent Scientific Computing,"Sanjiva Weerawarana, Elias N. Houstis, John R. Rice, Anupam Joshi","1. ABSTRACT Domain specific Problem Solving Environments (PSEs) are the key new ingredients that will aid in the widespread use of Computational Science & Engineering (CS&E) systems. Each PSE consists of a well defined library that supports the numerical and symbolic solution of certain mathematical model(s) characterizing a specific discipline, together with an easy to use software environment. This environment should ideally interact with the user in a language ""natural"" to the associated discipline, and provide a high level abstraction of the underlying, computationally complex, model. However, it appears that almost all extant PSEs assume that the user is familiar with the specific functionality/applicability of the PSE. Their primary design objective is to support some form of high level programming with predefined state-of-the-art algorithmic infrastructure. As the functionality of these systems increases, the user is expected to make complex decisions in the parametric space of the algorithmic infrastructure supported by the PSE. In this paper we describe a knowledge based system, PYTHIA, to automate this decision making process and aid in providing a high level abstraction to the user. Specifically, PYTHIA addresses the problem of (parameter, algorithm) pair selection within a scientific computing domain assuming some minimum user specified computational objectives and some characteristics of the given problem. PYTHIA's framework and methodology is general and applicable to any class of scientific problems and",Purdue University,"West Lafayette, IN 47907, USA;",4
322,Authorship Analysis: Identifying The Author of a Program 1,Ivan Krsul,,The COAST Project Department of Computer Sciences Purdue University,"West Lafayette, IN 47907-1398",3
323,A Coarse-Grained Parallel QR-Factorization Algorithm for Sparse Least Squares Problems,Tz. Ostromsky P. C. Hansen Z. Zlatev,"Abstract A sparse QR-factorization algorithm SPARQR for coarse-grained parallel computations is described. The coefficient matrix, which is assumed to be general sparse, is reordered in an attempt to bring as many zero elements in the lower left corner as possible. The reordered matrix is then partitioned into block rows, and Givens plane rotations are applied in each block-row. These are independent tasks and can be done in parallel. Row and column permutations are carried out within the diagonal blocks in an attempt to preserve better the sparsity of the matrix. The algorithm can be used for solving least squares problems either directly or combined with an iterative method (preconditioned conjugate gradients are used). Small non-zero elements can optionally be dropped in the latter case. This leads to a better preservation of the sparsity and, therefore, to a faster factorization. The price which has to be paid is some loss of accuracy. The iterative method is used to regain the accuracy lost during the factorization. Numerical results from several experiments with matrices from the well-known Harwell-Boeing collection as well as with some larger sparse matrices are presented in this work. An SGI Power Challenge computer with 16 processors has been used in the experiments.","Purdue University, Department of Computer Science,","West Lafayette, IN 47907, USA",2
324,Evaluating the Performance of Software Distributed Shared Memory as a Target for Parallelizing Compilers,"Alan L. Cox , Sandhya Dwarkadas , Honghui Lu and Willy Zwaenepoel","Abstract In this paper, we evaluate the use of software distributed shared memory (DSM) on a message passing machine as the target for a parallelizing compiler. We compare this approach to compiler-generated message passing, hand-coded software DSM, and hand-coded message passing. For this comparison, we use six applications: four that are regular and two that are irregular. Our results are gathered on an 8-node IBM SP/2 using the TreadMarks software DSM system. We use the APR shared-memory (SPF) compiler to generate the shared memory programs, and the APR XHPF compiler to generate message passing programs. The hand-coded message passing programs run with the IBM PVMe optimized message passing library. On the regular programs, both the compiler-generated and the hand-coded message passing outperform the SPF/TreadMarks combination: the compiler-generated message passing by 5.5% to 40%, and the hand-coded message passing by 7.5% to 49%. On the irregular programs, the SPF/TreadMarks combination outperforms the compiler-generated message passing by 38% and 89%, and only slightly underperforms the hand-coded message passing, differing by 4.4% and 16%. We also identify the factors that account for the performance differences, estimate their relative importance, and describe methods to improve the performance.",Rice University,"Houston, TX 77005-1892",4
325,On the Accuracy of Anderson's Fast N-body Algorithm,Yu Charlie Hu S. Lennart Johnsson,"Abstract We present an empirical study of the accuracy-cost tradeoffs of Anderson's method. The various parameters that control the degree of approximation of the computational elements and the separateness of interacting computational elements govern both the arithmetic complexity and the accuracy of the method. Our experiment shows that for a given error requirement, using a near-field containing only nearest neighbor boxes and a hierarchy depth that minimizes the number of arithmetic operations minimizes the total number of arithmetic operations.",,,0
326,"Synchronization, Coherence, and Consistency for High Performance Shared-Memory Multiprocessing",by Sandhya Dwarkadas,,RICE UNIVERSITY,,4
327,On GDM's: Geometrically Deformed Models for the Extraction of Closed Shapes from Volume Data,By James Vradenburg Miller,,Rensselaer Polytechnic Institute,"Troy, New York",2
328,Automated Design Optimization for the P2 and P8 Hypersonic Inlets,"Vijay Shukla , Andrew Gelsey , Mark Schwabacher  Donald Smith , Doyle D. Knight","Abstract An automated design methodology incorporating industry-standard Navier-Stokes codes and a gradient-based optimizer has been developed. This system is used to redesign the well-known NASA P2 and P8 hypersonic inlets. First, the Navier-Stokes simulations of the original P2 and P8 inlet designs are validated using numerical convergence studies and comparison with wind-tunnel experimental data for the original inlets published by NASA in the early 1970s. Second, the P2 and P8 inlets are redesigned with the objective of canceling the cowl shock (and, in the case of the P8 inlet, the additional cowl-generated compression) at the centerbody by appropriate contouring of the centerbody boundary. The original inlets were intended to achieve these same objectives, but detailed experimental measurements indicated that a substantial reflected shock system was present. The choice of the objective function, which is used to drive the optimization, has a significant impact on the final design. Several different formulations for the objective function have been employed, and improvements of 60% to 90% in the objective function have been achieved. This automated design system represents one of the first successful combinations of numerical optimization methods with Reynolds-averaged Navier-Stokes fluid dynamics simulation for high speed inlets, and demonstrates a new area in which High Performance Computing may have considerable impact on problems of military and industrial significance.","Rutgers, the State University of New Jersey","New Brunswick, NJ 08903",3
329,Towards a Cost-Effective Parallel Data Mining Approach,"Zoltan Jarai, Aashu Virmani, Liviu Iftode","Abstract Massive rule induction has recently emerged as one of the powerful data mining techniques. The problem is known to be exponential in the size of the attributes, and given its ever increasing use, can greatly benefit from parallelization. In this paper, we study cost-effective approaches to paral-lelize rule generation algorithms. In particular, we consider the propositional rule generation algorithm of the Discovery Board system, and present our design and implementation of a parallel algorithm for the same task. We then present some early performance results of our parallelization scheme on hardware and software distributed shared memory multiprocessors.",Department of Computer Science Rutgers University,"Piscataway, NJ 08854",2
330,Why Should Architectural Principles be Enforced?,Naftaly H. Minsky,"Abstract There is an emerging consensus that an explicit architectural model would be invaluable for large evolving software systems, providing them with a framework within which such a system can be reasoned about and maintained. But the great promise of architectural models has not been fulfilled so far, due to a gap between the model and the system it purports to describe. It is our contention that this gap is best bridged if the model is not just stated, but is enforced. This gives rise to a concept enforced architectural model |or, a law | which is explored in this paper. We argue that this model has two major beneficial consequences: First, by bridging the above mentioned gap between an architectural model and the actual system, an enforced architectural model provides a truly reliable framework within which a system can be reasoned about and maintained. Second, our model provides software developers with a carefully circumscribed flexibility in molding the law of a project, during its evolutionary lifetime|while maintaining certain architectural principles as invariant of evolution.",Department of Computer Science Rutgers University,"New Brunswick, NJ, 08903 USA",1
331,Learning Novel Domains Through Curiosity and Conjecture,Paul D. Scott & Shaul Markovitch,"Abstract This paper describes DIDO, a system we have developed to carry out exploratory learning of unfamiliar domains without assistance from an external teacher. The program incorporates novel approaches to experience generation and representation generation. The experience generator uses a heuristic based on Shannon's uncertainty function to find informative examples. The representation generator makes conjectures on the basis of small amounts of evidence and retracts them if they prove to be wrong or useless. A number of experiments are described which demonstrate that the system can distribute its learning resources to steadily acquire a good representation of the whole of a domain, and that the system can readily acquire both disjunctive and conjunctive concepts even in the presence of noise.",Center for Machine Intelligence,"2001,Commonwealth Blvd., Ann Arbor, Michigan 48105",4
332,Practical PAC Learning,Dale Schuurmans,"Abstract We present new strategies for ""probably approximately correct"" (pac) learning that use fewer training examples than previous approaches. The idea is to observe training examples one-at-a-time and decide ""on-line"" when to return a hypothesis, rather than collect a large fixed-size training sample. This yields sequential learning procedures that pac-learn by observing a small random number of examples. We provide theoretical bounds on the expected training sample size of our procedure | but establish its efficiency primarily by a series of experiments which show sequential learning actually uses many times fewer training examples in practice. These results demonstrate that pac-learning can be far more efficiently achieved in practice than previously thought.",Department of Computer Science University of Toronto,"Toronto, Ontario M5S 1A4, Canada",0
333,Learning Useful Horn Approximations,Russell Greiner,"Abstract While the task of answering queries from an arbitrary propositional theory is intractable in general, it can typically be performed efficiently if the theory is Horn. This suggests that it may be more efficient to answer queries using a ""Horn approximation""; i.e., a horn theory that is semantically similar to the original theory. The utility of any such approximation depends on how often it produces answers to the queries that the system actually encounters; we therefore seek an approximation whose expected ""coverage"" is maximal. Unfortunately, there are several obstacles to achieving this goal in practice: (i) The optimal approximation depends on the query distribution, which is typically not known a priori; (ii) identifying the optimal approximation is intractable, even given the query distribution; and (iii) the optimal approximation might be too large to guarantee tractable inference. This paper presents an approach that overcomes (or side-steps) each of these obstacles. We define a learning process, AdComp, that uses observed queries to estimate the query distribution ""online"", and then uses these estimates to hill-climb, efficiently, in the space of size-bounded Horn approximations, until reaching one that is, with provably high probability, effectively at a local optimum.",Siemens Corporate Research,755 College Road East,4
334,Coding Segments Inside and Outside the Chromosome,Thomas Haynes,"Abstract Coding segments are those sub-segments of the chromosome which contribute either positively or negatively to the fitness evaluation of the chromosome. We extract coding segments from chromosomes and we investigate the sharing of coding segments both inside and outside of the chromosome. We find duplication of coding segments inside the chromosomes provides a back-up mechanism for the search heuristics. We further find local search in a collective memory of coding segments outside of the chromosome, collective adaptation, enables the search heuristic to represent partial solutions which are larger than realistic chromosomes lengths and to express the solution outside of the chromosome.",Department of Computer Science Wichita State University,"Wichita, KS 67260",2
335,Support for Implementation of Evolutionary Concurrent Systems in Concurrent Programming Languages,Raju Pandey 1 and J. C. Browne 2,"Abstract. In many concurrent programming languages, concurrent programs are difficult to extend and modify: small changes in a concurrent program may require re-implementations of a large number of its components. In this paper a novel concurrent program composition mechanism is presented in which implementations of computations and synchronizations are completely separated. Separation of implementations facilitates extensions and modifications of programs by allowing one to change implementations of both computations and synchronizations. The paper also describes a concurrent programming model and a programming lan guage that support the proposed approach.","1 Computer Science Department, University of California,","Davis, CA 95616",0
336,The Test Matrix Toolbox for Matlab (Version 3.0),N. J. Higham,,Department of Mathematics University of Manchester,Manchester M13 9PL England,3
337,Greedy Algorithms in Greedy with Choice and Negation,Sergio Greco Carlo Zaniolo,"Abstract In the design of algorithms, the greedy paradigm provides a powerful tool for solving efficiently classical computational problems, within the framework of procedural languages. However, expressing these algorithms within the declarative framework of logic-based languages has proved to be a difficult research challenge. In this paper, we extend the framework of Datalog-like languages to obtain simple declarative formulations for such problems, and propose effective implementation techniques to ensure computational complexities comparable to those of procedural formulations. These advances are achieved through the use of the choice construct, that has semantics reducible to that of programs with negation under stable model semantics. Then we extend the fixpoint-based semantics of choice programs with preference annotations to guide search strategies and simple logic-based formulations of classical greedy algorithms.",Dip: Elettr: Informatica Sist: Computer Science Dept: Universita della Calabria Univ: of California at Los Angeles,87030 Rende; Italy LosAngeles; CA 90024,3
338,Run-time Compilation for Parallel Sparse Matrix Computations,Cong Fu and Tao Yang,"Abstract Run-time compilation techniques have been shown effective for automating the parallelization of loops with unstructured indirect data accessing patterns. However, it is still an open problem to efficiently parallelize sparse matrix factorizations commonly used in iterative numerical problems. The difficulty is that a factorization process contains irregularly-interleaved communication and computation with varying granularities and it is hard to obtain scalable performance on distributed memory machines. In this paper, we present an inspector/executor approach for parallelizing such applications by embodying automatic graph scheduling techniques to optimize interleaved communication and computation. We describe a run-time system called RAPID that provides a set of library functions for specifying irregular data objects and tasks that access these objects. The system extracts a task dependence graph from data access patterns, and executes tasks efficiently on a distributed memory machine. We discuss a set of optimization strategies used in this system and demonstrate the application of this system in parallelizing sparse Cholesky and LU factorizations.","Department of Computer Science University of California,","Santa Barbara, CA 93106.",2
339,A SUIF Java Compiler,Holger M. Kienle,,,,3
340,EXPLOITING COMMUTING OPERATIONS IN PARALLELIZING SERIAL PROGRAMS,PEDRO DINIZ AND MARTIN RINARD,Abstract. Two operations commute if the result of their execution is independent of the order in which they execute. Commuting operations can be executed concurrently provided they execute atomically on the objects they access. Statically recognizing commuting operations is of great interest because they increase the amount of concurrency a compiler can exploit. In this document we introduce commutativity analysis anew technique for automatically parallelizing serial programs. We then conduct a feasibility study of existing scientific applications as to the existence and exploitability of commuting operations. We study the commuting operations present in one such application the Barnes-Hut hierarchical N-body algorithm. We then parallelize this application using knowledge of commuting operations and present performance results of the parallel code for a shared-memory multiprocessor.,"DEPARTMENT OF COMPUTER SCIENCE UNIVERSITY OF CALIFORNIA, SANTA BARBARA","SANTA BARBARA, CA 93106",4
341,Scintilla: Cluster Computing with SCI,"Max Ibel, Klaus E. Schauser, Chris J. Scheiman, and Michael Schmitt","Abstract The Scintilla project at UCSB studies SCI-based cluster computing. The Scalable Coherent Interface (SCI) is a recent communication standard for cluster interconnects. We focus on non-coherent SCI, using our cluster setup of SBus-based and PCI-based workstations connected via Dolphin SCI adapters. Our motivation for choosing SCI as network fabric is the very low latency and high bandwidth. We study how to map a variety of programming models efficiently onto the SCI hardware, focusing on message passing and global address space support, implementing Active Messages and Split-C. We present implementation trade-offs, present performance measurements and compare the PCI and SBus adapters. We found that the user-level load/store programming interface of SCI is very convenient to use, achieves low latencies, and is fully virtualized, simultaneously supporting multiple parallel programs and communication channels. On the other hand, neither of the programming models studied maps directly to SCI. Issues such as notification, atomic operations, and virtual address space limitations represent major implementation challenges, which we address with a combination of compiler and run-time support. Overall, we found the SCI network a good substrate for high-performance cluster computing.","Department of Computer Science University of California, Santa Barbara","Santa Barbara, CA 93106",4
342,Design and Implementation of a System to Support Integration of Autonomous Database Systems,by Tze Kwan Lau,,UNIVERSITY of CALIFORNIA Santa Barbara,,2
343,How to Sign Given Any Trapdoor Permutation,Mihir Bellare Silvio Micali,"Abstract We present a digital signature scheme which is based on the existence of any trapdoor permutation. Our scheme is secure in the strongest possible natural sense: namely, it is secure against existential forgery under adaptive chosen message attack.","Department of Computer Science & Engineering,","Mail Code 0114,",1
344,Mapping Parallel Applications to Distributed Heterogeneous Systems,Silvia M. Figueira 1 and Francine Berman 2,"Abstract Fast networks have made it possible to coordinate distributed heterogeneous CPU, memory and storage resources to provide a powerful platform for executing high-performance applications. However, the performance of parallel applications on such systems is highly dependent on the mapping of application tasks to machines. In this paper, we propose a mapping strategy for applications formed by multiple tasks targeted to heterogeneous platforms. We first define a mapping model, the match-tree, which reects the data movement and conversion costs of distributed algorithms and allows for alternative implementations of individual tasks on different machines. We then define the find-mapping and split-partition algorithms, based on the match-tree model, to determine the best allocation of tasks to resources in heterogeneous systems. We illustrate the use of these algorithms with a sample distributed application.","Department of Computer Science and Engineering University of California, San Diego",,3
345,Learning to Schedule Straight-Line Code,J. Eliot B. Moss,"Abstract Execution speed of programs on modern computer architectures is sensitive, by a factor of two or more, to the order in which instructions are presented to the processor. To realize potential execution efficiency, it is now customary for an optimizing compiler to employ a heuristic algorithm for instruction scheduling. These algorithms are painstakingly hand-crafted, which is expenseive and time-consuming. We show how to cast the instruction scheduling problem as a learning task, so that one obtains the heuristic scheduling algorithm automatically. Our focus is the narrower problem of scheduling straight-line code, also known as a basic block of instructions. Our empirical results show that just a few features are adequate for quite good performance at this task for a real modern processor, and that any of several supervised learning methods perform nearly optimally with respect to the features used.",Dept. of Comp. Sci. Univ. of Mass.,"Amherst, MA 01003",5
346,Intra-Option Learning about Temporally Abstract Actions,Richard S. Sutton,"Abstract Several researchers have proposed modeling temporally abstract actions in reinforcement learning by the combination of a policy and a termination condition, which we refer to as an option. Value functions over options and models of options can be learned using methods designed for semi-Markov decision processes (SMDPs). However, all these methods require an option to be executed to termination. In this paper we explore methods that learn about an option from small fragments of experience consistent with that option, even if the option itself is not executed. We call these methods intra-option learning methods because they learn from experience within an option. Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporal-difference mechanisms to learn simultaneously about all the options consistent with an experience, not just the few that were actually executed. In this paper we present intra-option learning methods for learning value functions over options and for learning multi-time models of the consequences of options. We present computational examples in which these new methods learn much faster than SMDP methods and learn effectively when SMDP methods cannot learn at all. We also sketch a convergence proof for intra option value learning.",Department of Computer Science University of Massachusetts,"Amherst, MA 01003-4610",3
347,Composite Model Checking with Type Specific Symbolic Encodings,Tevfik Bultan Richard Gerber,"Abstract We present a new symbolic model checking technique, which analyzes temporal properties in multi-typed transition systems. Specifically, the method uses multiple type-specific data encodings to represent system states, and it carries out fixpoint computations via the corresponding type-specific symbolic operations. In essence, different symbolic encodings are unified into one composite model checker. Any type-specific language can be included in this framework provided that the language is closed under Boolean connectives, propositions can be checked for satisfiability, and relational images can be computed. Our technique relies on conjunctive partitioning of transition relations of atomic events based on variable types involved, which allows independent computation of one-step pre- and post-conditions for each variable type. In this paper we demonstrate the effectiveness of our method on a nontrivial data-transfer protocol, which contains a mixture of integer and Boolean-valued variables. The protocol operates over an unreliable channel that can lose, duplicate or reorder messages. Moreover, the protocol's send and receive window sizes are not specified in advance; rather, they are represented as symbolic constants. The resulting system was automatically verified using our composite model checking approach, in concert with a conservative approximation technique.","Department of Computer Science University of Maryland,","College Park, MD 20742, USA",2
348,"Ratio Rules: A New Paradigm for Fast, Quantifiable Data Mining","Flip Korn, Alexandros Labrinidis, Yannis Kotidis","Abstract Association Rule Mining algorithms operate on a data matrix (e.g., customers fi products) to derive association rules [2, 23]. We propose a new paradigm, namely, Ratio Rules, which are quantifiable in that we can measure the ""goodness"" of a set of discovered rules. We propose to use the ""guessing error"" as a measure of the ""goodness"", that is, the root-mean-square error of the reconstructed values of the cells of the given matrix, when we pretend that they are unknown. Another contribution is a novel method to guess missing/hidden values from the Ratio Rules that our method derives. For example, if somebody bought $10 of milk and $3 of bread, our rules can ""guess"" the amount spent on, say, butter. Thus, we can perform a variety of important tasks such as forecasting, answering ""what-if"" scenarios, detecting outliers, and visualizing the data. Moreover, we show how to compute Ratio Rules in a single pass over the dataset with small memory requirements (a few small matrices), in contrast to traditional association rule mining methods that require multiple passes and/or large memory. Experiments",Department of Computer Science University of Maryland,"College Park, MD 20742",0
349,CIRCA and the Cassini Saturn Orbit Insertion: Solving a Prepositioning Problem,David J. Musliner and Robert P. Goldman,,Automated Reasoning Group Honeywell Technology Center,"3660 Technology Drive Minneapolis, MN 55418",4
350,Within the Letter of the Law: open-textured planning,Kathryn E. Sanders,"Abstract Most case-based reasoning systems have used a single ""best"" or ""most similar"" case as the basis for a solution. For many problems, however, there is no single exact solution. Rather, there is a range of acceptable answers. We use cases not only as a basis for a solution, but also to indicate the boundaries within which a solution can be found. We solve problems by choosing some point within those boundaries. In this paper, I discuss this use of cases with illustrations from chiron, a system I have implemented in the domain of personal income tax planning.",Department of Computer Science University of Maryland,"College Park, MD 20742",5
351,Efficiently Supporting Ad Hoc Queries in Large Datasets of Time Sequences,Flip Korn,"Abstract Ad hoc querying is difficult on very large datasets, since it is usually not possible to have the entire dataset on disk. While compression can be used to decrease the size of the dataset, compressed data is notoriously difficult to index or access. In this paper we consider a very large dataset comprising multiple distinct time sequences. Each point in the sequence is a numerical value. We show how to compress such a dataset into a format that supports ad hoc querying, provided that a small error can be tolerated when the data is uncompressed. Experiments on large, real world datasets (AT&T customer calling patterns) show that the proposed method achieves an average of less than 5% error in any data value after compressing to a mere 2.5% of the original space (i.e., a 40:1 compression ratio), with these numbers not very sensitive to dataset size. Experiments on aggregate queries achieved a 0.5% reconstruction error with a space requirement under 2%.",Dept. of Computer Science University of Maryland,"College Park, MD 20742",1
352,GENERATING REDESIGN SUGGESTIONS TO REDUCE SETUP COST: A STEP TOWARDS AUTOMATED REDESIGN,Diganta Das,"Abstract All mechanical designs pass through a series of formal and informal redesign steps, involving the analysis of functionality, manufacturability, cost and other life-cycle factors. The speed and efficacy of these steps has a major influence on the lead time of the product from conceptualization to launching. In this paper we propose a methodology for automatically generating redesign suggestions for reducing setup costs for machined parts. Given an interpretation of the design as a collection of machinable features, our approach is to generate alternate machining features by making geometric changes to the original features, and add them to the feature set of the original part to create an extended feature set. The designer may provide restrictions on the design indicating the type and extent of modifications allowed on certain faces and volumes, in which case all redesign suggestions generated by our approach honor those restrictions. By taking combinations of features from the extended feature set generated above, we can generate modified versions of the original design that still satisfy the designer's intent. By considering precedence constraints and approach directions for the machining operations as well as simple fixturability constraints, we can estimate the setup time that will be required for each design. Any modified design whose setup time is less than that of the original design can be presented to the designer as a possible way to modify the original design.","Mechanical Engr. Dept. and Institute for Systems Research, University of Maryland","College Park, MD 20742",3
353,Supporting Stored Video: Reducing Rate Variability and End-to-End Resource Requirements through Optimal Smoothing,"James D. Salehi, Zhi-Li Zhang, James F. Kurose, and Don Towsley","Abstract VBR compressed video is known to exhibit significant, multiple- time-scale bit rate variability. In this paper, we consider the transmission of stored video from a server to a client across a high speed network, and explore how the client buffer space can be used most effectively toward reducing the variability of the transmitted bit rate. We present two basic results. First, we present an optimal smoothing algorithm for achieving the greatest possible reduction in rate variability when transmitting stored video to a client with given buffer size. We provide a formal proof of optimality, and demonstrate the performance of the algorithm on a set of long MPEG-1 encoded video traces. Second, we evaluate the impact of optimal smoothing on the network resources needed for video transport, under two network service models: Deterministic Guar- anteed service [1, 11] and Renegotiated CBR (RCBR) service [9, 8]. Under both models, we find the impact of optimal smoothing to be dramatic.","Department of Computer Science, University of Massachusetts,","Amherst MA 01003, USA",3
354,Reducing False Sharing on Shared Memory Multiprocessors through Compile Time Data Transformations,Tor E. Jeremiassen,"Abstract We have developed compiler algorithms that analyze explicitly parallel programs and restructure their shared data to reduce the number of false sharing misses. The algorithms analyze per-process shared data accesses, pinpoint the data structures that are susceptible to false sharing and choose an appropriate transformation to reduce it. The transformations either group data that is accessed by the same processor or separate individual data items that are shared. This paper evaluates that technique. We show through simulation that our analysis successfully identifies the data structures that are responsible for most false sharing misses, and then transforms them without unduly decreasing spatial locality. The reduction in false sharing positively impacts both execution time and program scalability when executed on a KSR2. Both factors combine to increase the maximum achievable speedup for all programs, more than doubling it for several. Despite being able to only approximate actual inter-processor memory accesses, the compiler-directed transformations always outperform programmer efforts to eliminate false sharing.",AT&T Bell Laboratories,"600 Mountain Ave. Murray Hill, New Jersey 07974",2
355,Compiler Support for Maintaining Cache Coherence Using Data Prefetching ?,"Hock-Beng Lim 1 , Lynn Choi 2 and Pen-Chung Yew 3",,"1 Center for Supercomputing R & D, Univ. of Illinois,","Urbana, IL 61801",6
356,Constraint-Based Animations,Allan Heydon Greg Nelson,,Digital Systems Research Center,"130 Lytton Ave., Palo Alto, CA 94301",2
357,Parallel Multilevel k -way Partitioning Scheme for Irregular Graphs,George Karypis and Vipin Kumar,"Abstract In this paper we present a parallel formulation of a multilevel k-way graph partitioning algorithm. The multilevel k-way partitioning algorithm reduces the size of the graph by collapsing vertices and edges (coarsening phase), finds a k-way partitioning of the smaller graph, and then it constructs a k-way partitioning for the original graph by projecting and refining the partition to successively finer graphs (uncoarsening phase). A key innovative feature of our parallel formulation is that it utilizes graph coloring to effectively parallelize both the coarsening and the refinement during the uncoarsening phase. Our algorithm is able to achieve a high degree of concurrency, while maintaining the high quality partitions produced by the serial algorithm. We test our scheme on a large number of graphs from finite element methods, and transportation domains. For graphs with a million vertices, our parallel formulation produces high quality 128-way partitions on 128 processors in a little over two seconds, on Cray T3D. Thus our parallel algorithm makes it feasible to perform frequent dynamic graph partition in adaptive computations without compromising quality.","University of Minnesota, Department of Computer Science","Minneapolis, MN 55455,",5
358,The COOL architecture and abstractions for object-oriented distributed operating systems,"Rodger Lea, Christian Jacquemot","abstract: Building distributed operating systems benefits from the micro-kernel approach by allowing better support for modularization. However, we believe that we need to take this support a step further. A more modular, or object oriented approach is needed if we wish to cross the barrier of complexity that is holding back distributed operating system development. The Chorus Ob ject Oriented Layer (COOL) is a layer built above the Chorus micro-kernel designed to extend the micro-kernel abstractions with support for object oriented systems. COOL v2, the second iteration of this layer provides generic support for clusters of objects, in a distributed virtual memory model. It is built as a layered system where the lowest layer support only clusters and the upper layers support objects.",,,2
359,AN ALGORITHM FOR GENERATING EXECUTABLE ASSERTIONS FOR FAULT TOLERANCE,"Martina Schollmeyer, Hanan Lutfiyya and Bruce McMillin",,Department of Computer Science University of Missouri at Rolla,"Rolla, Missouri 65401",2
360,Universal Constructions for Large Objects,James H. Anderson and Mark Moir,"Abstract We present lock-free and wait-free universal constructions for implementing large shared objects. Most previous universal constructions require processes to copy the entire object state, which is impractical for large objects. Previous attempts to address this problem require programmers to explicitly fragment large objects into smaller, more manageable pieces, paying particular attention to how such pieces are copied. In contrast, our constructions are designed to largely shield programmers from this fragmentation. Furthermore, for many objects, our constructions result in lower copying overhead than previous ones. Fragmentation is achieved in our constructions through the use of load-linked, store-conditional, and validate operations on a ""large"" multi-word shared variable. Before presenting our constructions, we show that these operations can be efficiently implemented from similar one-word primitives.","Dept. of Computer Science, University of North Carolina at Chapel Hill",,2
361,Modeling and Parameter Estimation of the Human Index Finger,Robert N. Rohling and John M. Hollerbach,"Abstract Precise teleoperation of dextrous robotic hands by hand masters requires an accurate human hand model. A kinematic model of a human index finger is developed as an example for human hand modeling. The parameters of the model are determined by open-loop kinematic calibration. Singular value decomposition is used as a tool for analyzing the kinematic model and the identification process. Accurate and reliable results are obtained only when the numerical condition is minimized through parameter scaling, model reduction and pose set selection. The identified kinematic parameters show the kinematic model and calibration procedure have an accuracy on the order of a few millimeters.","Biorobotics Laboratory, McGill University","3775 University St., Montreal, Quebec H3A 2B4",5
362,The Next Frontier: Interactive and Closed Loop Performance Steering,Daniel A. Reed Christopher L. Elford Tara M. Madhyastha Evgenia Smirni Stephen E. Lamm,"Abstract Software for a growing number of problem domains has complex, time varying behavior and unpredictable resource demands (e.g., WWW servers and parallel input/output systems). While current performance analysis tools provide insights into application dynamics and the causes of poor performance, with a posteriori analysis one cannot adapt to temporally varying application resource demands and system responses. We believe that the solution to this performance optimization conundrum is integration of dynamic performance instrumentation and on-the-fly performance data reduction with real-time adaptive control mechanisms that select and configure resource management algorithms automatically, based on observed application behavior, or interactively, through high-modality virtual environments. We motivate this belief by first describing our experiences with performance analysis tools, input/output characterization, and WWW server analysis, and then sketching the design of interactive and closed loop adaptive control systems.",Department of Computer Science University of Illinois,"Urbana, Illinois 61801",2
363,Optimal Wire-Sizing Formula Under the Elmore Delay Model,"Chung-Ping Chen, Yao-Ping Chen, and D. F. Wong","Abstract In this paper, we consider non-uniform wire-sizing. Given a wire segment of length L, let f(x) be the width of the wire at position x, 0 x L. We show that the optimal wire-sizing function that minimizes the Elmore delay through the wire is f(x) = ae bx , where a &gt; 0 and b &gt; 0 are constants that can be computed in O(1) time. In the case where lower bound (L &gt; 0) and upper bound (U &gt; 0) on the wire widths are given, we show that the optimal wire-sizing function f(x) is a truncated version of ae bx that can also be determined in O(1) time. Our wire-sizing formula can be iteratively applied to optimally size the wire segments in a routing tree.","Department of Computer Sciences, University of Texas,","Austin, Texas 78712",4
364,Programming the Web: An Application-oriented Language for Hypermedia Services,David A. Ladd J. Christopher Ramming,"Abstract MAWL is an application language for programming interactive services in the context of the Worldwide Web. The language is small, because no construct was introduced without compelling justification; as with yacc [8], general-purpose computation is done in a host language. MAWL offers conveniences such as control abstraction, persistent state management, synchronization, and shared memory. In addition, the MAWL compiler performs static checking designed to prevent common Web programming errors. In this paper we discuss the design and engineering of MAWL. We describe the problems MAWL is intended to solve, and then discuss our design choices in the context of our general language design philosophy, We also include an appendix of commentary on several short MAWL programs.",AT&T Bell Laboratories,,2
365,ON UNAPPROXIMABLE VERSIONS OF NP-COMPLETE PROBLEMS,DAVID ZUCKERMAN,"Abstract. We prove that all of Karp's 21 original NP -complete problems have a version that's hard to approximate. These versions are obtained from the original problems by adding essentially the same, simple constraint. We further show that these problems are absurdly hard to approximate. In fact, no polynomial-time algorithm can even approximate log (k) of the magnitude of these problems to within any constant factor, where log (k) denotes the logarithm iterated k times, unless N P is recognized by slightly superpolynomial randomized machines. We use the same technique to improve the constant * such that MAX CLIQUE is hard to approximate to within a factor of n * . Finally, we show that it is even harder to approximate two counting problems: counting the number of satisfying assignments to a monotone 2-SAT formula and computing the permanent of -1,0,1 matrices.",,,2
366,Frangipani: A Scalable Distributed File System,Chandramohan A. Thekkath Timothy Mann Edward K. Lee,"Abstract The ideal distributed file system would provide all its users with coherent, shared access to the same set of files,yet would be arbitrarily scalable to provide more storage space and higher performance to a growing user community. It would be highly available in spite of component failures. It would require minimal human administration, and administration would not become more complex as more components were added. Frangipani is a new file system that approximates this ideal, yet was relatively easy to build because of its two-layer structure. The lower layer is Petal (described in an earlier paper), a distributed storage service that provides incrementally scalable, highly available, automatically managed virtual disks. In the upper layer, multiple machines run the same Frangipani file system code on top of a shared Petal virtual disk, using a distributed lock service to ensure coherence. Frangipani is meant to run in a cluster of machines that are under a common administration and can communicate securely. Thus the machines trust one another and the shared virtual disk approach is practical. Of course, a Frangipani file system can be exported to untrusted machines using ordinary network file access protocols. We have implemented Frangipani on a collection of Alphas running DIGITAL Unix 4.0. Initial measurements indicate that Frangipani has excellent single-server performance and scales well as servers are added.",Systems Research Center Digital Equipment Corporation,"130 Lytton Ave, Palo Alto, CA 94301",2
367,Constructing Scripts from Components: Working Note 6,Peter Clark and Bruce Porter,,"Dept. CS, UT Austin",,6
368,Broadcasting on Meshes with Worm-Hole Routing,Michael Barnett,"Abstract We address the problem of broadcasting on mesh architectures with arbitrary (non-power-two) dimensions. It is assumed that such mesh architectures employ cut-through or worm-hole routing. The primary focus is on avoiding network conflicts in the various proposed algorithms. We give algorithms for performing a conflict-free minimum-spanning tree broadcast, a pipelined algorithm that is similar to Ho and Johnsson's EDST algorithm for hypercubes, and a novel scatter-collect approach that is a natural choice for communication libraries due to its simplicity. Results obtained on the Intel Paragon system are included.",Department of Computer Science University of Idaho,"Moscow, Idaho 83844-1010",2
369,Anatomy of a Parallel Out-of-Core Dense Linear Solver,Kenneth Klimkowski,"Abstract In this paper, we describe the design and implementation of the Platform Independent Parallel Solver (PIPSolver) package for the out-of-core (OOC) solution of complex dense linear systems. Our approach is unique in that it allows essentially all of RAM to be filled with the current portion of the matrix (slab) to be updated and factored, thereby greatly improving the computation to I/O ratio over previous approaches. Experiences and performance are reported for the Cray T3D system.",Texas Institute for Computational and Applied Mathematics The University of Texas at Austin,"Austin, Texas 78712",4
370,NetSolve: A Network Server for Solving Computational Science Problems,Henri Casanova,"Abstract This paper presents a new system, called NetSolve, that allows users to access computational resources, such as hardware and software, distributed across the network. The development of NetSolve was motivated by the need for an easy-to-use, efficient mechanism for using computational resources remotely. Ease of use is obtained as a result of different interfaces, some of which require no programming effort from the user. Good performance is ensured by a load-balancing policy that enables NetSolve to use the computational resources available as efficiently as possible. NetSolve offers the ability to look for computational resources on a network, choose the best one available, solve a problem (with retry for fault-tolerance), and return the answer to the user.","University of Tennessee UTK, Dept. of Computer Science","104, Ayres Hall. KNOXVILLE, TN 37996-1301.",4
371,An Ordering on Subgoals for Planning,Fangzhen Lin,Abstract Subgoal ordering is a type of control information that has received much attention in AI planning community. In this paper we formulate precisely a subgoal ordering in the situation calculus. We show how information about this subgoal ordering can be deduced from the background action theory. We also show for both linear and nonlinear planners how knowledge about this ordering can be used in a provably correct way to avoid unnecessary backtracking.,epartment of Computer Science The Hong Kong University of Science and Technology,"Clear Water Bay, Kowloon, Hong Kong",5
372,Specifying Instructions' Semantics Using CSDL (Preliminary Report),Norman Ramsey and Jack W. Davidson,,Department of Computer Science University of Virginia,"Charlottesville, VA 22903",5
373,A Distributed Protocol for Channel-Based Communication with Choice,Frederick Knabe,"Abstract Recent attempts at incorporating concurrency into functional languages have identified synchronous communication via shared channels as a promising primitive. An additional useful feature found in many proposals is a nondeterministic choice operator. Similar in nature to the CSP alternative command, this operator allows different possible actions to be guarded by sends or receives. Choice is difficult to implement in a distributed environment because it requires offering many potential communications but closing only one. In this paper we present the first distributed, deadlock-free algorithm for choice.",European Computer-Industry Research Centre GmbH,"Munich, Germany",1
374,Non-Tree Routing,Bernard A. McCoy and Gabriel Robins,"Abstract An implicit premise of existing routing methods is that the routing topology must correspond to a tree (i.e., it does not contain cycles). In this paper we investigate the consequences of abandoning this basic axiom, and instead we allow routing topologies that correspond to arbitrary graphs (i.e., where cycles are allowed). We show that non-tree routing can significantly improve signal propagation delay, reduce signal skew, and afford increased reliability with respect to open faults that may be caused by manufacturing defects and electro-migration. Simulations on uniformly-distributed nets indicate that depending on net size and technology parameters, our non-tree routing construction reduces maximum sourse-sink SPICE delay by an average of up to 62%, and reduces signal skew by an average of up to 63%, as compared with Steiner routing. Moreover, up to 77% of the total wirelength in non-trees can tolerate an open fault without disconnecting the circuit.","Department of Computer Science, University of Virginia,","Charlottesville, VA 22903-2442",0
375,Using Runtime Measured Workload Characteristics in Parallel Processor Scheduling,"Thu D. Nguyen, Raj Vaswani, and John Zahorjan",,"Department of Computer Science and Engineering,",Box 352350,3
376,PCp 3 : A C Front End for Preprocessor Analysis and Transformation,Greg J. Badros,"Abstract Though the C preprocessor provides necessary language features, it does so in an unstructured way. The lexical nature of cpp creates numerous problems for software engineers and their tools, all stemming from the chasm between the engineer's view of the source code and the compiler's view. The simplest way to reduce this problem is to minimize use of the preprocessor. In light of the data collected in a prior empirical analysis, this paper describes a tool to aid the software engineer in analyses targeted at replacing preprocessor constructs with language features. Existing tools for analyzing C source in the context of the preprocessor are unsuitable for such transformations. This work introduces a new approach: tightly integrating the preprocessor with a C language parser, permitting the code to be analyzed at both the preprocessor and syntactic levels simultaneously. The front-end framework, called PCp 3 , combines a preprocessor, a parser, and arbitrary Perl subroutine ""hooks"" invoked upon various preprocessor and parser events. PCp 3 's strengths and weaknesses are discussed in the context of several program understanding and transformation tools, including a conservative analysis to support replacing cpp's #define directives with C++ language features.",,,4
377,Effective Cache Prefetching on Bus-Based Multiprocessors,Dean M. Tullsen and Susan J. Eggers,"Abstract Compiler-directed cache prefetching has the potential to hide much of the high memory latency seen by current and future high-performance processors. However, prefetching is not without costs, particularly on a multiprocessor. Prefetching can negatively affect bus utilization, overall cache miss rates, memory latencies and data sharing. We simulate the effects of a compiler-directed prefetching algorithm, running on a range of bus-based multiprocessors. We show that, despite a high memory latency, this architecture does not necessarily support prefetching well, in some cases actually causing performance degradations. We pinpoint several problems with prefetching on a shared memory architecture (additional conflict misses, no reduction in the data sharing traffic and associated latencies, a multiprocessor's greater sensitivity to memory utilization and the sensitivity of the cache hit rate to prefetch distance) and measure their effect on performance. We then solve those problems through architectural techniques and heuristics for prefetching that could be easily incorporated into a compiler: 1) victim caching, which eliminates most of the cache conflict misses caused by prefetching in a direct-mapped cache, 2) special prefetch algorithms for shared data, which significantly improve the ability of our basic prefetching algorithm to prefetch invalidation misses, and 3) compiler-based shared data restructuring, which eliminates many of the invalidation misses the basic prefetching algorithm doesn't predict. The combined effect of these improvements is to make prefetching effective over a much wider range of memory architectures.",University of Washington,,2
378,Model-Based Diagnosis using Structured System Descriptions,Adnan Darwiche,"Abstract This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form (NNF) and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions.",Department of Mathematics American University of Beirut,"PO Box 11-236 Beirut, Lebanon",4
379,Partition Based Spatial-Merge Join,Jignesh M. Patel David J. DeWitt,"Abstract This paper describes PBSM (Partition Based Spatial-Merge), a new algorithm for performing spatial join operation. This algorithm is especially effective when neither of the inputs to the join have an index on the joining attribute. Such a situation could arise if both inputs to the join are intermediate results in a complex query, or in a parallel environment where the inputs must be dynamically redistributed. The PBSM algorithm partitions the inputs into manageable chunks, and joins them using a computational geometry based plane-sweeping technique. This paper also presents a performance study comparing the the traditional indexed nested loops join algorithm, a spatial join algorithm based on joining spatial indices, and the PBSM algorithm. These comparisons are based on complete implementations of these algorithms in Paradise, a database system for handling GIS applications. Using real data sets, the performance study examines the behavior of these spatial join algorithms in a variety of situations, including the cases when both, one, or none of the inputs to the join have an suitable index. The study also examines the effect of clustering the join inputs on the performance of these join algorithms. The performance comparisons demonstrates the feasibility, and applicability of the PBSM join algorithm.","Computer Sciences Department, University of Wisconsin, Madison",,4
380,Maximal-Munch Tokenization in Linear Time,THOMAS REPS,"The lexical-analysis (or scanning) phase of a compiler attempts to partition the input stream into a sequence of tokens. The convention in most languages is that the input is scanned left to right, and each token identified is a maximal munch of the remaining inputthe longest prefix of the remaining input that is a token of the language. Most textbooks on compiling have extensive discussions of lexical analysis in terms of finite-state automata and regular expressions: Token classes are defined by a set of regular expressions R i , 1 i k, and the lexical analyzer is based on some form of finite-state automaton for recognizing the language L (R 1 + R 2 + . . . + R k ). However, the treatment is unsatisfactory in one respect: The theory of finite-state automata assumes that the end of the input stringi.e., the right-hand-side boundary of the candidate for recognitionis known a priori, whereas a scanner must identify the next token without knowing a definite bound on the extent of the token. Although most of the standard compiler textbooks discuss this issue, the solution they sketch out is one thatfor certain sets of token definitionscan cause the scanner to exhibit quadratic behavior in the worst case. This property is not only dissatisfying, it blemishes an otherwise elegant treatment of lexical analysis. In this paper, we rectify this defect: We show that, given a deterministic finite-state automaton that recognizes the tokens of a language, maximal-munch tokenization can always be performed in time linear in the size of the input.",University of Wisconsin,,5
381,Cost-Aware WWW Proxy Caching Algorithms,Pei Cao Sandy Irani,"Abstract Web caches can not only reduce network traffic and downloading latency, but can also affect the distribution of web traffic over the network through cost-aware caching. This paper introduces GreedyDual-Size, which incorporates locality with cost and size concerns in a simple and non-parameterized fashion for high performance. Trace-driven simulations show that with the appropriate cost definition, GreedyDual-Size outperforms existing web cache replacement algorithms in many aspects, including hit ratios, latency reduction and network cost reduction. In addition, GreedyDual-Size can potentially improve the performance of main-memory caching of Web documents.","Department of Computer Science, Information and Computer Science Department, University of Wisconsin-Madison. University of California-Irvine.",,4
382,High Performance Computing Division,Aart J.C. Bik,,Department of Computer Science Leiden University,"P.O. Box 9512, 2300 RA Leiden The Netherlands",4
383,Kerberos: An Authentication Service for Open Network Systems,Jennifer G. Steiner,"ABSTRACT In an open network computing environment, a workstation cannot be trusted to identify its users correctly to network services. Kerberos provides an alternative approach whereby a trusted third-party authentication service is used to verify users' identities. This paper gives an overview of the Kerberos authentication model as implemented for MIT's Project Athena. It describes the protocols used by clients, servers, and Kerberos to achieve authentication. It also describes the management and replication of the database required. The views of Kerberos as seen by the user, programmer, and administrator are described. Finally, the role of Kerberos in the larger Athena picture is given, along with a list of applications that presently use Kerberos for user authentication. We describe the addition of Kerberos authentication to the Sun Network File System as a case study for integrating Kerberos with an existing application.",Project Athena Massachusetts Institute of Technology,"Cambridge, MA 02139",4
384,Approximate Analysis of Parallel Processor Allocation Policies,Rajesh K. Mansharamani and Mary K. Vernon,"Abstract The complexity of parallel applications and parallel processor scheduling policies makes both exact analysis and simulation difficult, if not intractable, for large systems. In this paper we propose a new approach to performance modeling of multiprogrammed processor scheduling policies, that of interpolation approximations. We first define a workload model that contains parameters for the essential properties of parallel applications with respect to scheduling discipline performance, yet lends itself to mathematical analysis. Key features of the workload model include general distribution of total job processing time, general distribution of available job parallelism, and a simple characterization of parallelism overheads. We then show that one can find specific values of the system parameters for which the parallel system under a given scheduling policy reduces to a queueing system with a known (closed-form) solution. Finally, interpolation between the points with known solutions is used to arrive at mean response time estimates that hold over the entire system parameter space. The interpolation approximations readily yield insight into policy behavior and are easy to evaluate for systems with hundreds of processors. We illustrate the approach by developing and validating models of three scheduling policies, under the assumptions of linear job execution rates and independence between job parallelism and processing time. We discuss several insights and results obtained from the analysis of the three policies under the assumed workloads. One result clarifies and generalizes observations in two previous simulation studies of how policy performance varies with the coefficient of variation in job processing requirement. Another result of the interpolation models yields new insight into how policy performance varies with job parallelism. We also comment on the generalizations of these insights for workloads with less restrictive assumptions.",Computer Sciences Department University of Wisconsin,"1210 West Dayton Street Madison, WI 53706.",3
385,Use of Application Characteristics and Limited Preemption for Run-To-Completion Parallel Processor Scheduling Policies *,"Su-Hui Chiang , Rajesh K. Mansharamani , and Mary K. Vernon","Abstract The performance potential of run-to-completion (RTC) parallel processor scheduling policies is investigated by examining whether (1) application execution rate characteristics such as average parallelism (avg) and processor working set (pws) and/or (2) limited preemption can be used to improve the performance of these policies. We address the first question by comparing policies (previous as well as new) that differ only in whether or not they use execution rate characteristics and by examining a wider range of the workload parameter space than previous studies. We address the second question by comparing a simple two-level queueing policy with RTC scheduling in the second level queue against RTC policies that don't allow any preemption and against dynamic equiallocation (EQ). Using simulation to estimate mean response times we find that for promising RTC policies such as adaptive static partitioning (ASP) and shortest demand first (SDF), a maximum allocation constraint that is for all practical purposes independent of avg and pws provides greater and more consistent improvement in policy performance than using avg or pws. Also, under the assumption that job demand information is unavailable to the scheduler we show that the ASP-max policy outperforms all previous high performance RTC policies for workloads with coefficient of variation in processing requirement greater than one. Furthermore, a two-level queue that allows at most one preemption per job outperforms ASP-max but is not competitive with EQ.",Computer Sciences Department University of Wisconsin-Madison,"Madison, WI 53706.",1
386,Program Generalization for Software Reuse: From C to C++,Michael Siff,"Abstract We consider the problem of software generalization: Given a program component C, create a parameterized program component C 0 such that C 0 is usable in a wider variety of syntactic contexts than C. Furthermore, C 0 should be a semantically meaningful generalization of C; namely, there must exist an instantiation of C 0 that is equivalent in functionality to C. In this paper, we present an algorithm that generalizes C functions via type inference. The original functions operate on specific data types; the result of generalization is a collection of C++ function templates that operate on parameterized types. This version of the generalization problem is useful in the context of converting existing C programs to C++.",University of Wisconsin-Madison,"1210 West Dayton Street Madison, WI 53706",4
387,MULTI-COORDINATION METHODS FOR PARALLEL SOLUTION OF BLOCK-ANGULAR PROGRAMS,By Golbon Zakeri,,UNIVERSITY OF WISCONSIN - MADISON,,3
388,SYNCHRONOUS AND ASYNCHRONOUS MULTI-COORDINATION METHODS FOR THE SOLUTION OF BLOCK-ANGULAR PROGRAMS,R.R. MEYER AND G. ZAKERI,"Abstract. Several types of multi-coordination methods for block-angular programs are considered. We present a computational comparison of synchronous multi-coordination methods. The most efficient of these approaches is shown to involve an intermediate number of blocks in the coordination phase. We also develop a new stabilization algorithm and present asynchronous multi-coordination schemes, which are particularly useful when the number of blocks exceeds the number of available processors or when the block sizes vary significantly.",,,3
389,An Evaluation of Object Management System Architectures for Software Engineering Applications,"Jayavel Shanmugasundaram, Barbara Staudt Lerner, Lori Clarke","ABSTRACT Software engineering applications require sophisticated object management system support for creating and manipulating software objects. One of the key issues for object management systems is distribution. Addressing this issue in the context of software engineering applications is particularly challenging because they have widely varying object access profiles. Two fundamental approaches to dealing with distribution are the object server architecture, where objects are shipped to the application program, and the operation server architecture, where operation requests are shipped to where the objects reside. We compare these architectures experimentally to determine the conditions under which each performs better.",Department of Computer Science University of Massachusetts,"Amherst, MA 01003 USA",5
390,Fast and Accurate Flow-Insensitive Points-To Analysis,Marc Shapiro and Susan Horwitz,"Abstract In order to analyze a program that involves pointers, it is necessary to have (safe) information about what each pointer points to. There are many different approaches to computing points-to information. This paper addresses techniques for flow- and context-insensitive in-terprocedural analysis of stack-based storage. The paper makes two contributions to work in this area: * The first contribution is a set of experiments that explore the trade-offs between techniques previously defined by Lars Andersen and Bjarne Steens-gaard. The former has a cubic worst-case running time, while the latter is essentially linear. However, the former may be much more precise than the latter. We have found that in practice, Ander-sen's algorithm is consistently more precise than Steensgaard's. For small programs, there is very little difference in the times required by the two approaches; however, for larger programs, Ander-sen's algorithm can be much slower than Steens gaard's. * The second contribution is the definition of two new algorithms. The first algorithm can be ""tuned"" so that its worst-case time and space requirements, as well as its accuracy range from those of Steens-gaard to those of Andersen. We have experimented with several versions of this algorithm; one version provided a significant increase in accuracy over Steensgaard's algorithm, while keeping the running time within a factor of two. The second algorithm uses the first as a subroutine. Its worst-case time and space requirements are a factor of log N (where N is the number of variables in the program) worse than those of Steensgaard's algorithm. In practice, it appears to","Computer Sciences Department, University of Wisconsin-Madison","1210 West Dayton Street, Madison, WI 53706 USA",2
391,Storage Estimation for Multidimensional Aggregates in the Presence of Hierarchies,Amit Shukla Prasad M. Deshpande Jeffrey F. Naughton Karthikeyan Ramasamy,"Abstract To speed up multidimensional data analysis, database systems frequently precompute aggregates on some subsets of dimensions and their corresponding hierarchies. This improves query response time. However, the decision of what and how much to precompute is a difficult one. It is further complicated by the fact that precomputation in the presence of hierarchies can result in an unintuitively large increase in the amount of storage required by the database. Hence, it is interesting and useful to estimate the storage blowup that will result from a proposed set of precomputations without actually computing them. We propose three strategies for this problem: one based on sampling, one based on mathematical approximation, and one based on probabilistic counting. We investigate the accuracy of these algorithms in estimating the blowup for different data distributions and database schemas. The algorithm based upon probabilistic counting is particularly attractive, since it estimates the storage blowup to within provable error bounds while performing only a single scan of the data.",Computer Sciences Department University of Wisconsin - Madison,,3
392,Asking Questions to Minimize Errors,Nader H. Bshouty,"Abstract A number of efficient learning algorithms achieve exact identification of an unknown function from some class using membership and equivalence queries. Using a standard transformation such algorithms can easily be converted to on-line learning algorithms that use membership queries. Under such a transformation the number of equivalence queries made by the query algorithm directly corresponds to the number of mistakes made by the on-line algorithm. In this paper we consider several of the natural classes known to be learnable in this setting, and investigate the minimum number of equivalence queries with accompanying counterexamples (or equivalently the minimum number of mistakes in the on-line model) that can be made by a learning algorithm that makes a polynomial number of membership queries and uses polynomial computation time. We are able both to reduce the number of equivalence queries used by the previous algorithms and often to prove matching lower bounds. As an example, consider the class of DNF formulas over n variables with at most k = O(log n) terms. Previously, the algorithm of Blum and Rudich [BR92] provided the best known upper bound of 2 O(k) log n for the minimum number of equivalence queries needed for exact identification. We greatly improve on this upper bound showing that exactly k counterexamples are needed if the learner knows k a priori and exactly k +1 counterexamples are needed if the learner does not know k a priori. This exactly matches known lower bounds [BC92]. For many of our results we obtain a complete characterization of the tradeoff between the number of membership and equivalence queries needed for exact identification. The classes we consider here are monotone DNF formulas, Horn sentences, O(log n)-term DNF formulas, read-k sat-j DNF formulas, read-once formulas over various bases, and deterministic finite automata.",Department of Computer Science The University of Calgary,"2500 University Drive N.W. Calgary, Alberta, Canada T2N 1N4",2
393,Optimal Solution of Off-line and On-line Generalized Caching,"Saied Hosseini-Khayat and Jerome R. Cox, Jr.","Abstract. Network traffic can be reduced significantly if caching is utilized effectively. As an effort in this direction we study the replacement problem that arises in caching of multimedia objects. The size of objects and the cost of cache misses are assumed non-uniform. The non-uniformity of size is inherent in multimedia objects, and the non-uniformity of cost is due to the non-uniformity of size and the fact that the objects are scattered throughout the network. Although a special case of this problem, i.e. the case of uniform size and cost, has been extensively studied, the general case needs a great deal of study. We present a dynamic programming method of optimally solving the off-line and on-line versions of this problem, and discuss the complexity of this method.",Washington University in St. Louis,,5
394,Techniques for Developing and Measuring High-Performance Web Servers over ATM Networks,"James C. Hu , Sumedh Mungee, Douglas C. Schmidt","Abstract High-performance Web servers are essential to meet the growing demands of the Internet and large-scale intranets. Satisfying these demands requires a thorough understanding of key factors affecting Web server performance. This paper presents empirical analysis illustrating how dynamic and static adaptivity can enhance Web server performance. Two research contributions support this conclusion. First, the paper presents results from a comprehensive empirical study of Web servers (such as Apache, Netscape Enterprise, PHTTPD, Zeus, and JAWS) over high-speed ATM networks. This study illustrates their relative performance and precisely pinpoints the server design choices that cause performance bottlenecks. We found that once network and disk I/O overheads are reduced to negligible constant factors, the main determinants of Web server performance are its protocol processing path and concurrency strategy. Moreover, no single strategy performs optimally for all load conditions and traffic types. Second, we describe the design techniques and optimizations used to develop JAWS, our high-performance, adaptive Web server. JAWS is an object-oriented Web server that was explicitly designed to alleviate the performance bottlenecks we identified in existing Web servers. It consistently outperforms all other Web servers over ATM networks. The performance optimizations used in JAWS include adaptive pre-spawned threading, fixed headers, cached date processing, and file caching. In addition, JAWS uses a novel software architecture that substantially improves its portability and flex This work was funded in part by NSF grant NCR-9628218, Object Technologies International, Eastman Kodak, and Siemens MED. ibility, relative to other Web servers. Our empirical results illustrate that highly efficient communication software is not antithetical to highly flexible software.",Washington University,Campus Box 1045/Bryan 509,5
395,MINIMIZING MEMORY CACHE USAGE FOR MULTIGRID ALGORITHMS IN TWO DIMENSIONS,CRAIG C. DOUGLAS,"Abstract. Computers today rely heavily on good utilization of their cache memory subsystems. Compilers are optimized for business applications, not scientific computing ones, however. Automatic tiling of basic numerical algorithms is simply not provided by any compiler. Thus, absolutely terrible cache performance is normal for scientific computing applications. Multigrid algorithms combine several numerical algorithms into a more complicated algorithm. In this paper, an algorithm is derived that allows for data to pass through cache exactly once per multigrid level during a V cycle before the level changes. This is optimal cache usage for large problems that do not fit entirely in cache. The new algorithm would appear to be quite complicated to implement, leading to spaghetti coding. Actually, an efficient implementation of the algorithm requires a rigid, highly structured coding style. A coding example is given that is suitable for almost all common discretization methods. Numerical experiments are provided that show that the new algorithm is up to an integer factor faster than the traditional implementation method for common multigrid parameter choices.",,,4
396,GEMMW: A PORTABLE LEVEL 3 BLAS WINOGRAD VARIANT OF STRASSEN'S MATRIX-MATRIX MULTIPLY ALGORITHM,"CRAIG C. DOUGLAS , MICHAEL HEROUX , GORDON SLISHMAN x AND ROGER M. SMITH -","Abstract. Matrix-matrix multiplication is normally computed using one of the BLAS or a reinvention of part of the BLAS. Unfortunately, the BLAS were designed with small matrices in mind. When huge, well conditioned matrices are multiplied together, the BLAS perform like the blahs, even on vector machines. For matrices where the coefficients are well conditioned, Winograd's variant of Strassen's algorithm offers some relief, but is rarely available in a quality form on most computers. We reconsider this method and offer a highly portable solution based on the Level 3 BLAS interface.",,,3
397,Constructing Logic Programs with Higher-Order Predicates 1,Jtrgen Fischer Nilsson,"Abstract: This paper proposes a logic programming approach based on the application of a system of higher-order predicates put at disposal within ordinary logic programming languages such as prolog. These higher-order predicates parallel the higher-order functionals or combinators which form an established part of contemporary functional programming methodology. The suggested toolbox of higher-order predicates for composing logic programs is derived from one universal higher-order predicate. They take the form of recursion operators (in particular for expressing recursion along lists) intended to cover all commonly occurring recursion schemes in logic programming practice. Their theoretical sufficiency is proved and their practical adequacy is argued through examples. The recursion operators, denoting higher-order relations rather than functions, are brought about straightforwardly through a well-known metalogic programming technique, rendering superfluous the need for special higher-order unification mechanisms.",Department of Computer Science Technical University of Denmark,,3
398,Experience with MPI: 'Converting pvmmake to mpimake under LAM' and 'MPI and Parallel Genetic Programming',Judith Ellen Devaney,"Abstract This looks at the issues which arose in porting the pvmmake utility from PVM to MPI. Pvmmake is a PVM application which allows a user to send files, execute commands, and receive results from a single machine on any machine in the virtual machine. Its actions are controlled by the contents of a configuration file. Its most common use is to enable management of the development of a parallel program in a heterogeneous environment. A utility with the same features, mpimake, was coded up to run under LAM. Genetic programming is an algorithm which evolves an algorithm in the form of a program to solve your input problem. The implementation under MPI requires the transfer of dynamic data structures such as lists and trees. This paper discusses the match between the requirements of this algorithm and the datatype feature in MPI. A new library, MPI DataStruct is being developed which can transfer dynamic data structures, created with pointers, without intervention by the user.",NIST,,1
399,Microkernel Operating System Architecture and Mach,David L. Black David B. Golub Daniel P. Julin Richard F. Rashid Richard P. Draves Randall W. Dean Alessandro Forin Joseph Barrera Hideyuki Tokuda Gerald Malan David Bohman,"Abstract Modular architectures based on a microkernel are suitable bases for the design and implementation of operating systems. Prototype systems employing microkernel architectures are achieving the levels of functionality and performance expected and required of commercial products. Researchers at Carnegie Mellon University, the Open Software Foundation, and other sites are investigating implementations of a number of operating systems (e.g., Unix 1 , MS-DOS 2 ) that use the Mach microkernel. This paper describes the Mach microkernel, its use to support implementations of other operating systems, and the status of these efforts.",,,0
400,Weak and Strong Beta Normalisations in Typed -Calculi,Hongwei Xi,"Abstract. We present a technique to study relations between weak and strong fi-normalisations in various typed -calculi. We first introduce a translation which translates a -term into a I-term, and show that a -term is strongly fi-normalisable if and only if its translation is weakly fi-normalisable. We then prove that the translation preserves typability of -terms in various typed -calculi. This enables us to establish the equivalence between weak and strong fi-normalisations in these typed -calculi. This translation can deal with Curry typing as well as Church typing, strengthening some recent closely related results. This may bring some insights into answering whether weak and strong fi-normalisations in all pure type systems are equivalent.",Department of Mathematical Sciences Carnegie Mellon University,"Pittsburgh, PA 15213, USA",1
401,Realistic Parsing: Practical Solutions of Difficult Problems,SYLVAIN DELISLE a & STAN SZPAKOWICZ b,"Abstract This paper describes work on the linguistic analysis of texts within a project devoted to knowledge acquisition from text. We focus on syntactic processing and present some key elements of the projects parser that allow it to deal successfully with technical texts. The parser is fully implemented and tested on a variety of real texts; improvements and enhancements are in progress. Because our knowledge acquisition method assumes no a priori model of the domain of the source text, the parser relies as much as possible on lexical and syntactic clues. That is why it strives for full syntactic analysis rather than some form of text skimming. We present a practical approach to four acknowledged difficult problems which to date have no generally acceptable answers: phrase attachment; time constraints for problematic input (how to avoid long and unproductive computation); parsing conjoined structures (how to preserve broad coverage without losing control of the parsing process); and the treatment of fragmentary input or fragments that are a byproduct of a fallback parsing strategy. We review recent related work and conclude by listing several future work items.",a Dpartement de mathmatiques et dinformatique Universit du Qubec Trois-Rivires,"Trois-Rivires, Qubec, Canada G9A 5H7",5
402,Handling Infeasible Specifications of Cryptographic Protocols,Li Gong,"Abstract In the verification of cryptographic protocols along the approach of the logic for authentication by Burrows, Abadi, and Needham, it is possible to write a specification which does not faithfully represent the real world situation. Such a specification, though impossible or unreasonable to implement, can go undetected and be verified to be correct. It can also lead to logical statements that do not preserve causality, which in turn can have undesirable consequences. Such a specification, called an infeasible specification here, can be subtle and hard to locate. This note shows how the logic of cryptographic protocols by Gong, Needham, and Yahalom can be enhanced with a notion of eligibility to preserve causality of beliefs and detect infeasible specifications. It is conceivable that this technique can be adopted in other similar logics.",ORA Corporation Cornell University,301A Dates Drive,1
403,Abstract Datatypes in PVS,S. Owre N. Shankar,,Computer Science Laboratory SRI International,Menlo Park CA 94025,3
404,Enriching the Expressive Power of Security Labels,Li Gong and Xiaolei Qian,"Abstract| Common security models such as Bell-LaPadula focus on the control of access to sensitive data but leave some important systems issues unspecified, such as the implementation of read-only objects, garbage collection, and object upgrade and downgrade paths. Consequently, different implementations of the same security model may have conflicting operational and security semantics. We propose the use of more expressive security labels for specifying these system issues within the security model, so that the semantics of a system design are precisely understood and are independent of implementation details.",,,2
405,Indexing PROLOG Procedures into DAGs by Heuristic Classification,Michael Sintek,"Abstract This paper first gives an overview of standard PROLOG indexing and then shows, in a step-by-step manner, how it can be improved by slightly extending the WAM indexing instruction set to allow indexing on multiple arguments. Heuristics are described that overcome the difficulty of computing the indexing WAM code. In order to become independent from a concrete WAM instruction set, an abstract graphical representation based on DAGs (called DAXes) is introduced. The paper includes a COMMON LISP listing of the main heuristics implemented; the algorithms were developed for RELFUN, a relational-plus-functional language, but can easily be used in arbitrary PROLOG implementations.",DFKI,Postfach 2080 67608 Kaiserslautern Germany,5
406,Layer Reassignment for Antenna Effect Minimization in 3-Layer Channel Routing,Zhan Chen and Israel Koren,"Abstract As semiconductor technology enters the deep submicron era, reliability has become a major challenge in the design and manufacturing of next generation VLSI circuits. In this paper we focus on one reliability issue the antenna effect in the context of 3-layer channel routing. We first present an antenna effect model in 3-layer channel routing and, based on this, an antenna effect cost function is proposed. A layer reassignment approach is adopted to minimize this cost function and we show that the layer reassignment problem can be formulated as a network bipartitioning problem. Experimental results show that the antenna effect can be reduced considerably by applying the proposed technique. Compared with previous work, one advantage of our approach is that no extra channel area is required for antenna effect minimization. We show that layer reassignment technique can be used in yield-related critical area minimization in 3-layer channel routing as well. The trade-off between these two objectives is also presented.","Department of Electrical and Computer Engineering University of Massachusetts,","Amherst, MA 01003",0
407,Optimal Routing Control: Game Theoretic Approach,"Richard J. La, and Venkat Anantharam",Abstract Communication networks shared by selfish users are considered and modeled as noncooperative repeated games. Each user is interested only in optimizing its own performance by controlling the routing of its load. We investigate the existence of a NEP that achieves the system-wide optimal cost. The existence of a NEP that not only achieves the system-wide optimal cost but also yields a cost for each user no greater than its stage game NEP cost is shown for two-node multiple link networks. It is shown that more general networks where all users have the same source-destination pair have a NEP that achieves the minimum total system cost under a mild technical condition. It is shown general networks with users having multiple source-destination pairs don't necessarily have such an NEP.,Department of Electrical Engineering and Computer Science University of California at Berkeley,,1
408,"TIME-FREQUENCY SIGNAL MODELS FOR MUSIC ANALYSIS, TRANSFORMATION, AND SYNTHESIS",Michael Goodwin Martin Vetterli,"ABSTRACT In signal analysis-synthesis, the analysis derives a set of parameters that the synthesis uses to reconstruct the original signal. In musical applications, this reconstruction should be perceptually accurate, and the parameterization should allow for such desirable signal modifications as time-scaling, pitch-shifting, and cross-synthesis; the analysis parameters should correspond to a signal model that is flexible enough to allow these transformations. Sinusoidal modeling meets this flexibility requirement, but has difficulty representing some salient features of musical signals such as attack transients and noiselike processes. In this paper, sinusoidal modeling is reviewed and some variations are proposed to account for its shortcomings; also, wavelet-based representations of musical signals are considered.",Department of Electrical Engineering and Computer Science & Center for New Music and Audio Technologies University of California at Berkeley,,0
409,"Capacity, Mutual Information, and Coding for Finite-State Markov Channels","Andrea J. Goldsmith, Member, IEEE and Pravin P. Varaiya, Fellow, IEEE","Abstract The Finite-State Markov Channel (FSMC) is a discrete-time varying channel whose variation is determined by a finite-state Markov process. These channels have memory due to the Markov channel variation. We obtain the FSMC capacity as a function of the conditional channel state probability. We also show that for i.i.d. channel inputs, this conditional probability converges weakly, and the channel's mutual information is then a closed-form continuous function of the input distribution. We next consider coding for FSMCs. In general, the complexity of maximum-likelihood decoding grows exponentially with the channel memory length. Therefore, in practice, interleaving and memoryless channel codes are used. This technique results in some performance loss relative to the inherent capacity of channels with memory. We propose a maximum-likelihood decision-feedback decoder with complexity that is independent of the channel memory. We calculate the capacity and cutoff rate of our technique, and show that it preserves the capacity of certain FSMCs. We also compare the performance of the decision-feedback decoder with that of interleaving and memoryless channel coding on a fading channel with 4PSK modulation.",,,0
410,An Analysis of Geographical Push-Caching +L,"James Gwertzman,","Abstract Most caching schemes in wide-area, distributed systems are client-initiated. Decisions of when and where to cache information are made without the benefit of the server's global knowledge of the usage patterns. In this paper, we present a new caching strategy: geographical push-caching. Using the server's global knowledge and a derived network topology, we distribute data to cooperating servers. The World Wide Web is an example of a wide-area system that will benefit from distance-sensitive caching, and we present an architecture that allows a Web server to autonomously replicate HTML pages. We use a trace-driven simulation to evaluate several competing caching strategies. Our results show that geographical push-caching reduces bandwidth consumption and sever load by the same amount as web proxy caching, but with a savings in global cache space of almost two orders of magnitude. More importantly, servers that wish to reduce Internet bandwidth consumption and their load can do so without waiting for web proxies to be implemented world-wide. Furthermore, geographical push-caching helps distribute server load for all web servers, not just the most popular as is the case with proxy caching.",Microsoft Corporation,,1
411,Efficient Formulation for Optimal Modulo Schedulers,Alexandre E. Eichenberger Edward S. Davidson,"Abstract Modulo scheduling algorithms based on optimal solvers have been proposed to investigate and tune the performance of modulo scheduling heuristics. While recent advances have broadened the scope for which the optimal approach is applicable, this approach increasingly suffers from large execution times. In this paper, we propose a more efficient formulation of the modulo scheduling space that significantly decreases the execution time of solvers based on integer linear programs. For example, the total execution time is reduced by a factor of 8.6 when 782 loops from the Perfect Club, SPEC, and Livermore Fortran Kernels are scheduled for minimum register requirements using the more efficient formulation instead of the traditional formulation. Experimental evidence further indicates that significantly larger loops can be scheduled under realistic machine constraints.",ECE Department EECS Department North Carolina State University University of Michigan,"Raleigh, NC 27695-7911 Ann Arbor, MI 48109-2122",5
412,A Scalable Key Distribution Hierarchy,Patrick McDaniel Sugih Jamin,"Abstract As the use of the Internet for electronic commerce, audio and video conferencing, and other applications with sensitive content grows, the need for secure services becomes critical. Central to the success of these services is the support for secure public key distribution. Although there are several existing services available for this purpose, they are not very scalable, either because they depend on a centralized server or rely on ad hoc trust relationships. In this paper, we present and examine a flexible approach to certificate distribution scalable to arbitrarily large networks. We propose a two level hierarchy where certificates can be independently authenticated by one or more peer authorities, called keyservers. Certificates for end-user and host entities are managed within local domains, called enterprises. By administering certificates close to the source, we reduce the load on the key servers and the effects of network topology changes. We describe the design of our system and present a preliminary performance analysis based on traces of present-day DNS requests.",Electrical Engineering and Computer Science Department University of Michigan,"Ann Arbor, MI 48109-2122",3
413,A Novel Framework for Decentralized Supervisory Control with Communication,George Barrett,"ABSTRACT The decentralized control problem that we address in this paper is that of several communicating supervisory controllers, each with different information, working in concert to exactly achieve a given legal sublanguage of the uncontrolled system's language model. We present a novel information structure formalism for dealing with this class of problems. Preliminary results are presented which elucidate a fundamental concept in decentralized control problems: the importance of controllers anticipating future possible communications.",Department of Electrical Engineering and Computer Science The University of Michigan,"1301 Beal Avenue Ann Arbor, MI 48109-2122",6
414,Internet Routing Instability,"Craig Labovitz, G. Robert Malan, and Farnam Jahanian","Abstract This paper examines the network inter-domain routing information exchanged between backbone service providers at the major U.S. public Internet exchange points. Internet routing instability, or the rapid fluctuation of network reach-ability information, is an important problem currently facing the Internet engineering community. High levels of network instability can lead to packet loss, increased network latency and time to convergence. At the extreme, high levels of routing instability have lead to the loss of internal connectivity in wide-area, national networks. In this paper, we describe several unexpected trends in routing instability, and examine a number of anomalies and pathologies observed in the exchange of inter-domain routing information. The analysis in this paper is based on data collected from BGP routing messages generated by border routers at five of the Internet core's public exchange points during a nine month period. We show that the volume of these routing updates is several orders of magnitude more than expected and that the majority of this routing information is redundant, or pathological. Furthermore, our analysis reveals several unexpected trends and ill-behaved systematic properties in Internet routing. We finally posit a number of explanations for these anomalies and evaluate their potential impact on the Internet infrastructure.",University of Michigan Department of Electrical Engineering and Computer Science,"1301 Beal Ave. Ann Arbor, Michigan 48109-2122",3
415,"Distributed Pipeline Scheduling: End-to-End Analysis of Heterogeneous, Multi-Resource Real-Time Systems",Saurav Chatterjee and Jay Strosnider,"Abstract This paper presents an hierarchical end-to-end analysis technique that decomposes the very complex heterogeneous multi-resource scheduling problem into a set of single resource scheduling problems with well defined interactions. We define heterogeneity both in resource types, e.g., CPU, and in scheduling policies, e.g., rate-monotonic scheduling. This analysis technique is one phase of our systems integration framework for designing large-scale, heterogeneous, distributed real-time systems whose timing properties can be strictly controlled and analyzed. This approach, denoted the Distributed Pipelining Framework, exploits the natural pipelining execution pattern found in a large number of continuous (periodic) applications executing over heterogenous resources. A teleconference application is used in this paper to show the utility of the approach.",Department of Electrical & Computer Engineering Carnegie Mellon University,"Pittsburgh, PA 15213",6
416,Some Geographical Applications of Genetic Programming on the Cray T3D Supercomputer,"I. Turton, S. Openshaw and G. Diplock",Abstract The paper describes some geographical applications of a parallel GP code which is run on a Cray T3D 512 processor supercomputer to create new types of well performing mathematical models. A series of results are described which allude to the potential power of the method for which there are many practical applications in spatial data rich environments where there are no suitable existing models and no soundly based theoretical framework on which to base them.,"School of Geography, University of Leeds,","Leeds, UK",0
417,On the Optimality of the Simple Bayesian Classifier under Zero-One Loss,PEDRO DOMINGOS,"Abstract. The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.","Department of Information and Computer Science, University of California,","Irvine, CA 92697",3
418,Some MPEG Decoding Functions on Spert An Example for Assembly Programmers,Arno Formella,"Abstract We describe our method how to implement C-program sequences in torrent (T0) assembler code while there is no efficient automatic tool. We use re-structuring of the source code, vectorization, dataflow graphs, a simple scheduling strategy and a straight forward register allocation algorithm. We define some lower and an upper bound for the expected run time. For two functions, namely the color transformation and reverse DCT, we achieve almost 54, respectively 16 times the performance of a Sparc 2 workstation.",INTERNATIONAL COMPUTER SCIENCE INSTITUTE,"I 1947 Center St. * Suite 600 * Berkeley, California 94704-1198 *",1
419,"Storage of Two and Three Dimensional Raster Type Data for Optimized Retrieval of One, Two or Three Dimensional Features",Kjell Bratbergsengen,"Abstract We are analyzing storage structures for two and three dimensional raster type data which are used for feature retrieval. The features are one, two or three dimensional objects with regular outlines like a rectangle or a prism. The features could be parts of a map or image, an area of special interest for searching after oil, a sequence of ultra sound images, and so on. The storage medium is magnetic disk. The data are stored in chunks or blocks representing a regular part of the source object. We analyze the shape and the size to minimize the cost of retrieval. The optimization is based on minimum time to do retrieval. We have five combinations: Lines and areas from areas and volumes, and volumes from volumes. The optimal block sizes for random retrieval varies, with case, feature size and disk characteristics. One general observation is that longer disk tracks gives larger blocks. For line retrieval the optimal block size is only depending on disk track length. For other cases it is also depending on the feature size. For partly sequential retrieval the block size is not the actual block size used during retrieval, but the smallest addressing unit, and the optimal addressing unit could be rather small. The analysis reveals that using too small blocks could be very costly. The time could easily double or triple if small blocks are used. In many cases the optimal block size is several tracks.","Department of Computer Systems and Information Science Norwegian University of Science and Technology,","Trondheim, Norway",3
420,Write Optimized Object-Oriented Database Systems,Kjetil Nrvag and Kjell Bratbergsengen,"Abstract In a database system, read operations are much more common than write operations, and consequently, database systems have been read optimized. As the size of main memory increases, more of the database read requests will be satisfied from the buffer system, and the amount of disk write operations relative to disk read operations will increase. This calls for a focus on write optimized database systems. In this paper, we present solutions to this problem. We describe in detail the data structures and algorithms needed to realize a write optimized object-oriented database system in the context of Vagabond, an OODB currently being implemented at our department. In Vagabond, focus has been to provide support for applications which have earlier used file systems because of the limited data bandwidth in current database systems, typical examples are super computing applications and geographical information systems",Department of Computer and Information Science Norwegian University of Science and Technology,"7034 Trondheim, Norway",5
421,Signal and Image Processing in Java,Jonathan Campbell and Fionn Murtagh,"Abstract We describe the implementation of a multi-purpose data analysis laboratory, DataLab-J, in the programming language Java. We briefly trace the stages of the evolution of DataLab from a FORTRAN-IV system in 1973 to the current Java development. Description of this evolution allows us to discuss some key design and functionality decisions and issues that arose throughout the years; many of these issues remain topical, so, in addition to an evaluation of Java, we identify and discuss what are for us the major issues in the design of such software. Moreover, we address questions raised by the need to convert legacy systems, e.g. those programmed in C and various versions of FORTRAN. The experience of redesign and implementation in Java is described, together with a brief evaluation of the suitability of Java for 'number-crunching'. Overall conclusions are drawn, regarding design of such software, lessons learned, traps to avoid, and on Java itself.","University of Ulster, Magee College,","Derry, BT48 7JL",4
422,"SEE ME, HEAR ME: INTEGRATING AUTOMATIC SPEECH RECOGNITION AND LIP-READING",Paul Duchnowski 1 Uwe Meier 1 Alex Waibel 1;2,"ABSTRACT We present recent work on integration of visual information (automatic lip-reading) with acoustic speech for better overall speech recognition. A Multi-State Time Delay Neural Network performs the recognition of spelled letter sequences taking advantage of lip images from a standard camera. The problems addressed include efficient but effective representation of the visual information and optimum manner of combining the two modalities when rendering a decision. We show results for several alternatives to direct gray level image as the visual evidence. These are: Principal Components, Linear Discriminants, and DFT coefficients. Dimensionality of the input is decreased by a factor of 12 while maintaining recognition rates. Combination of the visual and acoustic information is performed at three different levels of abstraction. Results suggest that integration of higher order input features works best. On a continuous spelling task, visual-alone recognition of 45-55%, when combined with acoustic data, lowers audio-alone error rates by 30-40%.","1 University of Karlsruhe,","Karlsruhe, Germany",3
423,Tracking Drifting Concepts By Minimizing Disagreements,David P. Helmbold and Philip M. Long,"Abstract In this paper we consider the problem of tracking a subset of a domain (called the target) which changes gradually over time. A single (unknown) probability distribution over the domain is used to generate random examples for the learning algorithm and measure the speed at which the target changes. Clearly, the more rapidly the target moves, the harder it is for the algorithm to maintain a good approximation of the target. Therefore we evaluate algorithms based on how much movement of the target can be tolerated between examples while predicting with accuracy *. Furthermore, the complexity of the class H of possible targets, as measured by d, its VC-dimension, also effects the difficulty of tracking the target concept. We show that if the problem of minimizing the number of disagreements with a sample from among concepts in a class H can be approximated to within a factor k, then there is a simple tracking algorithm for H which can achieve a probability * of making a mistake if the target movement rate is at most a constant times * 2 =(k(d + k) ln 1 * ), where d is the Vapnik-Chervonenkis dimension of H. Also, we show that if H is properly PAC-learnable, then there is an efficient (randomized) algorithm that with high probability approximately minimizes disagreements to within a factor of 7d + 1, yielding an efficient tracking algorithm for H which tolerates drift rates up to a constant times * 2 =(d 2 ln 1 In addition, we prove complementary results for the classes of halfspaces and axis-aligned hy perrectangles showing that the maximum rate of drift that any algorithm (even with unlimited computational power) can tolerate is a constant times * 2 =d.",CIS Board UC Santa Cruz,"Santa Cruz, CA 95064",4
424,Understanding Neural Networks via Rule Extraction,Rudy Setiono and Huan Liu,"Abstract Although backpropagation neural networks generally predict better than decision trees do for pattern classification problems, they are often regarded as black boxes, i.e., their predictions are not as interpretable as those of decision trees. This paper argues that this is because there has been no proper technique that enables us to do so. With an algorithm that can extract rules 1 , by drawing parallels with those of decision trees, we show that the predictions of a network can be explained via rules extracted from it, thereby, the network can be understood. Experiments demonstrate that rules extracted from neural networks are comparable with those of decision trees in terms of predictive accuracy, number of rules and average number of conditions for a rule; they preserve high predictive accuracy of original networks.",Department of Information Systems and Computer Science National University of Singapore,"Kent Ridge, Singapore 0511",0
425,Integrating Pedagogical Agents into Virtual Environments,W. Lewis Johnson Jeff Rickel Randy Stiles Allen Munro 1,"Abstract In order for a virtual environment to be effective as a training tool, it is not enough to concentrate on the fidelity of the renderings and the accuracy of the simulated behaviors. The environment should help trainees develop an understanding of the task being trained, and should provide guidance and assistance as needed. This paper describes a system for developing virtual environments in which pedagogical capabilities are incorporated into autonomous agents that interact with trainees. These pedagogical agents can monitor trainees progress and provide guidance and assistance. The agents interact with simulations of objects in the environment, and with trainees. The paper describes the architectural features of the environment and of the agents that permit the agents to meet instructional objectives within the virtual environment. It also discusses how agent-based instruction is combined with other methods of delivering instruction.",,,5
426,A Multicast Congestion Control Mechanism Using Representatives,Dante DeLucia Katia Obraczka,"Abstract In this paper, we propose a congestion control mechanism for reliable multicast applications that uses a small set of group members, or representatives, to provide timely and accurate feedback on behalf of congested subtrees of a multicast distribution tree. Our algorithm does not need to compute round-trip time (RTT) from all receivers to the source, nor does it require knowledge of group membership or network topology. Through simulations, we evaluate our algorithm with and without TCP cross traffic. This initial evaluation study shows that our algorithm takes advantage of network bandwidth when available, yet does not starve competing flows.",Hughes Research Laboratories USC Information Sciences Institute,"3011 Malibu Canyon Road 4676 Admiralty Way Suite 1001 Malibu CA 90265 Marina Del Rey, CA 90292",2
427,Information Gathering Plans With Sensing Actions,Naveen Ashish and Craig A. Knoblock Alon Levy,"Abstract Information gathering agents can automate the task of retrieving and integrating data from a large number of diverse information sources. The key issue in their performance is efficient query planning that minimizes the number of information sources used to answer a query. Previous work on query planning has considered generating information gathering plans solely based on compile-time analysis of the query and the models of the information sources. We argue that at compile-time it may not be possible to generate an efficient plan for retrieving the requested information because of the large number of possibly relevant sources. We describe an approach that naturally extends query planning to use run-time information to optimize queries that involve many sources. First, we describe an algorithm for generating a discrimination matrix, which is a data structure that identifies the information that can be sensed at run-time to optimize a query plan. Next, we describe how the discrimination matrix is used to decide which of the possible run-time sensing actions to perform. Finally, we demonstrate that this approach yields significant savings (over 90% for some queries) in a real-world task.",Information Sciences Institute and AT&T Bell Laboratories Department of Computer Science AI Principles Research Dept.,"600 Mountain Ave., Room 2A-440 4676 Admiralty Way Murray Hill, NJ 07974 Marina del Rey, CA 90292",1
428,A Synergy of Agent Components: Social Comparison for Failure Detection,Gal A. Kaminka Milind Tambe,,Information Sciences Institute and Computer Science Department University of Southern California,"4676 Admiralty Way, Marina del Rey, CA 90292",2
429,Adaptive Agent Tracking in Real-world Multi-Agent Domains: A Preliminary Report,"Milind Tambe, Lewis Johnson and Wei-Min Shen","Abstract Intelligent interaction in multi-agent domains frequently requires an agent to track other agents' mental states: their current goals, beliefs, and intentions. Accuracy in this agent tracking task is critically dependent on the accuracy of the tracker's (tracking agent's) model of the trackee (tracked agent). Unfortunately, in real-world situations, model imperfections arise due to the tracker's resource and information constraints, as well as due to trackees' dynamic behavior modification. While such model imperfections are unavoidable, a tracker must nonetheless attempt to be adaptive in its agent tracking. This article identifies key issues in adaptive agent tracking and presents an approach called DEFT. At its core, DEFT is based on discrimination-based learning. The main idea is to identify the deficiency of a model based on tracking failures, and revise the model by using features that are critical in discriminating successful and failed tracking episodes. Because in real-world situations the set of candidate discriminating features is very large, DEFT relies on knowledge-based focusing to limit the discrimination to those features that it determines were relevant in successful tracking episodes with an autonomous explanation capability as a major source of this knowledge. This article reports on experiments with an implementation of key aspects of DEFT in a complex synthetic air-to-air combat domain.",Information Sciences Institute and Computer Science Department University of Southern California,"4676 Admiralty Way, Marina del Rey, CA 90292",4
430,Reactive and Automatic Behavior in Plan Execution,Pat Langley Wayne Iba Jeff Shrager,"Abstract Much of the work on execution assumes that the agent constantly senses the environment, which lets it respond immediately to errors or unexpected events. In this paper, we argue that this purely reactive strategy is only optimal if sensing is inexpensive, and we formulate a simple model of execution that incorporates the cost of sensing. We present an average-case analysis of this model, which shows that in domains with high sensing cost or low probability of error, a more `automatic' strategy - one with long intervals between sensing can lead to less expensive execution. The analysis also shows that the distance to the goal has no effect on the optimal sensing interval. These results run counter to the prevailing wisdom in the planning community, but they promise a more balanced approach to the interleaving of execution and sensing.",Robotics Laboratory Recom Technologies Palo Alto Research Center Computer Science Dept.,Mail Stop 269-2,4
431,Advanced Transaction Processing in Multilevel Secure File Stores,Elisa Bertino Sushil Jajodia Luigi Mancini Indrajit Ray x,"Abstract The concurrency control requirements for transaction processing in a multilevel secure file system are different from those in conventional transaction processing systems. In particular, there is the need to coordinate transactions at different security levels avoiding both potential timing covert channels and the starvation of transactions at higher security levels. Suppose a transaction at a lower security level attempts to write a data item that is being read by a transaction at a higher security level. On the one hand, a timing covert channel arises if the transaction at the lower security level is either delayed or aborted by the scheduler. On the other hand, the transaction at the high security level may be subjected to an indefinite delay if it is forced to abort repeatedly. This paper extends the classical two-phase locking mechanism to multilevel secure file systems. The scheme presented here prevents potential timing covert channels and avoids the abort of higher level transactions nonetheless guaranteeing serializability. The programmer is provided with a powerful set of linguistic constructs that supports exception handling, partial rollback and forward recovery. The proper use of these constructs can prevent the indefinite delay in completion of a higher level transaction, and allows the programmer to trade off starvation with transaction isolation.",,,4
432,Using Introspective Reasoning to Refine Indexing,Susan Fox and David B. Leake,"Abstract Introspective reasoning about a system's own reasoning processes can form the basis for learning to refine those reasoning processes. The ROBBIE 1 system uses introspective reasoning to monitor the retrieval process of a case-based planner to detect retrieval of inappropriate cases. When retrieval problems are detected, the source of the problems is explained and the explanations are used to determine new indices to use during future case retrieval. The goal of ROBBIE's learning is to increase its ability to focus retrieval on relevant cases, with the aim of simultaneously decreasing the number of candidates to consider and increasing the likelihood that the system will be able to successfully adapt the retrieved cases to fit the current situation. We evaluate the benefits of the approach in light of empirical results examining the effects of index learning in the ROBBIE system.",Computer Science Department,Lindley Hall 215,4
433,NEURAL NETWORKS FOR CONTROL,Eduardo D. Sontag,"Abstract This paper starts by placing neural net techniques in a general nonlinear control framework. After that, several basic theoretical results on networks are surveyed.","Department of Mathematics, Rutgers University","New Brunswick, NJ 08903, USA",3
434,A Construction of a Cipher From a Single Pseudorandom Permutation,Shimon Even 1 and Yishay Mansour 2,"Abstract We suggest a scheme for a block cipher which uses only one randomly chosen permutation, F . The key, consisting of two blocks, K 1 and K 2 is used in the following way: The message block is XORed with K 1 before applying F , and the outcome is XORed with K 2 , to produce the cryptogram block. We show that the resulting cipher is secure (when the permutation is random or pseudorandom). This removes the need to store, or generate a multitude of permutations.","1 Comp. Sci. Dept., Technion, Israel Institute of Technology,","Haifa, Israel 32000.",4
435,Constructing Small Sample Spaces Satisfying Given Constraints,Daphne Koller,"Abstract Abstract. The subject of this paper is finding small sample spaces for joint distributions of n discrete random variables. Such distributions are often only required to obey a certain limited set of constraints of the form P r(E) = . We show that the problem of deciding whether there exists any distribution satisfying a given set of constraints is NP-hard. However, if the constraints are consistent, then there exists a distribution satisfying them which is supported by a ""small"" sample space (one whose cardinality is equal to the number of constraints). For the important case of independence constraints, where the constraints have a certain form and are consistent with a joint distribution of n independent random variables, a small sample space can be constructed in polynomial time. This last result is also useful for de-randomizing algorithms. We demonstrate this technique by an application to the problem of finding large independent sets in sparse hypergraphs.","Department of Computer Science, Stanford University,","Stanford, CA 94305;",0
436,Authoring and Transcription Tools for Speech-Based Hypermedia Systems,Barry Arons,"Abstract Authoring is usually one of the most difficult parts in the design and implementation of hypertext and hypermedia systems. This problem is exacerbated if the data to be presented by the system is speech, rather than text or graphics, because of the slow and serial nature of speech. This paper provides an overview of speech-only hypermedia, discusses the difficulties associated with authoring databases for such a system, and explores a variety of techniques to assist in the authoring process.",MIT Media Laboratory,"20 Ames Street, E15-353 Cambridge MA, 02139",3
437,DESIGNING AN ECOLOGY OF DISTRIBUTED AGENTS,by Nelson Minar,,,,5
438,Stability of the replica symmetric solution for the information conveyed by a neural network,Simon Schultzy and Alessandro Trevesz,"Abstract The information that a pattern of firing in the output layer of a feedforward network of threshold-linear neurons conveys about the network's inputs is considered. A replica-symmetric solution is found to be stable for all but small amounts of noise. The region of instability depends on the contribution of the threshold and the sparseness: for distributed pattern distributions, the unstable region extends to higher noise variances than for very sparse distributions, for which it is almost nonexistant.","Department of Experimental Psychology,","South Parks Rd.,",6
439,A Comparative Study of Reliable Error Estimators for Pruning Regression Trees,Lus Torgo,Abstract. This paper presents a comparative study of several methods for estimating the true error of treestructured regression models. We evaluate these methods in the context of regression tree pruning. Pruning is considered a key issue for obtaining reliable treestructured models in a real world scenario. The major step of a pruning process consists of obtaining accurate estimates of the error of alternative tree models. We evaluate experimentally four methods for obtaining these estimates in twelve domains. The goal of this evaluation was to characterise the performance of the methods in the task of selecting the best possible tree among the set of trees considered during pruning. The results of the comparison show that certain estimators lead to poor decisions in some domains. The Cross Validation variant that we have proposed achieved the best results on the setups we have considered.,LIACC/FEP University of Porto,"R. Campo Alegre, 823, 2 - 4150 PORTO - PORTUGAL",4
440,Rule Revision with Recurrent Neural Networks,Christian W. Omlin a;b and C.L. Giles a;c,"Abstract Recurrent neural networks readily process, recognize and generate temporal sequences. By encoding grammatical strings as temporal sequences, recurrent neural networks can be trained to behave like deterministic sequential finite-state automata. Algorithms have been developed for extracting grammatical rules from trained networks. Using a simple method for inserting prior knowledge (or rules) into recurrent neural networks, we show that recurrent neural networks are able to perform rule revision. Rule revision is performed by comparing the inserted rules with the rules in the finite-state automata extracted from trained networks. The results from training a recurrent neural network to recognize a known non-trivial, randomly generated regular grammar show that not only do the networks preserve correct rules but that they are able to correct through training inserted rules which were initially incorrect. (By incorrect, we mean that the rules were not the ones in the randomly generated grammar.)","a NEC Research Institute,","4 Independence Way, Princeton, New Jersey",2
441,What Size Neural Network Gives Optimal Generalization? Convergence Properties of Backpropagation,"Steve Lawrence 1;2 , C. Lee Giles 1 , Ah Chung Tsoi 2","Abstract One of the most important aspects of any machine learning paradigm is how it scales according to problem size and complexity. Using a task with known optimal training error, and a pre-specified maximum number of training updates, we investigate the convergence of the backpropagation algorithm with respect to a) the complexity of the required function approximation, b) the size of the network in relation to the size required for an optimal solution, and c) the degree of noise in the training data. In general, for a) the solution found is worse when the function to be approximated is more complex, for b) oversized networks can result in lower training and generalization error in certain cases, and for c) the use of committee or ensemble techniques can be more beneficial as the level of noise in the training data is increased. For the experiments we performed, we do not obtain the optimal solution in any case. We further support the observation that larger networks can produce better training and generalization error using a face recognition example where a network with many more parameters than training points generalizes better than smaller networks.","1 NEC Research Institute,","4 Independence Way, Princeton, NJ 08540",0
442,On the uniqueness of the convolution theorem for the fourier transform,Harold S. Stone Lance R. Williams,"Abstract This paper shows that members of the fourier transform family are the only linear transforms that have a convolution theorem, that is, that can replace O(N 2 ) operations of a convolution in a time domain by O(N) operations in a transform domain. Generally, there is an additional cost to compute the transform itself. Our observation is motivated by recent activity in wavelet and subband decompositions and related spectral analyses, which are attractive alternatives for signal compression applications. A natural question when using such techniques is to determine if convolutions of N -point signals can be calculated with fewer operations in a compressed transform domain than in an uncompressed time domain. The answer is negative for a broad set of assumptions. This paper indicates what assumptions must be relaxed in seeking a linear transform that has a convolution theorem comparable to the convolution theorem for fourier transforms.",NEC Research Institute,"4 Independence Way Princeton, NJ 08540",5
443,The Anisotropy in the Cosmic Microwave Background At Degree Angular Scales.,"C. B. Netterfield, N. Jarosik, L. Page, D. Wilkinson, & E. Wollack 1",,"Princeton University, Department of Physics,","Jadwin Hall, P.O. Box 708, Princeton, NJ 08544",4
444,Laser Remote Sensing Techniques for Vertical Profiling of Cloud and Aerosol Extinction and Back-scatter in the Lower Atmosphere. A Brief Review,"$flfl Papayannis*, E. Fokitis","Abstract In this brief contribution we present the three principal laser remote sensing ( lidar) techniques developed to retrieve the vertical profiling of clouds and of the suspended aerosols (extinction and backscatter) in the lower atmosphere, namely in the 0-7 km altitude region. The three lidar techniques include the elastic ( Klett inversion, Doppler broadening) and the nonelastic backscattering techniques ( Raman scattering). We report on the potential of these techniques, as well as on the typical accuracies of these techniques in the retrieval of the cloud and aerosol extinction and backscatter vertical profiles in the troposphere (0-7 km ASL).","National Technical University of Athens, Physics Department","Zografou Campus, 15780 Zografou, GREECE",4
445,The Dynamics of Prefrontal Cortico-Thalamo-Basal Ganglionic Loops and Short-Term Memory Interference Phenomena,"Jack Gelfand 1 , Vijay Gullapalli 1 , Marcia Johnson 1 , Carol Raye 1 and Jeffrey Henderson 2",Abstract We present computer simulations of a model of the brain mechanisms operating in short-term memory tasks that are consistent with the anatomy and physiology of prefrontal cortex and associated subcortical structures. These simulations include dynamical processes in thalamo-cortical loops which are used to generate short-term persistent responses in prefrontal cortex. We discuss this model in terms of the representation of input stimuli in cortical association areas and prefrontal short-term memory areas. We report on interference phenomena that result from the interaction of these dynamical processes and lateral projections within cortical columns. These interference phenomena can be used to elucidate the representational organization of short-term memory.,Department of Psychology 1 and Department of Computer Science 2 Princeton University,"Princeton, NJ 08544",4
446,On the Boosting Ability of Top-Down Decision Tree Learning Algorithms,Michael Kearns,"Abstract We analyze the performance of top-down algorithms for decision tree learning, such as those employed by the widely used C4.5 and CART software packages. Our main result is a proof that such algorithms are boosting algorithms. By this we mean that if the functions that label the internal nodes of the decision tree can weakly approximate the unknown target function, then the top-down algorithms we study will amplify this weak advantage to build a tree achieving any desired level of accuracy. The bounds we obtain for this amplification show an interesting dependence on the splitting criterion used by the top-down algorithm. More precisely, if the functions used to label the internal nodes have error 1=2  as approximations to the target function, then for the splitting criteria used by CART and C4.5, trees of size (1=*) O(1= 2 * 2 ) and (1=*) O(log(1=*)= 2 ) (respectively) suffice to drive the error below *. Thus (for example), a small constant advantage over random guessing is amplified to any larger constant advantage with trees of constant size. For a new splitting criterion suggested by our analysis, the much stronger bound of (1=*) O(1= 2 ) (which is polynomial in 1=*) is obtained, which is provably optimal for decision tree algorithms. The differing bounds have a natural explanation in terms of concavity properties of the splitting criterion. The primary contribution of this work is in proving that some popular and empirically successful heuristics that are based on first principles meet the criteria of an independently motivated theoretical model.",AT&T Research,"600 Mountain Avenue, Room 2A-423, Murray Hill, New Jersey 07974;",3
447,A Systematic Approach to Host Interface Design for High-Speed Networks,Peter Steenkiste,"Abstract In recent years, networks with media rates of 100 Mbit/second or more have become widely available (FDDI, ATM, HIPPI, ..). However, many computer systems cannot make use of the available bandwidth because of the high overhead associated with network communication. In this paper we review the operations involved in communication over high-speed networks, and we describe optimizations of the network interface that improve network throughput. We also discuss how the payoff of the optimizations is influenced by features of the host software and architecture. This paper is based on our experience with the interfaces for the Nectar and Gigabit Nectar networks.",School of Computer Science Carnegie Mellon University,"5000 Forbes Avenue Pittsburgh, Pennsylvania 15213-3891",6
448,DYNAMIC COUPLING OF UNDERACTUATED MANIPULATORS,Marcel Bergerman Christopher Lee Yangsheng Xu,"Abstract In recent years, researchers have been dedicated to the study of underactuated manipulators which have more joints than control actuators. In previous works, assumptions were made as to the existence of enough dynamic coupling between the active and the passive joints of the manipulator for it to be possible to control the position of the passive joints via the dynamic coupling. In this work, the authors aim to develop an index to measure the dynamic coupling, so as to address when control of the underactuated system is possible, and how the motion and robot configuration can be designed. We discuss extensively the nature of the dynamic coupling and of the proposed coupling index, and their applications in the analysis and design of underactuated systems, and in control and planning of robot motion configuration.",The Robotics Institute Carnegie Mellon University,Pittsburgh PA 15213,6
449,A Linear Spine Calculus,Iliano Cervesato and Frank Pfenning 1,"Abstract We present the spine calculus S !ffi&&gt; as an efficient representation for the linear -calculus !ffi&&gt; which includes intuitionistic functions (!), linear functions (ffi), additive pairing (&), and additive unit (&gt;). S !ffi&&gt; enhances the representation of Church's simply typed -calculus as abstract Bohm trees by enforcing extensionality and by incorporating linear constructs. This approach permits procedures such as unification to retain the efficient head access that characterizes first-order term languages without the overhead of performing -conversions at run time. Potential applications lie in proof search, logic programming, and logical frameworks based on linear type theories. We define the spine calculus, give translations of !ffi&&gt; into S !ffi&&gt; and vice-versa, prove their soundness and completeness with respect to typing and reductions, and show that the spine calculus is strongly normalizing and admits unique canonical forms.",School of Computer Science Carnegie Mellon University,"Pittsburgh, PA 15213",5
450,Tactile Gestures for Human/Robot Interaction,"Richard M. Voyles, Jr. Pradeep K. Khosla","Abstract Gesture-Based Programming is a new paradigm to ease the burden of programming robots. By tapping in to the users wealth of experience with contact transitions, compliance, uncertainty and operations sequencing, we hope to provide a more intuitive programming environment for complex, real-world tasks based on the expressiveness of non-verbal communication. A requirement for this to be accomplished is the ability to interpret gestures to infer the intentions behind them. As a first step toward this goal, this paper presents an application of distributed perception for inferring a users intentions by observing tactile gestures. These gestures consist of sparse, inexact, physical nudges applied to the robots end effector for the purpose of modifying its trajectory in free space. A set of independent agents - each with its own local, fuzzified, heuristic model of a particular trajectory parameter - observes data from a wrist force/torque sensor to evaluate the gestures. The agents then independently determine the confidence of their respective findings and distributed arbitration resolves the interpretation through voting.",Robotics Ph.D. Program Dept. of Electrical and Computer Engineering Carnegie Mellon University,"Pittsburgh, PA 15213",2
451,Using a DEM to Determine Geospatial Object Trajectories,"Robert T. Collins, Yanghai Tsin, J. Ryan Miller and Alan J. Lipton","Abstract This paper addresses the estimation of moving object trajectories within a geospatial coordinate system, using a network of video sensors. A high-resolution (0.5m grid spacing) digital elevation map (DEM) has been constructed using a helicopter-based laser range-finder. Object locations are estimated by intersecting viewing rays from a calibrated sensor platform with the DEM. Continuous object trajectories can then be assembled from sequences of single-frame location estimates using spatio-temporal filtering and domain knowledge.","The Robotics Institute, Carnegie Mellon University,","Pittsburgh, PA. 15213",4
452,Protective Interface Specifications,Gary T. Leavens 2 and Jeannette M. Wing 3,"Abstract. The interface specification of a procedure describes the procedure's behavior using pre- and postconditions. These pre- and postconditions are written using various functions. If some of these functions are partial, or underspec-ified, then the procedure specification may not be well-defined. We show how to write pre- and postcondition specifications that avoid such problems, by having the precondition ""protect"" the postcondition from the effects of partiality and underspecification. We formalize the notion of protection from partiality in the context of specification languages like VDM-SL and COLD-K. We also formalize the notion of protection from underspecification for the Larch family of specification languages, and for Larch show how one can prove that a procedure specification is protected from the effects of underspecification.","1 Department of Computer Science, Iowa State University,","Ames, IA 50011 USA",5
453,Geometric Sensing of Known Planar Shapes,Yan-Bin Jia Michael Erdmann,,The Robotics Institute and School of Computer Science Carnegie Mellon University,"Pittsburgh, Pennsylvania 15213-3891",1
454,Remote Access to Interactive Media,Roger B. Dannenberg,"ABSTRACT Digital interactive media augments interactive computing with video, audio, computer graphics and text, allowing multimedia presentations to be individually and dynamically tailored to the user. Multimedia, and particularly continuous media pose interesting problems for system designers, including those of latency and synchronization. These problems are especially evident when multimedia data is remote and must be accessed via networks. Latency and synchronization issues are discussed, and an integrated system, Tactus, is described. Tactus facilitates the implementation of interactive multimedia computer programs by managing latency and synchronization in the framework of an object-oriented graphical user interface toolkit.","Carnegie Mellon University, School of Computer Science","Pittsburgh, PA 15213 USA",3
455,A Whole Sentence Maximum Entropy Language Model,R. Rosenfeld,"Abstract We introduce a new kind of language model, which models whole sentences or utterances directly using the Maximum Entropy paradigm. The new model is conceptually simpler, and more naturally suited to modeling whole-sentence phenomena, than the conditional ME models proposed to date. By avoiding the chain rule, the model treats each sentence or utterance as a ""bag of features"", where features are arbitrary computable properties of the sentence. The model is unnor-malizable, but this does not interfere with training (done via sampling) or with use. Using the model is computationally straightforward. The main computational cost of training the model is in generating sample sentences from a Gibbs distribution. Interestingly, this cost has different dependencies, and is potentially lower, than in the comparable conditional ME model.",School of Computer Science Carnegie Mellon University,"Pittsburgh, PA 15213",6
456,Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors,Todd Mowry and Anoop Gupta,"Abstract The large latency of memory accesses is a major obstacle in obtaining high processor utilization in large scale shared-memory multiprocessors. Although the provision of coherent caches in many recent machines has alleviated the problem somewhat, cache misses still occur frequently enough that they significantly lower performance. In this paper we evaluate the effectiveness of non-binding software-controlled prefetching, as proposed in the Stanford DASH Multiprocessor, to address this problem. The prefetches are non-binding in the sense that the prefetched data is brought to a cache close to the processor, but is still available to the cache coherence protocol to keep it consistent. Prefetching is software-controlled since the program must explicitly issue prefetch instructions. The paper presents results from detailed simulation studies done in the context of the Stanford DASH multiprocessor. Our results show that for applications with regular data access patterns|we evaluate a particle-based simulator used in aeronautics and an LU-decomposition application|prefetching can be very effective. It was easy to augment the applications to do prefetching and it increased their performance by 100-150% when we prefetched directly into the processor's cache. However, for applications with complex data usage patterns, prefetching was less successful. After much effort, the performance of a distributed-time logic simulation application that made extensive use of pointers and linked lists could be increased only by 30%. The paper also evaluates the effects of various hardware optimizations such as separate prefetch issue buffers, prefetching with exclusive ownership, lockup-free caches, and weaker memory consistency models on the performance of prefetching.","Computer Systems Laboratory Stanford University, CA 94305",,3
457,Learning Maps for Indoor Mobile Robot Navigation,Sebastian Thrun,"Abstract Autonomous robots must be able to learn and maintain models of their environments. Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their complexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and consistent topological maps are often difficult to learn and maintain in large-scale environments, particularly if momentary sensor data is highly ambiguous. This paper describes an approach that integrates both paradigms: grid-based and topological. Grid-based maps are learned using artificial neural networks and naive Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms, the approach presented here gains advantages from both worlds: accuracy/consistency and efficiency. The paper gives results for autonomous exploration, mapping and operation of a mobile robot in populated multi-room environments.","Computer Science Department and Robotics Institute Carnegie Mellon University, Pittsburgh",,0
458,Bayesian Analysis of Variance Component Models via Rejection Sampling,Russell D. Wolfinger,"Abstract We consider the usual Normal linear mixed model for ""components of variance"" from a Bayesian viewpoint. Instead of using Gibbs sampling or other Markov Chain schemes that rely on full conditional distributions, we propose and investigate a method for simulating from posterior distributions based on rejection sampling. The method applies with arbitrary prior distributions but we also employ as a default reference prior a version of Jeffreys's prior based on the integrated (""restricted"") likelihood. We demonstrate the ease of application and flexibility of this approach in several familiar settings, even in the presence of unbalanced data. A program implementing the algorithm discussed here will be available in the SAS MIXED procedure.","SAS Institute Inc.,","SAS Campus Drive, Cary, NC 27513, U.S.A.",3
459,Uncovering Hidden Structure in Bond Futures Trading,"Fei CHEN , Stephen FIGLEWSKI , Jeffrey HEISLER zz , Andreas S. WEIGEND","Abstract. This study uncovers trading styles in the transaction records of US Treasury bond futures. It uses transaction-by-transaction data from the Commodity Futures Trading Commissions' (CFTC) Computerized Trade Reconstruction (CTR) records. The data set consists of 30 million transaction| the complete US T-bond futures market for 3 years. Each transaction record consists of time (by the minute), price, volume, buy/sell, and an identifier of the specific account. We use statistical clustering techniques to group together trades that are similar. Two sets of assumptions have to be made: (1) What is a trade? We define a trade to begin when an account opens a position, and to end when its position size returns to zero. We describe each trade by several trade-specific variables (e.g., length of trade, maximum position size, opening move, long or short) and several exogenous, market-specific variables (e.g., price, volatility, trading volume). (2) What process generated the data? We assume a mixture of Gaussians. An observed trade is interpreted as a noisy realization of one of the mixture components. This paper assumes identity covariance matrices. Furthermore, each trade is fully assigned to a single cluster. We compare this approach to diagonal and to full covariance structure with probabilistic assignments. Trade profit was held back in the clustering process. It turns out that the clusters differ significantly in their profit and risk characteristics. Using conditional distributions, we summarize features of profitable trading styles and contrast them with losing strategies. We find that profitable styles tend to hold trades longer, trade at higher volatility, and trade earlier in the contracts. We also show how some clusters uncover ""technical"" traders. Using the information about the individual accounts, the assignments of accounts to clusters are described by entropy, and the transitions of a given account through clusters is modeled by a first order Markov model.","Leonard N. Stern School of Business, New York University.",,5
460,An Extensible Protocol Architecture for Application-Specific Networking,Marc E. Fiuczynski Brian N. Bershad,"Abstract Plexus is a networking architecture that allows applications to achieve high performance with customized protocols. Application-specific protocols are written in a typesafe language and installed dynamically into the operating system kernel. Because these protocols execute within the kernel, they can access the network interface and other operating system services with low overhead. Protocols implemented with Plexus outperform equivalent protocols implemented on conventional monolithic systems. Plexus runs in the context of the SPIN extensible operating system.",Department of Computer Science and Engineering University of Washington,"Seattle, WA 98195",1
461,"Two Computer Systems Paradoxes: Serialize-to-Parallelize, and Queuing Concurrent-Writes",Rimon Orni and Uzi Vishkin,"Abstract We present and examine the following Serialize-to-Parallelize Paradox: suppose a programmer has a parallel algorithm in mind; the programmer must serialize the algorithm, and is actually trained to suppress its parallelism, while writing code; later, however, compilation and runtime techniques are used to reverse the results of this serialization effort and extract as much parallelism as possible. This work actually provides examples where parallel or parallel-style code enables extracting more parallelism than standard serial code. The ""arbitrary concurrent-write"" convention is useful in parallel algorithms and programs and appears to be not too difficult to implement in hardware for serial machines. Still, typically concurrent-writes to the same memory location in a program are implemented by queuing the write operations, thus requiring time linear in the number of writes. We call this the Queuing Concurrent-Writes Paradox. Assuming that providing useful, easy-to-program programming paradigms to improve the overall effectiveness of computer systems is of interest, this work is a modest example for applying such software-driven considerations to computer architecture issues. This work may be the first to relate parallel algorithms and parallel programming with the technology of instruction level parallelism.",,,3
462,Learning and Vision Algorithms for Robot Navigation,by Margrit Betke,,Department of Electrical Engineering and Computer Science,,5
463,Augmenting Collective Adaptation with Simple Process Agents,Thomas Haynes,"Abstract We have integrated the distributed search of genetic programming based systems with collective memory to form a collective adaptation search method. Such a system significantly improves search as problem complexity is increased. However, there is still considerable scope for improvement. In collective adaptation, search agents gather knowledge of their environment and deposit it in a central information repository. Process agents are then able to manipulate that focused knowledge, exploiting the exploration of the search agents. We examine the utility of increasing the capabilities of the centralized process agents.",Department of Mathematical & Computer Sciences,600 South College Ave.,2
464,Trainable Cataloging for Digital Image Libraries with Applications to Volcano Detection,"M.C. Burl yz , U.M. Fayyad , P. Perona , P. Smyth","Abstract Users of digital image libraries are often not interested in image data per se but in derived products such as catalogs of objects of interest. Converting an image database into a usable catalog is typically carried out manually at present. For many larger image databases the purely manual approach is completely impractical. In this paper we describe the development of a trainable cataloging system: the user indicates the location of the objects of interest for a number of training images and the system learns to detect and catalog these objects in the rest of the database. In particular we describe the application of this system to the cataloging of small volcanoes in radar images of Venus. The volcano problem is of interest because of the scale (30,000 images, order of 1 million detectable volcanoes), technical difficulty (the variability of the volcanoes in appearance) and the scientific importance of the problem. The problem of uncertain or subjective ground truth is of fundamental importance in cataloging problems of this nature and is discussed in some detail. Experimental results are presented which quantify and compare the detection performance of the system relative to human detection performance. The paper concludes by discussing the limitations of the proposed system and the lessons learned of general relevance to the development of digital image libraries.",California Institute of Technology Jet Propulsion Laboratory,"MS 116-81 | Pasadena, CA 91125 MS 525-3660 | Pasadena, CA 91109",4
465,Progress for Local Variables in UNITY,Rob Udink and Ted Herman and Joost Kok,A new notion of refinement for UNITY programs with local variables is defined. This notion is compositional in the following sense: programs can be refined in arbitrary contexts such that all unless and leadsto properties (i.e. temporal properties for both safety and progress) of the composition are preserved. The refinement notion is based on preservation of a new kind of UNITY-like property that takes into account the locality of variables. We do a small case study about registers.,"Department of Computer Science, Utrecht University,","P.O. Box 80089, 3508 TB Utrecht, The Netherlands",2
466,Adaptive Information Filtering using Evolutionary Computation,D.R. Tauritz & J.N. Kok & I.G. Sprinkhuizen-Kuyper,,"Department of Computer Science, Leiden University","P.O. Box 9512, 2300 RA Leiden, The Netherlands",3
